
## 语音合成经典论文阅读
- [x] deep voice 系列
- [x] tacotron 系列
- [ ] Parallel tacotron
- [x] fast speech 系列
- [x] Transformer-TTS
- [x] VITS
- [ ] ClariNet
- [x] DurIAN
- [ ] Diff-TTS
- [x] Grad-TTS
- [x] Glow-TTS
- [ ] Flow-TTS
- [x] PortaSpeech
- [ ] ProDiff
- [ ] FastDiff
- [x] Diff-Wave
- [x] DiffGAN-TTS
- [ ] Diffsound（！！！）
- [ ] Guided-TTS 系列
- [ ] WaveGrad 系列（！！！）
- [ ] BDDM（！！！）
- [ ] CRASH
- [ ] Char2Wav
- [x] EfficientTTS
- [ ] ItôTTS
- [ ] EdiTTS
- [ ] NoreSpeech
- [x] InstructTTS
- [x] SPEAR-TTS 
- [ ] ViT-TTS
- [ ] U-DiT TTS
- [ ] Mega-TTS
- [x] StyleTTS 系列
- [ ] DiCLET-TTS
- [x] LightGrad
- [ ] MusicLDM
- [x] UniCATS
- [ ] DiffVoice
- [x] VQTTS
- [x] prompt TTS 系列
- [x] Natural TTS 系列
- [x] Spectron
- [x] QWen-Audio
- [x] Mega-TTS 系列
- [ ] 综述
	- [ ] An investigation into the adaptability of a diffusion-based TTS model
	- [ ] Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech
- [ ] 对齐
	- [ ] Montreal Forced Aligner
	- [x] Monotonic Alignment Search 
	- [ ] Monotonic Chunkwise Attention
	- [ ] Monotonic Attention
	- [x] Monotonic Alignments
- [ ] 声码器
	- [x] WaveNet
	- [x] MelGAN
	- [x] WaveGAN
	- [x] WaveRNN
	- [ ] SampleRNN
	- [x] WaveGlow
	- [ ] Parallel WaveGAN
	- [x] HiFi-GAN
	- [x] DiffWave
	- [ ] WaveFlow
	- [x] Multiband WaveRNN
	- [ ] FFTNet
	- [x] LPCNet
	- [x] LVCNet
	- [ ] ParaNet
- [x] 其他
	- [x] 评估指标
		- [x] PESQ
		- [x] MOS
	- [ ] probability of voice (POV)？
	- [ ] Generative Speech Coding with Predictive Variance Regularization
	- [ ] Neural analysis and synthesis: Reconstructing speech from self-supervised representations（NIPS）
- [x] 基本原理
	- [x] diffusion
	- [x] SDE
	- [x] score matching
	- [x] flow
	- [x] VAE

## 其他和合成相关的

语音转换
- [x] StarGAN-VC 系列
- [x] MWDLP

codec：
- [x] Encodec
- [x] Sound Stream
- [x] HiFi-Codec

歌声合成
- [x] DiffSinger
- [ ] DurIAN-SC
- [ ] Learning the Beauty in Songs: Neural Singing Voice Beautifier


VQ+Diffusion 调研：
- [x] Vector Quantized Diffusion Model for Text-to-Image Synthesis
- [x] Improved Vector Quantized Diffusion Models
- [x] Argmax flows and multinomial diffusion- Towards non-autoregressive language models
- [x] Structured denoising diffusion models in discrete state-spaces
- [ ] Discrete Contrastive Diffusion for Cross-Modal and Conditional Generation（16）
- [x] Global Context with Discrete Diffusion in Vector Quantised Modelling for Image Generation（CVPR 2022，19）
- [ ] Diffusion bridges vector quantized Variational AutoEncoders（8）
- [ ] Regularized Vector Quantization for Tokenized Image Synthesis（3）
- [ ] Spiking-Diffusion: Vector Quantized Discrete Diffusion Model with Spiking Neural Networks（preprint）
- [ ] Q-Diffusion: Quantizing Diffusion Models（12，ICCV 2023）

## 一些可以做的点

可控生成+韵律建模+离散化

歌声虚假检测？

code switch TTS ？
> 这玩意好像确实没人做，但是做的意义在哪？？？

情感识别的论文？

## 碎碎念 

text encoder + f0 encoder + duration encoder + energy encoder 得到最终的 style embedding，然后进行聚类（但是需要无监督学习啊？！）

把文本作为 VAE 的 condition 来建模 style？

李宏毅：
要实现一个真正好的语音 LM，需要文本的信息。
因为文本到语音的映射是一到多的，960 小时的语音对应的文本用来训练 word embedding 的效果很差，因为数据量太少！
文字的辅助是必要的！
可以启发 TTS？

根据 prompt TTS 的启发，要实现一个真正的 TTS 系统，必须有从文本中读取 风格 的能力，这个文本分为两类：
+ 一个是 从 风格描述 的 文本
+ 一个是要 合成 的 文本

但是训练的时候需要 label，这个 label 怎么来呢？
prompt TTS 的 label 来自于 ChatGPT。

那我可不可以参考条件 diffusion，输入为文本，建模 style？
我可以只预测总的时间：
+ 然后就直接把文本作为条件，输入为纯噪声，输出为每一帧的韵律向量，然后输出的这些韵律向量在训练的时候让他们预测 pitch、energy
+ 好处就是，速度可控了，其他的呢？

或许一个好的整体的 TTS 的结构会比使用的具体模型甚至建模范式要更管用？？？
> 2023.12.11 所谓的好的 TTS 结构，或许就是如果更充分地 disentangle 语音中的不同信息？

参考 mega tts 1 和 2，用 ASR 来辅助解耦岂不是更有效？

参考 style tts，其实可以不用大模型也可以实现很好的效果。

组合 style tts 和 mega tts 中的优点，是否可以实现一个轻量化的 tts ？

韵律建模困难，那么我可以实现层级韵律建模？类似于 RVQ？
或者可以参考 一种 residual 的结构，也是多层的建模？

又是一个创新点，style 如何嵌入，AdaIN 是一种，创新在于，我可以参考 GLU 的方式，把 style 通过 sigmoid 那部分来引入。

WavLM 作为 discriminator 的效果应该不错，可以考虑改进。

PBTC 模块建模 pitch 似乎很强？

length regulator 直接拓展确实不太好？style TTS 或者 EATS 中的可微拓展感觉更好

cqcc等其他特征能不能辅助tts？或者直接作为 intermediate feature？

把 word 和 word boundary 引入 TTS 应该也可以提高性能。

联邦学习 + 语音合成？个性化、隐私保护的语音合成。

## 不同模型 aligner 的使用：

fastspeech：用的是预训练好的模型作为 teacher model
fastspeech 2：用 MFA（Montreal forced alignment）

Glow-TTS：MAS（提出了 MAS）

radtts：one tts alignment to rule them all 的前作，用的是 前向后向算法+维特比算法

natural speech：MAS
natural speech 2：论文中没写，代码中用的是 one tts alignment to rule them all 的 aligner

style tts：MAS
style tts 2：和 style tts 一样

mega tts：用的是 MFA
mega tts 2：还是 MFA

### 轻量化 TTS

1. 引入 prosody encoder 增加韵律（创新点 2）
2. 引入更强的 discriminator（创新点 1 ？？）
3. mamba 作为 text encoder（暂时不行）
4. 引入可学习的 alignment（会增加参数），如果不想增加参数就用 MFA（还是用 MFA 吧，但是要看 backbone 模型）
5. 引入 reference mel encoder（存疑）
6. 引入 Snake activations（暂时不行）

1. WavLM
2. Flow 参数共享（可以分组）
3. vocos
4. 将 vocos 中的深度卷积转为深度可分离卷积
5. 将 text encoder 中的 attention 改为 CGA 
6. 数据增强
7. 参考 albert，transformer 参数共享

减少特征维度用于自己和自己比较。

引入  warm-up，对角线强制对齐，加快收敛。

subscale prediction 也可以加速，但是用在哪呢？

用 DSP synthesizer 中的模型，提取不同的基频幅度特征模式，用于分类。（感觉或许更适合虚假检测？）

## VITS 的改进？

> multilingual 先放一放

1. 参考 YourTTS 引入 speaker encoder？
2. 添加 辅助 loss，说话人分类网络？
3. text encoder 用 GNN
4. d-vector 提取 speaker embedding
5. 改进 discriminator（参考 encodec）

## TTS 常用数据集

