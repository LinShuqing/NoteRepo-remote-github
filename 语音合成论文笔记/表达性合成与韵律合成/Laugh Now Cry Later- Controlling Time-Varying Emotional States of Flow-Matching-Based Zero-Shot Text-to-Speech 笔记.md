> 2024 preprint，台大、微软
<!-- 翻译 & 理解 -->
<!-- People change their tones of voice, often accompanied by non- verbal vocalizations (NVs) such as laughter and cries, to convey rich emotions. However, most text-to-speech (TTS) systems lack the capability to generate speech with rich emotions, including NVs. This paper introduces EmoCtrl-TTS, an emotion-controllable zero- shot TTS that can generate highly emotional speech with NVs for any speaker. EmoCtrl-TTS leverages arousal and valence values, as well as laughter embeddings, to condition the flow-matching- based zero-shot TTS. To achieve high-quality emotional speech gen- eration, EmoCtrl-TTS is trained using more than 27,000 hours of expressive data curated based on pseudo-labeling. Comprehensive evaluations demonstrate that EmoCtrl-TTS excels in mimicking the emotions of audio prompts in speech-to-speech translation scenarios. We also show that EmoCtrl-TTS can capture emotion changes, ex- press strong emotions, and generate various NVs in zero-shot TTS. See https://aka.ms/emoctrl-tts for demo samples. -->
1. 人类改变语调时，通常伴随着 non- verbal vocalizations （NVs）如笑声和哭声，而大多数 TTS 很难生成
2. 提出 EmoCtrl-TTS，一个 emotion-controllable zero-shot TTS，可以生成带有 NVs 的高情感语音
3. EmoCtrl-TTS 使用 arousal 和 valence 值，以及 laughter embeddings 来调节 flow-matching-based zero-shot TTS
4. 使用 27000 小时的 expressive data 进行训练，可以在 speech-to-speech translation 场景中模仿 audio prompts 的情感，同时可以捕捉情感变化，生成各种 NVs

## Introduction
<!-- Humans express a wide range of emotions by changing their tone of voice, often accompanied by nonverbal vocalizations (NVs) such as laughter and crying. While current emotional text-to-speech (TTS) systems have made significant advancements [1–12], they still lack the ability to generate emotional speech with fine-grained control (e.g. changing the emotion states within a single generated utter- ance) and with various types of NVs like laughter and crying. In ad- dition, current emotional TTS systems [1–12] are typically trained on staged datasets with a limited number of speakers; in extreme cases, some are trained on only one speaker. These TTS models of- ten lack the ability to generate emotional speech for any speaker, a feature critical for applications like speech-to-speech translation sys- tems that need to retain both the emotion and speaker characteristics of the source audio when generating the translated speech. -->
<!-- In this paper, we introduce EmoCtrl-TTS, an emotion-controllable zero-shot TTS system that can generate highly emotional speech with NVs for any speaker. EmoCtrl-TTS generates the speech by mimicking the voice characteristics and emotion presented by an audio sample, referred to as an audio prompt. EmoCtrl-TTS is based on the flow-matching-based zero-shot TTS [13] and utilizes valence and arousal values to mimic the time-varying characteristics of emotions. In addition, it also utilizes laughter embeddings [14], which we find to be effective for generating not only laughter but also other NVs, including crying. Furthermore, by leveraging over 27k hours of highly expressive real-world data through careful data mining, EmoCtrl-TTS achieves significant enhancements in robust- ness. Comprehensive evaluations demonstrate that EmoCtrl-TTS excels in reproducing the emotions of audio prompts across multiple languages in speech-to-speech translation scenarios. We also show that EmoCtrl-TTS can capture emotion changes, express strong emotions, and generate various types of NVs in zero-shot TTS. -->
1. 提出 EmoCtrl-TTS，一个 emotion-controllable zero-shot TTS，可以生成带有 NVs 的高情感语音
2. 可以模仿 audio prompt 的 voice characteristics 和 emotion 来生成 speech
3. 基于 [Voicebox- Text-Guided Multilingual Universal Speech Generation at Scale 笔记](../Voicebox-%20Text-Guided%20Multilingual%20Universal%20Speech%20Generation%20at%20Scale%20笔记.md)，采用 valence 和 arousal 值来模仿 emotion 的 time-varying 特征
4. 使用 laughter embeddings，可以生成 laughter 和其他 NVs，包括 crying

## 相关工作

### TTS 中的情感控制
现有的带有情感能力的 TTS：
![](image/Pasted%20image%2020240726165414.png)

需要考虑以下几点：
<!-- The first point is whether the TTS systems can control the fine- grained emotional attributes within one utterance. Such fine-grained control is ideal for many applications, for example, speech-to-speech translation where nuanced emotional changes need to be transferred to the translated speech. However, as shown in the table, most prior works focused on controlling the utterance-level emotion, and only a few works tackled the control of time-varying emotional status. MsEmoTTS [7] used a local emotional strength predictor to estimate syllable-level emotion strength, leveraging it as a condition to con- trol the emotion strength of the generated speech. ELaTE [14] lever- aged laughter representation to condition the flow-matching-based zero-shot TTS, and showed superior controllability of laughter gen- eration. However, these works still lack full controllability of the emotional status. -->
1. TTS 系统是否可以控制 fine-grained emotional attributes
<!-- The second point is the capability to generate NVs. As far as
we investigated, most prior emotional TTS works were not able to generate NVs. While ELaTE [14] can generate natural laughter, it was not investigated with other NVs such as cries. Our work aims to generate arbitrary types of NVs, including laughter and cries. -->
2. 是否可以生成 NVs
<!-- The third point is the size of the training data. Due to the dif- ficulty in developing high-quality emotional training data with supervision, most works utilized less than 100 hours of training data. While ELaTE [14] used 460 hours of speech containing laughter, the data scale is still less than 500 hours. To the best of our knowl- edge, ours is the first to investigate the impact of using large-scale emotional data for TTS training. -->
3. 训练数据的规模
<!-- The fourth point is the number of speakers. As shown in the ta- ble, most of the emotional TTS systems utilized voices from fewer than 100 speakers, with some exceptions where the number of speak- ers is not available. While the number of speakers in our data is also unavailable due to anonymization, we expect that our training data contains a significantly large variation of speakers given the data scale, which is beneficial for the zero-shot TTS capability. -->
4. 说话人数量
<!-- Finally, the fifth point is whether the emotional training data is staged data or real data. As shown in the table, most existing works utilized staged data for their training, which inevitably limited the variety of the speech. For example, in a speech-to-speech translation scenario, the source language speakers are often not professional actors, and their voice characteristics are different from the staged voice. By using large-scale training data, we aim to achieve highly faithful emotion transfer in the zero-shot TTS scenario -->
5. 训练数据是 staged data 还是 real data

### 基于 Flow-matching 的 TTS
<!-- Voicebox [13] is the first to utilize conditional flow matching for training zero-shot TTS. It is designed to perform speech-infilling tasks given audio context and frame-wise phoneme sequence as con- ditions. Given the promising performance of Voicebox, we also em- ploy conditional flow matching to develop our TTS model. -->
1. Voicebox 是第一个使用 conditional flow matching 训练 zero-shot TTS 的模型
<!-- ELaTE [14] was proposed to generate natural laughing speech with fine-grained controllability. It utilized a frame-level laughter repre- sentation derived from the laughter detector [17,18]1 to condition the flow-matching-based zero-shot TTS, showing significantly higher quality and better controllability in generating laughing speech com- pared to conventional models. However, ELaTE was only tested with laughing speech, and the effects on other NVs, such as crying, have not been investigated. In our preliminary experiment, we found that ELaTE sometimes generates laughing speech even when the au- dio prompt contains different NVs, such as crying. Our work can be regarded as an extension of ELaTE, where we aim to achieve better emotion controllability as well as the generation of various NVs. -->
2. ELaTE 用于生成 natural laughing speech，但只测试了 laughing speech，没有测试其他 NVs

## EmoCtrl-TTS

### 概述
<!-- Figure 1 (a) illustrates the training procedure of EmoCtrl-TTS. Given a training audio sample s with transcription y, we extract its mel-filterbank features sˆ ∈ RF ×T , where F denotes the feature dimension and T represents the sequence length. Additionally, we employ force alignment and a phoneme embedding layer to obtain a frame-wise phoneme embedding a ∈ RDphn×T , where Dphn is the phoneme embedding dimension. The phoneme embedding layer is a part of the audio model and is jointly trained. Furthermore, we extract frame-wise embeddings that represent NV h ∈ RDNV ×T and emotion e ∈ RDemo×T , where DNV and Demo denote the dimensions of NV and emotion embeddings respectively. The em- beddings h and e are extracted by using pre-trained NV and emotion detector, respectively, which are discussed in Section 3.2 and 3.3. We leverage the speech infilling task introduced in [13] to train the audio model, focusing on training a conditional flow-matching model to estimate the distribution P (m ⊙ sˆ|(1 − m) ⊙ sˆ, a, h, e), where m ∈ {0, 1}F ×T the Hadamard product -->
EmoCtrl-TTS 训练和推理过程如图：
![](image/Pasted%20image%2020240726170142.png)

给定带有文本 $y$ 的训练样本 $s$，提取 mel-filterbank 特征 $\hat{s} \in \mathbb{R}^{F \times T}$，其中 $F$ 表示特征维度，$T$ 表示序列长度。使用 force alignment 和 phoneme embedding 层来获得 frame-wise phoneme embedding $a \in \mathbb{R}^{D^{phn} \times T}$，其中 $D^{phn}$ 是 phoneme embedding 维度。frame-wise embeddings 表征 NV 为 $h \in \mathbb{R}^{D^{NV} \times T}$ 和 emotion $e \in \mathbb{R}^{D^{emo} \times T}$，其中 $D^{NV}$ 和 $D^{emo}$ 分别表示 NV 和 emotion embeddings 的维度。embeddings $h$ 和 $e$ 通过预训练的 NV 和 emotion detector 提取。使用 Voicebox 中的 speech infilling task 训练 audio model，训练 conditional flow-matching model 来估计分布 $P(m \odot \hat{s}|(1 - m) \odot \hat{s}, a, h, e)$，其中 $m \in \{0, 1\}^{F \times T}$ 是 binary temporal mask，$\odot$ 表示 Hadamard product。
<!-- Figure 1 (b) illustrates the inference procedure of EmoCtrl-TTS. During inference, the model takes four inputs: text prompt ytext, speaker prompt audio sspk , NV prompt audio sN V , and emotion prompt audio semo. The text prompt represents the content of the generated speech. Meanwhile, the speaker, NV, and emotion prompts control the characteristics of the speaker, NV, and emotion in the generated speech, respectively. In speech-to-speech transla- tion scenario, we use the source audio for sspk , sN V and semo , and translated text as ytext. This results in the translated speech main- taining the source speaker’s voice and emotional characteristics. -->
推理时，模型接受四个输入：文本 prompt $y^{text}$，说话人 prompt 音频 $s^{spk}$，NV prompt 音频 $s^{NV}$ 和 emotion prompt 音频 $s^{emo}$。文本 prompt 表示生成语音的内容。同时，说话人、NV 和 emotion prompt 控制生成语音中说话人、NV 和 emotion 的特征。在 speech-to-speech translation 场景中，使用源音频作为 $s^{spk}$，$s^{NV}$ 和 $s^{emo}$，翻译后的文本作为 $y_{text}$。这样可以保留源说话人的声音和情感特征。
<!-- The speaker prompt sspk is first converted to the mel-filterbank spk spk features sˆ . It is also converted to phoneme embeddings a by applying automatic speech recognition (ASR) and then the phoneme embedding layer. The speaker prompt is further converted to NV embeddings hspk and emotion embeddings espk based on the NV detector and emotion detector, respectively. -->
说话人 prompt $s^{spk}$ 首先转换为 mel-filterbank 特征 $\hat{s}^{spk}$。然后 ASR 和 phoneme embedding 层转换为 phoneme embeddings $a^{spk}$。说话人 prompt 还通过 NV detector 和 emotion detector 转换为 NV embeddings $h^{spk}$ 和 emotion embeddings $e^{spk}$。
<!-- Meanwhile, the text prompt ytext is converted to text prompt embeddings atext based on the phone duration model [13] followed by the phoneme embedding layer. The NV prompt embedding hN V and the emotion prompt embedding eemo are extracted from the NV detector and emotion detector, respectively. Note if the lengths of hNV and hemo are different from that of atext, we apply linear in- terpolation to hNV and hemo to match their lengths to that of atext. -->
文本 prompt $y^{text}$ 转换为 text prompt embeddings $a^{text}$，然后通过 phoneme embedding 层。NV prompt embedding $h^{NV}$ 和 emotion prompt embedding $e^{emo}$ 从 NV detector 和 emotion detector 提取。如果 $h^{NV}$ 和 $h^{emo}$ 的长度与 $a^{text}$ 不同，我们对 $h^{NV}$ 和 $h^{emo}$ 进行线性插值，使其长度与 $a^{text}$ 匹配。