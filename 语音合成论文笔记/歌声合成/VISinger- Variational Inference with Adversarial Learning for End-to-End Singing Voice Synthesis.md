> ICASSP 2022，西工大、ASLP
<!-- 翻译+总结+理解 -->
<!-- In this paper, we propose VISinger, a complete end-to-end high- quality singing voice synthesis (SVS) system that directly generates singing audio from lyrics and musical score. Our approach is in- spired by VITS [1], an end-to-end speech generation model which adopts VAE-based posterior encoder augmented with normalizing flow based prior encoder and adversarial decoder. VISinger fol- lows the main architecture of VITS, but makes substantial improve- ments to the prior encoder according to the characteristics of singing. First, instead of using phoneme-level mean and variance of acous- tic features, we introduce a length regulator and a frame prior net- work to get the frame-level mean and variance on acoustic features, modeling the rich acoustic variation in singing. Second, we further introduce an F0 predictor to guide the frame prior network, lead- ing to stabler singing performance. Finally, to improve the singing rhythm, we modify the duration predictor to specifically predict the phoneme to note duration ratio, helped with singing note normal- ization. Experiments on a professional Mandarin singing corpus show that VISinger significantly outperforms FastSpeech+Neural- Vocoder two-stage approach and the oracle VITS; ablation study demonstrates the effectiveness of different contributions. -->
1. 提出 VISinger，端到端高质量歌声合成，可以直接从歌词和乐谱中合成歌声
2. 基于 VITS，但对 prior encoder 进行了改进
    1. 引入 length regulator 和 frame prior network，得到 frame-level 声学特征的均值和方差
    2. 引入 F0 predictor，引导 frame prior network，提高稳定性
    3. 修改 duration predictor，预测 phoneme 到 note 的 duration ratio，辅助歌声 note normalization
3. 在中文数据集上实验，效果优于 FastSpeech + Neural Vocoder 和 oracle VITS

## Introduction
<!-- Automatic human voice generation has been significantly advanced by deep learning. Aiming to mimic human speaking, text to speech (TTS) has witnessed tremendous progress with near human-parity performance. On the other hand, singing voice synthesis (SVS), which aims to generate singing voice from lyrics and music scores, has also been advanced with similar neural modeling framework in speech generation. Compared with speech synthesis, synthetic singing should not only be pronounced correctly according to the lyrics, but also conform to the labels of the music score. As the varia- tion of acoustic features including fundamental frequency in singing is more abundant, and there are subtle pronunciation skills such as vibrato, modeling of singing voice is more challenging. -->
1. 相比于 TTS，SVS 需要更多的声学特征，如 F0，更多的发音技巧，如颤音，更难建模
<!-- A typical deep learning based two-stage singing synthesis sys- tem generally consists of an acoustic model and a vocoder [2, 3, 4, 5]. The acoustic model generates intermediate acoustic features from lyrics and music scores while the vocoder converts these acoustic features into waveform. For example, in [6, 7], the neural acoustic model generates spectrum, excitation and aperiodicity parameters and then singing voice is synthesized using the World [8] vocoder. FastSpeech [9], as a non-AR (auto-aggressive) model, was first used for speech generation and recently adopted in singing synthesis [6] with state-of-the-art performance. As neural vocoders, e.g., Wav- eRNN [10] and HiFiGAN [11] have achieved high-fidelity speech generation, they have become the mainstream in current singing voice synthesis systems to reconstruct singing waveform from inter- mediate acoustic representation like mel-spectrum [12, 13, 14]. -->
<!-- Although these two-stage human voice generation systems made huge progress, the independent training of the two stages, i.e., the neural acoustic model and the neural vocoder, also leads to a mis- match between the training and inference stages, resulting in de- graded performance. Specifically, the neural vocoder is trained us- ing the ground truth intermediate acoustic representation, e.g., mel- spectrum, but the predicted representation from the acoustic model is adopted during inference, resulting in distributional difference be- tween the real and predicted intermediate representations. There are some tricks to alleviate this mismatch problem, including adopting the predicted acoustic features in neural vocoder fine-tuning and ad- versarial training [15]. -->
<!-- A straightforward solution is to plug the two stages to become a unified model trained in an end-to-end manner. In text-to-speech synthesis, this kind of solutions have been recently explored, in- cluding FastSpeech2s [16], EATS [17], Glow-WaveGAN [18] and VITS [1]. In general, these works merge acoustic model and neural vocoder into one model enabling end-to-end learning or adopt a new latent representation instead of mel-spectrum to more easily confine the two parts work on the same distribution. Theoretically, end-to- end training can achieve better sound quality and simpler training and inference process. Among the end-to-end models, VITS uses a variational autoencoder (VAE) [19] to connect the acoustic model and the vocoder, which adopts variational inference augmented with normalizing flows and an adversarial training process, generating more natural-sounding than current two-stage models. -->
<!-- In this paper, we build upon VITS and propose VISinger, an end- to-end singing voice synthesis system based on variational inference (VI). To the best of our knowledge, VISinger is the first end-to-end solution in solving the two-stage mismatch problem in singing gen- eration. It is non-trivial to adopt VITS in singing voice synthesis, be- cause singing has substantial difference with speaking, although they both evolve from the same human vocal system. First, phoneme- level mean and variance of acoustic features are adopted in the flow- based prior encoder of VITS. In the singing task, we introduce a length regulator and a frame prior network to obtain the frame-level mean and variance instead, modeling the rich acoustic variation in singing and leading to more natural singing performance. As an ablation study, we find that simply increasing the number of layers of flow without adding the frame prior network can not achieve the same performance gain. Second, as intonational rendering is vital in singing, we particularly model the intonational aspects by introduc- ing an F0 predictor to further guide the frame prior network, leading to more stable singing with natural intonation. Finally, to improve the rhythm delivery in singing, we modify the duration predictor to specifically predict the phoneme to note duration ratio, helped with singing note normalization. Experiments on a professional Mandarin singing corpus show that the proposed VISinger significantly outper- forms the FastSpeech+Neural-Vocoder two-stage approach and the oracle VITS. -->
2. 本文基于 VITS，提出 VISinger，第一个解决两阶段 mismatch 问题的端到端歌声合成系统，相比于语音，歌声合成又一些难点：
    1. VITS 中用的是 phoneme-level 的均值和方差，这里引入 length regulator 和 frame prior network，得到 frame-level 均值和方差
    2. 引入 F0 predictor，引导 frame prior network，提高稳定性
    3. 修改 duration predictor，预测 phoneme to note 的 duration ratio，辅助歌声 note normalization

## 方法
<!-- As illustrated in Figure 1, inspired by VITS [1], we formulate the proposed model as a conditional variational autoencoder (CVAE), which mainly includes three parts: a posterior encoder, a prior en- coder and a decoder. The posterior encoder extracts the latent rep- resentation z from the waveform y, and the decoder reconstructs the waveform yˆ according to z: -->
如图：
![](image/Pasted%20image%2020240205171550.png)
包含三个部分：
+ posterior encoder 从波形 $y$ 中提取表征 $z$
+ prior encoder
+ decoder 从 $z$ 重构波形 $\hat{y}$

公式表述为：
$$\begin{aligned}z&=Enc(y)\sim q(z|y)\\\hat{y}&=Dec(z)\sim p(y|z)\end{aligned}$$
<!-- In addition, we use a prior encoder to get a prior distribution p(z|c) of the latent variables z given music score condition c. CVAE adopts a reconstruction objective Lrecon and a prior regularization term as: -->
然后用 prior encoder 得到给定音乐乐谱条件下的潜变量 $z$ 的先验分布 $p(z|c)$。CVAE 采用重构目标 $L_{\text{recon}}$ 和先验正则项：
$$L_{cvae}=L_{recon}+D_{KL}(q(z|y)||p(z|c))+L_{ctc},$$
<!-- where DKL is the Kullback-Leibler divergence and Lctc is the connectionist temporal classification (CTC) loss [20]. For the re- construction loss, we use L1 distance of mel-spectrum between the ground truth and the generated waveform. In the following, we will introduce the details of the three modules. -->
其中 $D_{KL}$ 是 KL 散度，$L_{ctc}$ 是 CTC 损失。重构损失用 mel-spectrum 的 L1 距离。

### Posterior Encoder
<!-- The posterior encoder encodes the waveform y into a latent repre- sentation z. To keep the original viewpoint in autoencoder, we treat the Liner Spectrum extractor as a fixed signal processing layer in the encoder. The encoder firstly transforms raw waveform to liner- spectrum with the signal processing layer. Similar with VITS, taking the linear spectrum as input, we use several WaveNet [21] residual blocks to extract a sequence of hidden vector and then produces the mean and variance of the posterior distribution p(z|y) by a linear projection. Then we can get the latent z sampled from p(z|y) using the reparametrization trick. -->
posterior encoder 将波形 $y$ 编码为潜变量 $z$。首先用线性谱提取器将原始波形转换为线性谱，然后用几个 WaveNet 残差块提取一系列隐藏向量，最后用线性投影得到后验分布 $p(z|y)$ 的均值和方差。最后用重参数化技巧得到从 $p(z|y)$ 中采样的潜变量 $z$。