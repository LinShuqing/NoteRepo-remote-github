> 期刊？，高丽大学
<!-- Large language models (LLM)-based speech synthesis has been widely adopted in zero-shot speech synthesis. However, they require a large-scale data and possess the same limitations as previous autoregressive speech models, including slow inference speed and lack of robustness. This paper proposes HierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech (TTS) and voice conversion (VC). We verified that hierarchical speech synthesis frameworks could significantly improve the robustness and expressiveness of the synthetic speech. Furthermore, we significantly improve the naturalness and speaker similarity of synthetic speech even in zero-shot speech synthesis scenarios. For text-to-speech, we adopt the text-to-vec framework, which generates a self-supervised speech representation and an F0 representation based on text representations and prosody prompts. Then, HierSpeech++ generates speech from the generated vector, F0, and voice prompt. We further introduce a high-efficient speech super-resolution framework from 16 kHz to 48 kHz. The experimental results demonstrated that the hierarchical variational autoencoder could be a strong zero-shot speech synthesizer given that it outperforms LLM-based and diffusion-based models. Moreover, we achieved the first human-level quality zero-shot speech synthesis. Audio samples and source code are available at https://github.com/sh-lee-prml/HierSpeechpp. -->
1. 基于 LLM 的 zero-shot 语音合成已经广泛使用了，但是推理速度慢且缺乏鲁棒性
2. 提出了 HierSpeech++，一个快速且强大的 zero-shot 语音合成器，用于 TTS 和 VC
3. 验证了 hierarchical 语音合成框架可以显著提高合成语音的鲁棒性和表现力，即使在 zero-shot 下中也可以显著提高自然度和说话人相似度
4. 对于 TTS，采用 text-to-vec 框架，基于 text representations 和 prosody prompts 生成自监督 speech representations 和 F0 representations，然后 HierSpeech++ 从生成的向量、F0 和 voice prompt 生成语音
5. 还提出了一个从 16 kHz 到 48 kHz 的高效语音超分辨率（super-resolution）框架
6. 实验表明，HierSpeech++ 性能优于 LLM-based 和 diffusion-based 模型，在 zero-shot 语音合成中达到了 human-level quality

## Introduction
<!-- The advent of large language models (LLM) have facil- itated the widespread adoption of LLM-based models in speech synthesis and audio generation tasks. Conventional speech synthesis frameworks have advanced significantly driven by the integration of new features such as neural audio codecs for discrete speech or audio units. Although there is still a room for improvement in LLM-based speech models, these models possess four major limitations: 1) the first drawback is their auto-regressive generative manner, which has a slow inference speed and lack of robustness, resulting in repeating, skipping, and mispronunciation; 2) they are highly dependent on the pre-trained neural audio codec or discrete speech unit; 3) the audio quality of these models puts the clock back before the advent of the strong end-to-end speech synthesis framework proposed in [35]; 4) they require a large-scale dataset to train the model. -->
1. LLM-based 语音模型还是存在四个问题：
    1. 自回归生成方式，推理速度慢且缺乏鲁棒性
    2. 高度依赖预训练的 neural audio codec 或离散语音单元
    3. 语音质量不如 VITS
    4. 需要大规模数据集来训练模型
<!-- VITS [35] successfully introduced an end-to-end (E2E) speech synthesis framework by adopting a variational au- toencoder (VAE) augmented with normalizing flow and adversarial training. Driven by the ability to generate high- quality waveform audio within a fully end-to-end train- ing pipeline, the perceptual quality of synthetic speech is significantly better than that of two-stage speech synthesis models such as conventional text-to-speech (TTS) models and recent codec-based speech synthesis models. HierSpeech [48] further improved the reconstruction quality by adopting a hierarchical conditional VAE using self-supervised speech representation. They proposed a novel TTS frameworks, which can train the model without any text transcripts by leveraging self-supervised speech representation and hierarchical VAE. However, the E2E models have limitations in terms of zero-shot voice cloning. Although E2E models can synthesize speech with high-quality audio, their synthetic speech still has a little speaker similarity in zero-shot voice cloning scenarios, and their training processes require high computational complexity. -->
2. VITS 通过 VAE、normalizing flow 和 adversarial training 实现了很好的效果。HierSpeech 采用 hierarchical conditional VAE 来利用自监督 speech representation 提高了重构质量。但是 E2E 模型在 zero-shot voice cloning 方面还是存在一些问题
<!-- Meanwhile, diffusion-based speech synthesis models have also shown their strengths in terms of speaker adapta- tion. Diff-VC [64] introduced a conditional diffusion proba- bilistic model with a data-dependent prior for zero-shot voice conversion. Additionally, the effectiveness of diffusion mod- els for speaker adaptation, including DDDM-VC [10], Diff- hierVC [11], and UnitSpeech [32], has been proven. Although diffusion models exhibit a good adaptation performance, they have several limitations: 1) they have a slow inference speed with their iterative generation processes, and 2) they are vulnerable to noisy data for speaker adaptation. With noisy speech prompts, they may generate much more noisy speech mainly due to the powerful adaptation performance, and this may further results in the degradation of the perceptual audio quality. 3) Although diffusion models have shown strong generative performance, they still possess lower audio quality owing the train-inference mismatch of the two-stage generation between the ground-truth and generated Mel-spectrogram. -->
3. diffusion-based speech synthesis models 在 speaker adaptation 方面表现很好，但是也存在一些问题：
    1. 推理速度慢
    2. 对于噪声数据很敏感
    3. 由于训练和推理的不匹配，语音质量不高
<!-- In this paper, we present HierSpeech++, a fast and strong zero-shot speech synthesis model that uses a hierarchical speech synthesis framework. By adopting the E2E speech synthesis frameworks to take the advantage of high-quality waveform generation, we solved the limitation of style adap- tation by adopting a self-supervised speech representation as a semantic speech representation and bridging the gap between semantic and acoustic representation hierarchically. We propose a novel speech synthesis framework consisting of a hierarchical speech synthesizer, text-to-vec (TTV), and speech super-resolution (SpeechSR). -->
4. 本文提出了 HierSpeech++，一个快速且强大的 zero-shot 语音合成模型，采用 hierarchical speech synthesis 框架。用自监督 speech representation 作为 semantic speech representation，通过 hierarchical 的方式来缩小 semantic 和 acoustic representation 之间的差距。提出的框架包括 hierarchical speech synthesizer、text-to-vec（TTV）和 speech super-resolution（SpeechSR）
<!-- Based on HierVST [45], we introduce an improved hier- archical speech synthesizer using a hierarchical conditional VAE. To improve audio quality beyond perceptual quality, we adopt a dual-audio acoustic encoder in order to enhance the acoustic posterior and utilize a BigVGAN-based hierar- chical adaptive generator with conditional and unconditional generation for better out-of-distribution generalization (zero- shot voice cloning). In addition, we adopt a source-filter theory-based multi-path semantic encoder to disentangle speech components and enhance the semantic prior for speaker-agnostic and speaker-related semantic information. By using a hierarchical VAE, we can connect and learn these representations hierarchically and infer the waveform audio by progressively adapting to the target voice style. For better adaptation and train-inference mismatch reduction, we introduce bidirectional normalizing flow Transformer networks using AdaLN-Zero. Without a text-speech paired dataset, we can simply scale-up the dataset to train a hierarchical speech synthesizer for zero-shot voice cloning. -->
5. 基于 HierVST，采用 hierarchical conditional VAE 来改进 hierarchical speech synthesizer。为了提高语音质量，采用 dual-audio acoustic encoder 来增强 acoustic posterior，采用 BigVGAN-based hierarchical adaptive generator 来提高 zero-shot voice cloning 的泛化能力。采用 source-filter theory-based multi-path semantic encoder 来分离 speech components 并增强 semantic prior。通过 hierarchical VAE，可以逐步地学习这些 representation 并适应目标说话人的风格。为了更好的 adaptation 和减少 train-inference mismatch，采用 bidirectional normalizing flow Transformer networks
<!-- For text-to-speech, we introduce a TTV that can generate a semantic representation and an F0 from text sequences. Owing to the semantic information that is extracted from self- supervised learning, we can transfer prosody information that is irrelevant to voice style. By connecting the TTV and a hierarchical speech synthesizer, we can synthesize high-quality speech from text by hierarchically adapting the prosody and voice style even in zero-shot scenarios. We also propose a simple speech super-resolution framework to upsample high-resolution waveform audio from 16 kHz to 48 kHz. This can facilitate data accessibility for scaling up datasets in that we can utilize low-resolution speech data such as the automatic speech recognition (ASR) dataset to train the speech synthesizer and TTV models. -->
6. 对于 TTS，提出了 TTV，可以从文本序列生成 semantic representation 和 F0。通过 self-supervised learning 提取的 semantic information，可以迁移与 voice style 无关的 prosody information。将 TTV 和 hierarchical speech synthesizer 连起来，可实现 zero-shot 场景下的高质量语音合成。还提出了一个简单的 speech super-resolution 框架，可以从 16 kHz 上采样到 48 kHz
<!-- The main contributions of this study are as follows:
For fast and strong zero-shot speech synthesis, we presented HierSpeech++, a novel fully-parallel hierar- chical speech synthesis framework.
Prosody and voice style can be transferred and controlled using a hierarchical speech synthesis frame- work.
We also present SpeechSR which can upsample wave- form audio from 16 kHz to 48 kHz for high-resolution speech synthesis and data scalability.
HierSpeech++ achieved the first human-level quality for zero-shot text-to-speech and voice conversion tasks.
Audio samples and source code are available at https: //sh-lee-prml.github.io/HierSpeechpp-demo/ -->
7. 主要贡献：
    1. 提出了 HierSpeech++，一个快速且强大的 zero-shot 语音合成框架
    2. 可以通过 hierarchical speech synthesis 框架来迁移和控制 prosody 和 voice style
    3. 提出了 SpeechSR，可以从 16 kHz 上采样到 48 kHz
    4. HierSpeech++ 在 zero-shot TTS 和 VC 任务上达到了 human-level quality
    5. 开源：https://sh-lee-prml.github.io/HierSpeechpp-demo/

## 相关工作（略）

## HierSpeech++