> preprint 2024.09，CUHK、网易伏羲，李海洲
<!-- 翻译 & 理解 -->
<!-- This paper introduces Easy One-Step Text-to-Speech (E1 TTS), an efficient non-autoregressive zero-shot text-to-speech system based on denoising diffusion pretraining and distribution matching distillation. The training of E1 TTS is straightforward; it does not require explicit monotonic alignment between the text and audio pairs. The inference of E1 TTS is efficient, requiring only one neural network evaluation for each utterance. Despite its sampling efficiency, E1 TTS achieves naturalness and speaker similarity comparable to various strong baseline models. Audio samples are available at e1tts.github.io. -->
1. 提出 Easy One-Step TTS（E1 TTS），非自回归 zero-shot TTS，基于 denoising diffusion 预训练和 distribution matching distillation
2. 训练简单，不需要文本和音频对之间的显式单调对齐
3. 推理高效，每个 utterance 只要一次 evaluation

## Introduction
<!-- Non-autoregressive (NAR) text-to-speech (TTS) models [1] gener- ate speech from text in parallel, synthesizing all speech units simul- taneously. This enables faster inference compared to autoregressive (AR) models, which generate speech one unit at a time. Most NAR TTS models incorporate duration predictors in their architecture and rely on alignment supervision [2]–[4]. Monotonic alignments between input text and corresponding speech provide information about the number of speech units associated with each text unit, guiding the model during training. During inference, learned duration predictors estimate speech timing for each text unit. -->
1. 大部分 NAR TTS 都包含 duration predictor，并依赖于 alignment supervision
<!-- Several pioneering studies [5], [6] have proposed implicit-duration non-autoregressive (ID-NAR) TTS models that eliminate the need for alignment supervision or explicit duration prediction. These models learn to align text and speech units in an end-to-end fashion using attention mechanisms, implicitly generating text-to-speech alignment. -->
2. 一些研究提出了 ID-NAR TTS 模型，不需要 alignment supervision 或 explicit duration prediction，采用 attention 机制端到端学习对齐
<!-- Recently, several diffusion-based [7] ID-NAR TTS models [8]–[14] have been proposed, demonstrating state-of-the-art naturalness and speaker similarity in zero-shot text-to-speech [15]. However, these models still require an iterative sampling procedure taking dozens of network evaluations to reach high synthesis quality. Diffusion distillation techniques [16] can be employed to reduce the number of network evaluations in sampling from diffusion models. Most distillation techniques are based on approximating the ODE sampling trajectories of the teacher model. For example, ProDiff [17] applied Progressive Distillation [18], CoMoSpeech [19] and FlashSpeech [20] applied Consistency Distillation [21], and VoiceFlow [22] and ReFlow-TTS [23] applied Rectified Flow [24]. Recently, a different family of distillation methods was discovered [25], [26], which directly approximates and minimizes various divergences between the generator’s sample distribution and the data distribution. Compared to ODE trajectory-based methods, the student model can match or even outperform the diffusion teacher model [26], as the distilled one- step generator does not suffer from error accumulation in diffusion sampling. -->
3. 基于 diffusion 的 ID-NAR TTS 可以实现 zero-shot TTS，在自然度和说话人相似度上达到 SOTA，但是需要多次的 evaluation，diffusion distillation 可以减少次数，且大多数 distillation 基于 ODE sampling trajectories
<!-- In this work, we distill a diffusion-based ID-NAR TTS model into a one-step generator with recently proposed distribution matching distillation [25], [26] method. The distilled model demonstrates better robustness after distillation, and it achieves comparable performance to several strong AR and NAR baseline systems. -->
4. 本文将基于 diffusion 的 ID-NAR TTS 模型 distill 成 one-step generator，采用 distribution matching distillation，实现和 AR/NAR baseline 系统相当的性能

## 背景

### Distribution Matching Distillation
<!-- Consider a data distribution p(x) on Rd. We can convolve the density p(x) with a Gaussian perturbation kernel qt(xt|x) = N (xt; αtx, σt2Id) to obtain the perturbed density pt(xt) := ∫p(x)qt (xt |x)dx, where αt, σt >0 ratio at each time t ∈ [0,1]. Various formulations of diffusion models exist in the literature [7], [24], most of which are equivalent to learning a neural network that approximates the score function sp(xt, t) := ∇xt log pt(xt) at each time t. -->
考虑位于 $\mathbb{R}^d$ 的数据分布 $p(x)$，将 $p(x)$ 与 Gaussian perturbation kernel $q_t(x_t|x) = N(x_t;\alpha_tx, \sigma_t^2I_d)$ 卷积，得到 perturbed density $p_t(x_t) := \int p(x)q_t(x_t|x)dx$，其中 $\alpha_t, \sigma_t >0$ 是每个时间 $t \in [0,1]$ 的 SNR。大部分 diffusion 模型等价于学习一个神经网络，逼近每个时间 $t$ 的 score function $s_p(x_t, t) := \nabla_{x_t} \log p_t(x_t)$。
<!-- Now, consider a generator function gθ(z) : Rd → Rd that takes in random noise Z ∼ N(0,Id) and outputs fake samples Xb := gθ(Z) with distribution qθ(x). Several studies [25], [27] have discovered that if we can obtain the two score functions sp (x) := ∇x log p(x) and sq(x) := ∇x logqθ(x), we can compute the gradient of the following KL divergence: -->
考虑 generator 函数 $g_{\theta}(z) : \mathbb{R}^d \rightarrow \mathbb{R}^d$，输入随机噪声 $Z \sim \mathcal{N}(0, I_d)$，输出 fake samples $\widehat{X} := g_{\theta}(Z)$，分布为 $q_{\theta}(x)$。如果可以得到两个 score functions $s_p(x) := \nabla_x \log p(x)$ 和 $s_q(x) := \nabla_x \log q_{\theta}(x)$，计算以下 KL 散度的梯度：
$$\nabla_\theta D_{\mathrm{KL}}\left(q_\theta(x)\|p(x)\right)=E\left[\left(s_q(\widehat{X})-s_p(\widehat{X})\right)\frac{\partial g_\theta(Z)}{\partial\theta}\right].$$
<!-- However, obtaining sp(x) and sq(x) directly is challenging. Instead, we can train diffusion models to estimate the score functions sp (xt , t) and sq(xt,t) of the perturbed distributions pt(xt) and qθ,t(xt) := R qθ (x)qt (xt |x)dx. Consider the following weighted average of KL divergence at all noise scales [25], [27]: -->
但是直接得到 $s_p(x)$ 和 $s_q(x)$ 很困难。可以训练 diffusion 模型估计 perturbed distributions $p_t(x_t)$ 和 $q_{\theta, t}(x_t) := R q_{\theta}(x)q_t(x_t|x)dx$ 的 score functions $s_p(x_t, t)$ 和 $s_{q, t}(x_t, t)$。考虑所有 noise scales 的 KL 散度的加权平均：