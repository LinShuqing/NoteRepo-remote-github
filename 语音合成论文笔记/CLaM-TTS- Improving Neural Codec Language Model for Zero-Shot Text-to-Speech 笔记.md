> KRAFTON，ICLR 2024
<!-- With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically am- plifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cas- caded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances. -->
1. audio tokenization 使得长序列和多序列建模很复杂
2. 本文提出 CLaM-TTS，采用 probabilistic residual vector quantization，实现 token 长度压缩，使得 LM 一次生成多个 token
3. 实验结果表明 CLaM-TTS 在自然度、可懂性、说话者相似性和推理速度上优于或与最先进的基于 neural codec 的 TTS 模型相当

## Introduction
<!-- Large language models (LLMs), characterized by a considerable number of model parameters and trained on massive text data, have demonstrated remarkable zero-shot learning capabilities (Brown et al., 2020; Chung et al., 2022; Kaplan et al., 2020). While scaling paradigm affects not only the natural language processing domain but also other fields such as image generation (Ramesh et al., 2021; Saharia et al., 2022), image recognition (Radford et al., 2021), and speech recogni- tion (Baevski et al., 2020b; Radford et al., 2023), significant challenges in their efficient training and inference simultaneously arise. In the realm of image processing, discretizing image represen- tation (Razavi et al., 2019; Ramesh et al., 2021; Esser et al., 2021) has been shown to mitigate these issues by effectively reducing the input length to a manageable size. -->
1. 在图像处理中，通过减少输入长度可以缓解训练和推理中的挑战
<!-- Language modeling in the speech domain has become feasible with the emergence of neural audio codecs (Zeghidour et al., 2021; De ́fossez et al., 2023) that enable high-fidelity audio tokenization. For Text-to-Speech (TTS) synthesis, there have been several attempts to adopt the LLMs for zero- shot TTS, which namely synthesize the diverse speech of any human voice (Zhang et al., 2023; Wang et al., 2023; Kharitonov et al., 2023; Rubenstein et al., 2023). These attempts move away from the previous research direction to train models on curated high-quality recording datasets and produce human-like voices on benchmark datasets (Li et al., 2019; Kim et al., 2021; Tan et al., 2024; Casanova et al., 2022). It is demonstrated that, by training LLMs on tens of thousands of hours of diverse audio data, zero-shot adaptation can be accomplished with just a few seconds of audio input. -->
2. 通过在数万小时的多样音频数据上训练 LLM，可以实现 zero-shot TTS
<!-- Despite the significant advancements in TTS at scale, it still poses challenges to further scale up the models. Neural audio codecs typically generate multiple sequences of audio tokens. For instance, Encodec (De ́fossez et al., 2023) encodes a 5-second speech into 8 sequences of 375 audio tokens. Several work (Kharitonov et al., 2023; Borsos et al., 2023b) employ the semantic tokens from self- supervised speech representation learning (Chung et al., 2021) as an intermediary between text and audio tokens. Although semantic tokens compress information more concisely than audio tokens, a 5-second speech segment still demands 125 semantic tokens, presenting a hurdle even setting aside the further complexities of audio token modeling from them. -->
<!-- In this work, we aim to bring the capability of efficient training and inference of large-language models within the TTS domain. To this end, we propose an improved Codec Language Model-based TTS (CLaM-TTS) system that encodes speech into multiple token sequences similar to existing methods but in a more concise way. With CLaM-TTS, all multiple tokens at each timestep in these sequences are generated through a single autoregressive step of a language model, eliminating the need for iterative generative modeling along the number of sequences. The core of our method lies in the probabilistic discrete representation learning, ensuring that all discrete latent codes participate in the training process, resulting in a high-quality autoencoder for speech. Furthermore, we provide a principled framework enabling a latent language model to efficiently generate a stack of tokens at once; the latent language model produces a continuous latent audio representation and converts it to a discrete representation with the proposed probabilistic quantization method. We scale up the training dataset to 100K hours. Our experimental findings indicate that CLaM-TTS either surpasses or is on par with leading zero-shot neural codec-based TTS models in aspects such as naturalness, intelligibility, speaker similarity, and inference speed. Furthermore, we investigate how the depth of pretraining in the language models and their methods of text tokenization influence TTS outcomes. Our generated samples are available on our demo page1. -->
3. 提出 CLaM-TTS，将 speech 编码为多个 token 序列，每个 time step 的多个 token 通过单个 LM 自回归生成：
    + 核心在于 probabilistic discrete representation learning，确保所有离散 code 都参与训练
    + 提出 principled framework，采用 latent language 方法，使 latent LM 一次生成一堆 token
    + 训练数据：100K 小时
    + 实验结果表明 CLaM-TTS 在自然度、可懂性、说话者相似性和推理速度上优于或与最先进的 zero-shot neural codec-based TTS 模型相当

## 相关工作（略）

## 