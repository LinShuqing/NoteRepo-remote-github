> ICML 2023，University of Surrey
<!-- 翻译 & 理解 -->
<!-- Text-to-audio (TTA) systems have recently gained attention for their ability to synthesize general au- dio based on text descriptions. However, previ- ous studies in TTA have limited generation qual- ity with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn continuous audio representations from contrastive language-audio pretraining (CLAP) embeddings. The pretrained CLAP models enable us to train LDMs with au- dio embeddings while providing text embeddings as the condition during sampling. By learning the latent representations of audio signals with- out modelling the cross-modal relationship, Au- dioLDM improves both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state- of-the-art TTA performance compared to other open-sourced systems, measured by both objec- tive and subjective metrics. AudioLDM is also the first TTA system that enables various text- guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm. github.io. -->
1. TTA 根据文本描述合成音频
2. 提出 AudioLDM，从 CLAP 中学习连续音频表征
3. AudioLDM 通过学习音频信号的潜在表示而不是建模跨模态关系，从而提高了生成质量和计算效率
4. 在单个 GPU 上训练，在 AudioCaps 上实现了 SOTA

## Introduction
<!-- Generating sound effects, music, or speech according to per- sonalized requirements is important for applications such as augmented and virtual reality, game development, and video editing. Traditionally, audio generation has been achieved through signal processing techniques (Andresen, 1979; Karplus & Strong, 1983). In recent years, generative models (Oord et al., 2016; Ho et al., 2020; Song et al., 2021; Tan et al., 2022), either unconditional or condi- tionedonothermodalities(Kreuketal.,2022;Z ̇elaszczyk & Man ́dziuk, 2022), have revolutionized this task. Previ- ous studies primarily worked on the label-to-sound setting with a small set of labels (Liu et al., 2021b; Pascual et al., 2022) such as the ten sound classes in the UrbanSound8K dataset (Salamon et al., 2014). In comparison, natural lan- guage is considerably more flexible than labels as they can include fine-grained descriptions of audio signals, such as pitch, acoustic environment, and temporal order. The task of generating audio prompted with natural language descrip- tions is known as text-to-audio (TTA) generation. -->
1. TTA：从语言描述生成音频
<!-- TTA systems are designed to generate a wide range of high- dimensional audio signals. To efficiently model the data, we adopt a similar approach as DiffSound (Yang et al., 2022) by employing a learned discrete representation to efficiently model high-dimensional audio signals. We also draw in- spiration from the recent advancements in autoregressive modelling of discrete representation learnt on the waveform, such as AudioGen (Kreuk et al., 2022), which has surpassed the capabilities of DiffSound. Building on the success of StableDiffusion (Rombach et al., 2022), which uses latent diffusion models (LDMs) for high-quality image genera- tion, we extend previous TTA approaches to continuous latent representations, instead of learning discrete represen- tations. Additionally, as audio manipulations, such as style transfer (Engel et al., 2020; Pascual et al., 2022), are de- sired for some applications such as games, we explore and achieve various zero-shot text-guided audio manipulations with LDMs, which have not been demonstrated before. -->
2. 本文将 LDMs 用于 TTA，同时实现 zero-shot 文本引导音频操作
<!-- For previous TTA works, a potential limitation for genera- tion quality is the requirement of large-scale high-quality audio-text data pairs, which are usually not readily avail- able, and where they are available, are of limited quality and quantity (Liu et al., 2022f). To better utilize the low-quality data, several methods for text preprocessing have been pro- posed (Kreuk et al., 2022; Yang et al., 2022). However, these preprocessing steps limit generation performances by overlooking the relations of sound events (e.g., a dog is barking at the bark is transformed into dog bark park). By comparison, our proposed method only requires audio data for generative model training, circumvents the challenge of text preprocessing, and performs better than using audio-text paired data, as we will discuss later. -->
3. TTA 的挑战：大规模高质量 音频-文本 数据对得不到或者质量低，本文只需要音频数据训练生成模型
<!-- In this work, we present a TTA system, AudioLDM, which achieves high generation quality with continuous LDMs, with good computational efficiency and enables text-conditional audio manipulations. The overview of AudioLDM design for TTA generation and text-guided audio manipulation is shown in Figure 1. Specifically, AudioLDM learns to generate the representation in a la- tent space encoded by a mel-spectrogram-based variational auto-encoder (VAE). An LDM conditioned on a contrastive language-audio pretraining (CLAP) embedding is developed for VAE latent generation. By leveraging the audio-text- aligned embedding space in CLAP, we remove the require- ment for paired audio-text data during training LDM, as the condition for VAE latent generation can directly come from the audio itself. We demonstrate that training an LDM with audio only can be even better than training with audio- text data pairs. The proposed AudioLDM achieves leading TTA performance on the AudioCaps dataset with a Freshet distance (FD) of 23.31, outperforming the DiffSound base- line (FD of 47.68) by a large margin. Our system also en- ables zero-shot audio manipulations in the sampling process. In summary, our contributions are as follows: -->
4. 提出 AudioLDM，如图：
![](image/Pasted%20image%2020240929110327.png)
    1. 通过 mel-spectrogram-based VAE 学习 latent space 来生成表征
    2. 基于 CLAP embedding 的 LDM 生成 VAE latent
    3. 将 CLAP 的 audio-text 对齐 embedding 空间用于 LDM，从而无需 audio-text 数据对

## 相关工作（略）

## Text-Conditional Audio Generation

### CLAP
<!-- Text-to-image generation models have shown stunning sam- ple quality by utilizing Contrastive Language-Image Pre- training (CLIP) (Radford et al., 2021) for generating the image prior. Inspired by this, we leverage Contrastive Language-Audio Pretraining (CLAP) (Wu et al., 2022) to facilitate TTA generation. -->
将 [Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation 笔记](../语音自监督模型论文阅读笔记/Large-scale%20Contrastive%20Language-Audio%20Pretraining%20with%20Feature%20Fusion%20and%20Keyword-to-Caption%20Augmentation%20笔记.md) 用于 TTA。
<!-- We denote audio samples as x and the text description as y. A text encoder ftext(·) and an audio encoder faudio(·) are used to extract a text embedding Ey ∈ RL and an audio embedding Ex ∈ RL respectively, where L is the dimen- sion of CLAP embedding. A recent study (Wu et al., 2022) has explored different architectures for both the text encoder and the audio encoder when training the CLAP model. We follow their result to build an audio encoder based on HT- SAT (Chen et al., 2022a), and built a text encoder based on RoBERTa (Liu et al., 2019). We use a symmetric cross- entropy loss as the training objective (Radford et al., 2021; Wu et al., 2022). For details of the training process and the language-audio datasets see Appendix A. -->
音频样本为 $x$，文本为 $y$。先用文本编码器 $f_{\text{text}}(\cdot)$ 和音频编码器 $f_{\text{audio}}(\cdot)$ 提取文本嵌入 $E^y \in \mathbb{R}^L$ 和音频嵌入 $E^x \in \mathbb{R}^L$，$L$ 是 CLAP 嵌入的维度。
<!-- After training the CLAP model, an audio sample x can be transformed into an embedding Ex within an aligned audio and text embedding space. The generalization ability of CLAP model has been demonstrated by various downstream tasks such as the zero-shot audio classification (Wu et al., 2022). Then, for unseen language or audio samples, CLAP embeddings also provide cross-modal information. -->
训练 CLAP 后，音频样本 $x$ 可以转换为一个嵌入 $E^x$，在一个对齐的音频和文本嵌入空间中。CLAP 模型的泛化能力已经通过各种下游任务展示，比如 zero-shot 音频分类。