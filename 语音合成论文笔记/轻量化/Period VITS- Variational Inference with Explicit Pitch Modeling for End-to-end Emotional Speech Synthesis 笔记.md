> ICASSP 2023，LINE Corp & NAVER Corp
<!-- 翻译&理解 -->
<!-- Several fully end-to-end text-to-speech (TTS) models have been proposed that have shown better performance compared to cas- cade models (i.e., training acoustic and vocoder models separately). However, they often generate unstable pitch contour with audible artifacts when the dataset contains emotional attributes, i.e., large diversity of pronunciation and prosody. To address this problem, we propose Period VITS, a novel end-to-end TTS model that incor- porates an explicit periodicity generator. In the proposed method, we introduce a frame pitch predictor that predicts prosodic features, such as pitch and voicing flags, from the input text. From these features, the proposed periodicity generator produces a sample-level sinusoidal source that enables the waveform decoder to accurately reproduce the pitch. Finally, the entire model is jointly optimized in an end-to-end manner with variational inference and adversarial objectives. As a result, the decoder becomes capable of generat- ing more stable, expressive, and natural output waveforms. The experimental results showed that the proposed model significantly outperforms baseline models in terms of naturalness, with improved pitch stability in the generated samples. -->
1. 一些全端到端的 TTS 通常会生成不稳定的 pitch contour（当数据集包含情感属性时）
2. 提出 Period VITS，端到端的 TTS 模型，引入了显式的 periodicity generator
    1. 引入 frame pitch predictor，从输入文本预测韵律特征，如 pitch 和 voicing flags
    2. periodicity generator 从这些特征中生成 sample-level sinusoidal source，使得波形解码器可以准确地重现 pitch
    3. 整个模型通过变分推断和对抗训练联合优化
3. 最终，解码器可以生成更稳定、更自然的输出波形

## Introduction
<!-- Text-to-speech (TTS) has recently had a significant impact due to the rapid advancement of deep neural network-based approaches [1]. In most previous studies, TTS models were built as a cascade archi- tecture of two separate models—an acoustic model that generates pre-defined acoustic features (e.g. mel-spectrogram) from text [2, 3] and a vocoder that synthesizes waveform from the acoustic fea- ture [4, 5, 6]. Although these cascade models were able to generate speech reasonably well, they typically suffered from an error deriv- ing from the use of pre-defined features and separated optimization for the two independent models. Sequential training or fine-tuning can mitigate the quality degradation [7], but the training procedure is complicated -->
1. 之前的级连模型通常存在问题，如使用预定义特征、两个独立模型的分开优化
<!-- To address this problem, several works have investigated the use of fully end-to-end architecture that jointly optimizes the acoustic and vocoding models1 [8, 9, 10, 11]. One of the most success- ful works is VITS [10], which adopts a variational autoencoder (VAE) [12] with the augmented prior distribution by normalizing flows [13]. The VAE is used to acquire the trainable latent acoustic features from waveforms, whereas the normalizing flows are used to make the hidden text representation as powerful as the latent features. -->
<!-- However, we found that although VITS generates natural- sounding speech when trained with a reading style dataset, its performance is limited when applied to more challenging tasks, such as emotional speech synthesis, where the dataset has signifi- cant diversity in terms of pronunciation and prosody. Specifically, the model generates less intelligible voices with unstable pitch con- tour. Although the intelligibility problem could be addressed by expanding the phoneme-level parameters of prior distribution to frame-level parameters [14], it is still a challenge to generate ac- curate pitch information due to the architectural limitation of the non-autoregressive vocoders [15]. -->
2. 基于 GAN 和 VAE 的全端到端模型，如 VITS，可以生成自然的语音，但是在情感语音合成等更具挑战性的任务上表现有限
<!-- In this paper, we propose Fre-GAN which synthesizes frequency-consistent audio on par with ground-truth audio. Fre-GAN employs resolution-connected generator and resolution-wise discriminators to learn various levels of spectral distributions over multiple frequency bands. The generator upsamples and sums multiple waveforms at different resolutions. Each waveform is adversarially evaluated in the corresponding resolution layers of the discriminators. To further facilitate the training of discriminators, we provide the downsampled audio to each resolution layer of the discriminators [15–17]. Based on this architecture, we use Discrete Wavelet Transform (DWT) as the downsampling method. While the conventional downsampling method, i.e. AP, washes the high-frequency components away, DWT guarantees that all the information can be kept due to its biorthogonal property. In Fig. 1, we provide evidence for the above statement. Unlike AP, DWT can safely deconstruct the signal into low-frequency and high-frequency sub-bands without losing high-frequency contents. In the experiments, the effectiveness of Fre-GAN is demonstrated on various metrics. -->
3. 提出 Fre-GAN，可以生成与 GT 音频的频率一致的音频；采用