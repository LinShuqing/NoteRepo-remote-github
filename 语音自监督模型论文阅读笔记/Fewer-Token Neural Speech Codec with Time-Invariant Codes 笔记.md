> ICASSP 2024，自动化所、清华
<!-- 翻译 & 理解 -->
<!-- Language model based text-to-speech (TTS) models, like VALL-E, have gained attention for their outstanding in-context learning capa- bility in zero-shot scenarios. Neural speech codec is a critical com- ponent of these models, which can convert speech into discrete token representations. However, excessive token sequences from the codec may negatively affect prediction accuracy and restrict the progres- sion of Language model based TTS models. To address this issue, this paper proposes a novel neural speech codec with time-invariant codes named TiCodec. By encoding and quantizing time-invariant information into a separate code, TiCodec can reduce the amount of frame-level information that needs encoding, effectively decreas- ing the number of tokens as codes of speech. Furthermore, this pa- per introduces a time-invariant encoding consistency loss to enhance the consistency of time-invariant code within an utterance, which can benefit the zero-shot TTS task. Experimental results demon- strate that TiCodec can not only enhance the quality of reconstruc- tion speech with fewer tokens but also increase the similarity and naturalness, as well as reduce the word error rate of the synthe- sized speech by the TTS model. The code is publicly available at https://github.com/y-ren16/TiCodec. -->
1. VALL-E 可以在 zero-shot 场景中实现 in-context learning，但是 codec 生成的 token 过多会影响预测准确性
2. 提出一种新的 codec，TiCodec，将 time-invariant 信息编码为单独的 code，减少需要编码的 frame-level 信息，减少 token 数量
3. 引入 time-invariant encoding consistency loss，增强 utterance 中 time-invariant code 的一致性，有利于 zero-shot TTS
4. 实验结果表明，TiCodec 可以减少 token 数量，提高语音质量，增加相似度和自然度，降低 TTS 模型合成语音的错误率

## Introduction
<!-- Recently, large language models have demonstrated remarkable per- formance on zero-shot text-to-speech (TTS) tasks such as VALL- E [1], SPEAR-TTS [2], and SoundStorm [3]. VALL-E uses dis- crete tokens derived from Encodec [4] as a representation of speech, and then trains an autoregressive (AR) language model and a non- autoregressive (NAR) language model to generate tokens from the first quantizer and the other seven quantizers separately. It can syn- thesize high-quality personalized speech by using a short recording of an unknown speaker as an acoustic prompt. However, the high- quality reconstruction of speech requires multiple frame-level token sequences, which affects the inference speed and robustness, and re- stricts the model structure and training methods of language model based TTS models. Therefore, how to represent speech better with fewer tokens has become a core issue. -->
1. 