> ICASSP 2023，Universite ́ de Montre ́al
<!-- 翻译 & 理解 -->
<!-- Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural lan- guage descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio en- coders and text encoders. We incorporate the feature fusion mecha- nism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio clas- sification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero- shot setting and is able to obtain performance comparable to models’ results in the non-zero-shot setting. LAION-Audio-630K1 and the proposed model2 are both available to the public. -->
1. 提出 contrastive language-audio pretraining，结合 audio data 和 natural language descriptions，得到音频 representation
    1. 发布 LAION-Audio-630K，633,526 个 audio-text pairs
    2. 构建 contrastive language-audio pretraining 模型，考虑不同的 audio encoders 和 text encoders
    3. 使用 feature fusion 和 keyword-to-caption augmentation，处理不同长度的 audio inputs，提高性能
2. 在 text-to-audio retrieval、zero-shot 音频分类和有监督音频分类三个任务上进行实验
3. 模型和代码开源

## Introduction
<!-- Audio is one of the most common information types in the world alongside text and image data. However, different audio tasks typi- cally require finely-annotated data, which limits the amount of avail- able audio data due to the labor-intensive collection procedure. Con- sequently, designing an effective audio representation for many au- dio tasks without requiring a lot of supervision remains a challenge. -->
<!-- The contrastive learning paradigm is a successful solution for training a model on large-scale noisy data collected from internet. The recently proposed Contrastive Language-Image Pretraining (CLIP) [1] learns the correspondence between text and image by projecting them into a shared latent space. The training is conducted by regarding the ground-truth image-text pair as the positive sample and left as negative. In contrast to training on unimodal data, CLIP is not constrained by data annotation and shows great robustness by achieving high accuracy in a zero-shot setting on out-of-domain variations of ImageNet dataset [2]. Additionally, CLIP shows great success in downstream tasks such as text-to-image retrieval and text- guided captioning. Similar to vision, audio and natural languages also contain overlapping information. In audio event classification task, for instance, some text descriptions of an event can be mapped to the corresponding audio. These text descriptions share a similar meaning that could be learned together with the related audio to form an audio representation of crossmodal information. Addition- ally, training such a model requires simply paired audio and text data, which is easy to collect.-->
1. CLIP 学习 text 和 image 的 correspondence，不需要 data annotation，zero-shot 效果好

