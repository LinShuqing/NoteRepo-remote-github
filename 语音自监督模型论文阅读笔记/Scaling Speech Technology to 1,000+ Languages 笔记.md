1. 目前语音的语音技术只服务于大概一百多种语种，但是全世界有 7000 多种语音
2. Massively Multilingual Speech 在不同的任务上将语种的数量增加了 10-40 倍
3. 训练了基于 wav2vec 2.0模型的1406种语音的自监督模型、1107种语言的单一多语言自动语音识别模型、语音合成模型、4017种语言语言的语言识别模型

## Introduction（略）

## 相关工作（略）

## 数据集

MMS-lab：语音+文本，44.7K 小时，1,107 种语言
MMS-unlab：纯语音，7.7K 小时， 3,809 种语言

MMS-lab-U：纯语音，55K 小时，1,362 种语言

## 跨语言自监督语音表征学习

使用wav2vec 2.0对无标签数据在1406种语言上进行预训练，然后在有标签的数据集下进行 fine tune 来处理各种任务。

XLSR和XLS-R则在多个数据集的多种不同语言上训练wav2vec 2.0 来获得跨语言的表征。为了平衡训练数据，采用两个 sample step：
+ 对于每个数据集，根据分布 $p_l \sim\left(\frac{n_l}{N}\right)^{\beta_L}$ 来采样，其中 $l=1,\dots,L,n_l$ 是每种语音无标签数据的数量，$\beta_l$ 为上采样因子，用于平衡性能和低资源语种
+ 把每个数据集都看成一种语言，和前面一样进行采样，然后用 $\beta_D$ 来平衡
### 预训练

使用 fairseq 中的 wav2vec2.0 实现，训练两个模型：
![](image/Pasted%20image%2020230608220037.png)
音频序列裁剪到 320K 样本种，每个长 20s，在 8G 的 A100 GPU 上更新参数 一百万次，MMS (0.3B) 的 batch size 包含 2.3h 数据，分布在 48 台 GPU 种，MMS(1B) batch size 包含 3.5h 数据，分布在 64 台 GPU 中。

总的预训练数据是 491K 小时的 1,406 种语言，训练时 $\beta_L=\beta_D=0.5$。

多语种 ASR 任务种和 XLS-R 对比：![](image/Pasted%20image%2020230608220525.png)
而且在一些低资源的语种上效果提升更多，但是在一些高资源语种上性能却略有下降（如英语）。

## 语音识别

使用标记数据微调预训练的MMS（1B）模型，在模型的顶部添加了一个 linear 层，映射到输出词表，使用 CTC 损失进行 fine tune。

同时还为每种语言使用了不同的 adapter 模块，inner dimension 为 16。

将多语种 ASR 从 61 拓展到 1107，不使用语言模型来比较 CER：![](image/Pasted%20image%2020230608221259.png)
对于没有特定语言参数（LSAH）的语种，性能会迅速下降。

