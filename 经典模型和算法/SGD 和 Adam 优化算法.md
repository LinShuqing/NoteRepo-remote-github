
深度学习优化算法经历了 SGD -> SGDM -> NAG ->AdaGrad -> AdaDelta -> Adam -> Nadam 这样的发展历程。

本文从一个统一的框架来看所有的这些优化算法。

## 问题描述

设 待优化参数为 $w$，优化的目标函数为 $f(w)$，初始学习率为 $\alpha$，此时神经网络的反向传播优化算法在每个 epoch $t$ 可以描述如下：
+ 计算目标函数对当前 epoch 下的参数的梯度 $g_t=\nabla f(w_t)$
+ 根据历史梯度计算一阶动量和二阶动量：$$m_t=\phi(g_1,g_2,\cdots,g_t);V_t=\psi(g_1,g_2,\cdots,g_t)$$这里的 $\phi,\psi$ 是任意函数，正是因为这两个的不同才有不同的优化算法
+ 计算梯度下降：$\eta_t=\alpha\cdot m_t/\sqrt{V_t}$
+ 更新参数：$w_{t+1}=w_t-\eta_t$

### SGD

SDG 没有动量的概念，即：
$$m_t=g_t;V_t=I^2$$
此时的梯度下降为：
$$\eta_t=\alpha\cdot g_t$$

### SGD with momentum

为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。SGDM全称是SGD with momentum，在SGD基础上引入了一阶动量：
$$m_t=\beta_1\cdot m_{t-1}+(1-\beta_1)\cdot g_t$$
一阶动量是各个时刻梯度方向的指数移动平均值。

也就是说，$t$ 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。$\beta_1$ 的经验值为 $0.9$，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。想象高速公路上汽车转弯，在高速向前的同时略微偏向，急转弯可是要出事的。

### AdaGrad

此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到。

对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。

于是使用 二阶动量 来度量历史更新频率。二阶动量 越大，说明更新地越快。

二阶动量计算为，该维度上，迄今为止所有梯度值的平方和：
$$V_t=\sum_{\tau=1}^tg_\tau^2$$
此时根据梯度下降公式：
$$\eta_t=\alpha\cdot m_t/\sqrt{V_t}$$
可以看出，此时实质上的学习率由 $\alpha$ 变成了 $\alpha/\sqrt{V_t}$ 。一般为了避免分母为0，会在分母上加一个小的平滑项 $\epsilon$。由于 $\sqrt{V_t}$ 是恒大于 0 的，当参数更新越频繁，二阶动量越大，学习率就越小。

但 AdaGrad 也存在一些问题：因为 $\sqrt{V_t}$ 是单调递增的，会使得学习率单调递减至 0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。

### AdaDelta / RMSProp

由于 AdaGrad 单调递减的学习率变化过于激进，考虑一个改变 二阶动量 计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是 AdaDelta 名称中 Delta 的来历。

修改的思路很简单，由于 指数移动平均值 大约就是过去一段时间的平均值，因此用这一方法来计算二阶累积动量：
$$V_t=\beta_2*V_{t-1}+(1-\beta_2)g_t^2$$
这就避免了二阶动量持续累积、导致训练过程提前结束的问题了。

### Adam

把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。

SGD的一阶动量：
$$m_t=\beta_1\cdot m_{t-1}+(1-\beta_1)\cdot g_t$$
加上AdaDelta的二阶动量：
$$V_t=\beta_2*V_{t-1}+(1-\beta_2)g_t^2$$
优化算法里最常见的两个超参数 $\beta_1,\beta_2$ 就都在这里了，前者控制一阶动量，后者控制二阶动量。

## 比较

由于 SGD 没有用到二阶动量，因此学习率是恒定的（实际使用过程中会采用学习率衰减策略，因此学习率递减）。AdaGrad 的二阶动量不断累积，单调递增，因此学习率是单调递减的。因此，这两类算法会使得学习率不断递减，最终收敛到 0，模型也得以收敛。

但 AdaDelta 和 Adam 则不然。二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得 $V_t$ 可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型**无法收敛**。

此外，也有文章表明，使用 Adam 可能**错过全局最优解**。

## 选择

目前主流的观点认为：
+ Adam 等自适应学习率算法对于稀疏数据具有优势，且收敛速度很快
+ 但精调参数的 SGD（+Momentum）往往能够取得更好的最终结果。