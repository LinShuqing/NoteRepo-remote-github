> 论文 - A Survey on Neural Speech Synthesis

> TTS：语音合成

1. 综述神经TTS，包括文本分析、声学模型和声码器
2. 综述高阶主题，包括快速 TTS，低资源 TTS、鲁棒 TTS、表达性 TTS 和 自适应 TTS

## Introduction

### TTS 历史

1. 发音合成：模拟人类发音器官的行为来产生语音
2. 共振峰合成：通过控制简化的源滤波器模型的一组规则来生成语音
3. 拼接合成：依赖于存储在数据库中的语音片段的拼接
4. 统计参数合成（SPSS）：首先得到合成语音所需的声学参数，使用算法从参数中恢复语音，包括：
	1. 文本分析
	2. 声学模型
	3. 声码器
5. 神经语音合成：从 WaveNet 开始，它采用（深度）神经网络作为语音合成的模型主干
## 文章组织

如图：![](image/Pasted%20image%2020230128114328.png)

TTS 中的关键技术包含：
+ 文本分析
+ 声学模型
+ 声码器
还介绍了全端到端的 TTS，以及一些其他的分类方法。

TTS 的进阶问题有：
+ 加快生成速率（快速 TTS），减小模型大小
+ 提高合成语音的可理解性和自然度
+ 低资源场景下构建数据高效的 TTS 模型
+ 提高语音合成的鲁棒性，避免生成的语音存在单词跳过或重复的现象
+ 生成富有表现力的语音，控制语音的风格、韵律等
+ 调整TTS模型以支持任何目标说话者的语音（自适应数据和参数）

## TTS 中的关键技术

![](image/Pasted%20image%2020230128115036.png)

### 主要分类

分离式：
1. 文本分析将字符转换为音素或语言特征
2. 声学模型从语言特征或字符/音素生成声学特征
3. 声码器根据语言特征或声学特征生成波形

全端到端式：直接将字符/音素转换为波形

TTS 过程中的几种数据格式：
1. 字符，文本的原始格式
2. 语言特征，分析文本获得，音素是语言特征中最重要的元素之一，通常在基于神经网络的 TTS 模型中单独用于表示文本
3. 声学特征，如 LSP、MCC、MGC、F0、BAP等，在基于神经网络的端到端 TTS 模型中，mel谱图或线性谱图通常用作声学特征
4. 波形：语音的最终格式

几种从文本到波形的数据流：
+ 字符→ 语言特征→ 声学特征→ 波形
+ 字符→ 音素→ 声学特征→ 波形
+ 字符→ 语言特征→ 波形
+ 字符→ 音素→ 声学特征→ 波形
+ 字符→ 音素→ 波形
+ 字符→ 波形

### 文本分析
> 将输入文本转换为包含丰富的语音和韵律信息的语言特征

在端到端的神经TTS中，由于基于神经的模型的建模能力大，字符或音素序列被直接作为合成的输入，因此文本分析模块被大大简化，但仍然需要文本规范化来从字符输入中获得标准单词格式，需要字形到音素的转换来从标准单词格式中获得音素。

文本分析的典型任务：![](image/Pasted%20image%2020230128152747.png)
+ 文本规范化：原始书面文本（非标准单词）通过文本规范化转换为可以发音的单词，如 ”1989“ 转换为 ”nineteen eighty nine“
+ 分词：对于汉语等语言，需要从文本中检测单词边界
+ 词性标注：标注每个单词的词性
+ 韵律预测：韵律信息，如语音的节奏、重音和语调，对应于音节持续时间、响度和音高的变化，在人类交流中起着重要的感知作用
+ 字素到音素转换（Grapheme-to-phoneme (G2P)）：如，单词“speech”被转换为“s p iy ch”

### 声学模型
> 从语言特征或直接从音素或字符 生成 声学特征

可以分为：
+ SPSS中的声学模型，通常根据语言特征预测如MGC、BAP和F0等声学特征
+ 基于神经网络的端到端 TTS 中的声学模型，根据音素或字符预测如mel谱图等声学特征
![](image/Pasted%20image%2020230128154654.png)
#### SPSS 中的声学模型

主要的方法有：
+ HMM，在改变说话者身份、情绪和说话风格方面更为灵活，但质量不行
+ DNN，提高了 HMM 的质量
+ LSTM，更好地建模长时间跨度的上下文
+ CBHG、VoiceLoop、GAN 等等

#### 端到端 TTS 中的声学模型

相比于 SPSS，端到端的 TTS 中的声学模型的优点有：
+ 无需语言和声学特征的对齐
+ 建模能力更强

##### 基于 RNN 的模型

Tacotron 系列：
+ 第一版，输入字符，输出线性谱，encoder-attention-decoder 结构，GriffinLim 生成波形
+ 第二版，生成 Mel 谱，使用额外的 WaveNet 生成波形
+ 改进：
	+ GST Tacotron 和 Ref-Tacotron：使用参考编码器和样式标记来增强语音的表现力
	+ DurIAN 和 非注意力Tacotron：去除注意力机制，使用持续时间预测因子进行自回归预测
	+ Parallel Tacotron 1、2：将Tacotron中的自回归生成改为非自回归生成
	+ Wave Tacotron：基于Tacotron构建端到端文本到波形模型

##### 基于 CNN 的模型

DeepVoice 系列：
+ 第一版：是一个用卷积神经网络增强的SPSS系统，通过神经网络获得语言特征后，DeepVoice利用基于 WaveNet 的声码器生成波形
+ 第二版：通过改进的网络结构和多说话人建模来增强 DeepVoice，采用       Tacotron+WaveNet，使用 Tacotron 生成线性频谱图，然后使用WaveNet生成波形
+ 第三版，使用 全卷积网络结构进行语音合成，从字符生成mel谱图，可拓展到多说话人

ClariNet：全端到端的方式从文本生成波形
ParaNet：全卷积非自回归模型
DCTTS：利用基于卷积的 encoder-attention-decoder 网络从字符序列生成mel谱。使用谱超分辨率网络获得线性谱，使用 GriffinLim 合成波形

##### 基于 Transformer 的模型

TransformerTTS：利用基于 Transformer 的 encoder-attention-decoder 架构从音素生成mel谱图
MultiSpeech：通过编码器归一化、解码器瓶颈和对角注意约束提高注意机制的鲁棒性
RobuTrans：利用持续时间预测来增强自回归生成中的鲁棒性

> 以前的基于神经的声学模型，如Tacotron 1/2、DeepVoice 3和TransformerTTS 都采用自回归生成，这存在几个问题：1）推理速度慢。2） 鲁棒性问题。生成的语音通常具有大量的单词跳过和重复以及问题，这主要是由于基于 encoder-attention-decoder 的自回归生成中文本和mel谱之间的注意力对齐不准确造成的。
> FastSpeech可以解决这些问题：
> 1. 采用前馈 Transformer 网络并行生成mel谱，这可以大大加快推断速度
> 2. 取消文本和语音之间的注意力机制，以避免单词跳过和重复问题，提高了鲁棒性

FastSpeech 系列：
+ 第一版：利用显式持续时间预测器来扩展音素隐藏序列，以匹配mel谱的长度
+ 第二版：使用 GT-mel谱 作为训练目标而非从自回归教师模型中提取mel谱；提供更多的信息，如音调、持续时间和能量作为解码器输入
+ 改进：
	+ FastPitch：使用音调信息作为解码器输入来改进FastSpeech

##### 其他模型

基于 flow 的模型，如：Flowtron、flow TTS、Glow TTS
基于 VAE 的模型：GMVAE Tacotron、VAE-TTS、BVAE-TTS
基于 GAN 的模型：GAN exposure、TTS-Stylization、Multi-SpectroGAN
基于 Diffusion 的模型：Diff-TTS、Grad-TTS、PriorGrad

### 声码器
> 声码器的发展可以分为两个阶段：SPSS中使用的声码器和基于神经网络的声码器
> SPSS中一些流行的声码器包括 STRIGHT 和 WORLD
> 下面主要介绍基于神经的声码器

早期的神经声码器如WaveNet、Char2Wav、WaveRNN直接将语言特征作为输入并生成波形。

后来使用各种生成模型进行波形生成，可以分为：
+ 自回归声码器
+ 基于 flow 的声码器
+ 基于 GAN 的声码器
+ 基于 VAE 的声码器
+ 基于 Diffusion 的声码器
![](image/Pasted%20image%2020230129102533.png)
#### 自回归声码器

WaveNet 是第一个基于神经的声码器，利用 dilated convolution 自回归生成波形，不需要任何先验知识，但是其生成速度慢。
SampleRNN 利用分层递归神经网络进行波形生成。
WaveRNN 使用递归神经网络，并利用 dual softmax层、权重修剪和subscaling 技术在内的多种设计来减少计算。
LPCNet 将传统的数字信号处理引入神经网络，使用线性预测系数来计算下一个波形点，同时利用轻量级RNN来计算残差。

#### 基于 flow 的声码器

归一化流是一种生成模型，利用一系列可逆映射变换概率密度。在采样期间，它通过这些变换的逆运算从标准概率分布生成数据。TTS 中基于流的模型可以分为：
+ 自回归变换：Parallel Wavenet、ClariNet，但是依赖于复杂的师生训练
+ 二分变换：WaveGlow、FloWaveNet，实现了高语音质量和快速生成
![](image/Pasted%20image%2020230129103954.png)
#### 基于 GAN 的声码器

包括：![](image/Pasted%20image%2020230129104033.png)

从生成器角度：大多数都使用 dilated convolution 来增加感受野以模拟波形序列中的长期依赖，使用转置卷积（反卷积）对 Mel 谱进行上采样以匹配波形序列长度。

判别器角度：主要集中于如何设计模型来捕获波形的特征：
+ GAN-TTS 使用随机窗口判别器
+ MelGAN 提出多尺度判别器，每个尺度聚焦于不同频率范围的特征
+ HiFiGAN 提出多周期判别器，通过观察不同周期中输入音频的不同部分来捕获不同的隐式结构
+ VocGAN 使用层级判别器，以从粗粒度到细粒度的不同分辨率判断生成的波形

从损失函数角度：除了常规的 GAN 损失，也使用了如 STFT 损失、特征匹配损失等来提高对抗训练的稳定性和训练效率。

#### 基于 Diffusion 的声码器
> 最近兴起的工作，用扩散过程和逆过程建立数据与潜在分布之间的映射

优点在于可以生成非常高质量的语音，但迭代过程漫长，生成速度慢。

常见的模型有：DiffWave、WaveGrad、PriorGrad

#### 其他声码器

使用基于神经网络的 source-filter 模型来进行可控的高质量语音生成。

#### 总结

![](image/Pasted%20image%2020230129110024.png)
+ 在数学简单性方面，基于自回归（AR）的模型比其他生成模型（如VAE、Flow、Diffusion和GAN）更容易
+ 除AR之外的所有生成模型都可以支持并行生成
+ 除AR模型外，所有生成模型都可以在某种程度上支持潜在变量
+ 基于GAN的模型无法估计数据样本的似然，而其他模型有这优势


### 全端到端的 TTS

直接从字符或者音素序列生成语音波形，优点为：
+ 需要较少的人工注释（如对齐信息）和特征
+ 联合优化可以避免传播误差
+ 降低训练、开发和部署成本

端到端 TTS 的发展：![](image/Pasted%20image%2020230129113536.png)

stage 0：统计参数合成使用三个基本模块进行语音合成

stage 1：在统计参数合成将文本分析和声学模型结合到端到端声学模型中，该模型直接从音素序列生成声学特征，然后使用声码器生成波形

stage 2：WaveNet 从 语言特征直接生成语音波形（但是输入为 Mel 谱啊？？），可以被视为声学模型和声码器的组合，但仍需文本分析模块

stage 3：Tacotron 使用 encoder-attention-decoder 模型直接预测字符/音素的线性谱，并使用Griffin Lim将线性谱转换为波形，其他如 DeepVoice 3、Tacotron 2 等先从字符/音素预测Mel谱，然后使用神经声码器生成波形

stage 4：直接从文本到波形：
+ Char2Wav 利用基于 RNN 的 encoder-attention-decoder 模型从字符中生成声学特征，然后使用 SampleRNN 生成波形，两个模型联合优化
+ ClariNet 联合优化 自回归声学模型和非自回归声码器，从而直接生成波形
+ FastSpeech 2s 直接从具有完全并行结构的文本生成语音
+ EATS 直接从字符/音素生成波形，利用持续时间插值和软动态时间包裹损失进行端到端对齐学习
+ Wave Tacotron 在 Tacotron 上构建了一个基于流的解码器，以直接生成波形

### 其他分类法

1. 自回归非自回归
2. 不同的生成模型
3. 不同的网络结构，如 CNN、RNN、CNN+RNN、self-attention 等
具体分类见原论文。

整个 TTS 的发展流程图（按照arxiv上的时间，截止到 2021，论文中有一个更详细的表格）：![](image/Pasted%20image%2020230129115144.png)

## TTS 进阶

### 快速 TTS

早期 TTS 采用自回归方法，一些改进的方法有：
+ 采用非自回归方法并行生成 mel 谱和波形
+ 轻量、高效的模型架构
+ 利用语音领域知识进行快速 TTS

#### 并行生成
![](image/Pasted%20image%2020230725193909.png)
+ 基于 RNN 的模型在训练和推理时都很慢，$N$ 为序列长度
+ DeepVoice 3 和 TransformerTTS 采用 CNN 和 自注意力架构实现并行训练，但是推理的时候仍是自回归的
+ FastSpeech 1/2 使用 feed-forward Transformer 同时实现了并行训练和推理
+ 大部分基于 GAN 的模型都是非自回归的 
+ Parallel WaveNet 和 ClariNet 利用反向自回归流，实现并行推理但是需要 teacher distillation
+ WaveGlow 和  FloWaveNet 使用 generative flow 进行并行的训练和推理，但是需要进行 $T$ 次迭代来确保生成质量
+ 基于 diffusion 的模型也需要多次迭代

#### 轻量模型
一些常用的技术包括：剪枝、量化、知识蒸馏、神经架构搜索（NAS）
+ WaveRNN 采用 dual softmax、weight pruning、subscale predication 加速推理
+ LightSpeech 采用 NAS 来寻找轻量的架构加速 FastSpeech2 的推理
+ SqueezeWave 利用 波形 reshaping 来减少时间长度，并且采用 depthwise separable convolution 替换 1D 卷积来减少计算量
+ 也有采用 tensor decomposition 来压缩模型的
+ DeviceTTS 使用  DFSMN 架构和 mix-resolution decoder 在一个 step 中预测多帧
+ 还有使用 半自回归生成的

#### 用 Domain Knowledge 加速
Domain knowledge 可以用来加速推理，如：线性预测、多子带建模、subscale predication、multi-frame prediction、streaming synthesis 等
+ LPCNet 把 DSP 和神经网络组合，采用 LPC 计算下一个波形，采用 轻量模型计算残差
+ 子带建模将波形分成多个自带来加速推理，典型的模型包括：DurIAN、multi-band Me-GAN、subband WaveNet 和 multi-band LPCNet
+ Bunched LPCNet 采用 sample bunching 和 bit bunching 来减少计算复杂度
+ Streaming TTS 流式合成
+ FFTNet 使用网络架构来模拟 FFT，可以实时生成音频
+ 有人提出 frame splitting 和 cross-fading 来合成部分波形，然后将合成的波形进行拼接
+ 还有通过 network reduction 和 fidelity improvement 技术来加速 DCTTS

### 低资源 TTS
构建高质量的 TTS 需要大量的成对文本语音数据，大部分的语言都缺乏数据。一些典型的低资源 TTS 技术如下：
+ 自监督训练：未成对的数据更容易获取。TTS 中的文本编码器可以可以用 BERT 增强；语音也可以被量化为离散的 token 序列，从而可以看成是伪的成对数据
+ 跨语言迁移：人类语言有相似的发声器官、发音等。但是高资源和低资源语言有不同的 phoneme set，于是有人提出映射不同语言的 embedding；也可以采用国际音标或字节表征来支持任意文本的音素
+ 跨说话人迁移：其他说话人的数据可以提高某个特定说话人的合成质量（如通过 voice conversion 实现），从而增加数据量。
+ Speech chain/Back transformation：结合 TTS 和 ASR 来获取数据
+ 数据挖掘

### 鲁棒 TTS
好的 TTS 系统应该确保生成正确的语言（和文本相对应的），鲁棒性问题包括：word skipping, repeating 和 attention collapse，造成这些问题的原因可以分为两类：
+ 字符/音素和 mel 谱 之间的对齐困难（不知道一个 字符或者音素应该对应几帧）
+ 自回归生成时的 exposure bias 和 error propagation 问题

但是声码器不会有严重的鲁棒性问题，因为声学特征和波形已经在帧级对齐（一帧对应固定长度的波形点）。

针对对齐问题现有工作分为两个方面：
+ 增强注意力机制的鲁棒性
+ 不用注意力机制，而是预测持续时间

针对 exposure bias 和 error propagation 问题：
+ 改善自回归生成
+ 不用自回归生成，而是使用非自回归生成

一些常用的技术如下：
![](image/Pasted%20image%2020230728113207.png)

#### 注意力增强
自回归声学模型中，大部分的 word skipping/repeating 和 attention collapse 问题都来自于不正确的注意力对齐。因此需要考虑 字符/音素序列和 mel 谱序列的对齐：
+ 位置性：一个字符/音素 可以与一个或多个连续的 mel 帧对齐，而一个 mel 帧只能和单个 字符/音素 对齐
+ 单调性：如果 A 在 B 后面，则 A 所对应的 mel 谱也在 B 所对应的后面
+ 完整性：每个 字符/音素 必须至少覆盖一个 mel 谱的帧，从而避免 word skipping

一些技术如下：
![](image/Pasted%20image%2020230728161044.png)
+ 基于内容的注意力：早期的 TTS 的注意力都是基于内容的
+ Location-based attention：利用位置信息进行对齐，如 Char2Wav、VoiceLoop 和 MelNet 
+ Content/Location-based hybrid attention：结合基于内容和位置的注意力的优势，提出了位置敏感注意力，在计算当前注意力对齐时，使用先前的注意力对齐
+ Monotonic attention：注意力位置是单调增加的
+ Windowing or off-diagonal penalty
+ Enhancing encoder-decoder connection
+ Positional attention

#### 预测持续时间
完全消除编码器-解码器的注意力，转而预测每个 字符/音素 的持续时间，并根据持续时间扩展文本隐序列以匹配 mel 谱序列的长度。

有两种方法：
+ 使用外部的对齐工具或者联合训练来得到持续时间
+ 以端到端的方式优化持续时间的预测器，或在训练时使用 GT 持续时间，在推理时预测持续时间

![](image/Pasted%20image%2020230728161826.png)
+ 外部对齐：根据使用的外部工具不同又可以分成几类：
	+ Encoder-decoder attention：FastSpeech 从自回归声学模型的注意力对齐中获得持续时间；SpeedySpeech 和 FastSpeech 差不多，但是使用的是 CNN 网络
	+ CTC 对齐：采用基于 CTC 的 ASR 模型来对齐 音素和 mel 谱序列
	+ HMM 对齐：FastSpeech 2 利用基于 HMM 的 Montreal forced alignment 获得持续时间
	+ 其他工作如  DurIAN， RobuTrans， Parallel Tacotron， and Non-Attentive Tacotron 使用强制对齐或者语音识别工具
+ 内部对齐：
	+ AlignTTS 遵循FastSpeech的基本模型结构，但利用基于 dp 的方法，通过多阶段训练来学习文本和 mel 谱序列之间的对齐
	+ JDI-T 遵循FastSpeech从自回归教师模型中提取持续时间，但联合训练自回归和非自回归模型，不需要两阶段训练
	+ Glow-TTS 利用一种新颖的单调对齐搜索来提取持续时间
	+ EATS 利用插值和 soft dynamic time warping 损失以端到端的方式优化持续时间预测
+ 非端到端优化：典型的持续时间预测方法通常使用从 外部/内部对齐工具 获得的持续时间进行训练，并使用预测的持续时间来进行推理。预测的持续时间不是通过接收来自mel谱损失的引导信号（梯度）来端到端优化的（没看懂）
+ 端到端优化：
	+ 为了联合优化持续时间以实现更好的韵律，如前面说的 EATS
	+ Parallel Tacotron 2 遵循EATS，以确保可微分的持续时间预测
	+ Non-Attentive Tacotron 提出了一种用于持续时间预测的半监督学习，其中，如果没有可用的持续时间标签，则预测的持续时间可以用于上采样

#### 改善自回归生成
自回归生成通常存在 exposure bias 和 error propagation 问题。

一些缓解此问题的研究：
+ 使用 professor forcing 来缓解真实数据和预测数据之间的不匹配
+ 使用 teacher-student distillation 

而由于 error propagation，mel 谱图右边生成的质量通常比左边更差，一些工作利用从左到右和从右到左的生成来进行数据增强和正则化。

#### 用非自回归替代自回归
根据注意力的使用或持续时间预测，非自回归生成可以分为两部分：
+ 并行生成中使用位置注意力进行文本和语音对齐
+ 用持续时间预测来弥补文本和语音序列之间的长度不匹配

一种新的分类：![](image/Pasted%20image%2020230729111329.png)

### 表达性 TTS
合成语音的自然度很大程度取决于合成语音的表现力，包含内容、音色、韵律、情感、风格等多种特征。

表达性语音合成的一个关键是处理一对多映射问题，即在持续时间、音高、音量、说话人风格、情感等方面，同一文本对应了多个语音。在没有这些额外信息时，直接建模会导致过度平滑的 mel 谱，从而导致低质量和低表现力的语音。因此将这些信息作为输入可以提高合成语音的表现力。

#### 信息分类
合成语音所需要的信息可以分为四类：
+ 文本信息：就是字符或者音素，即语音的内容
+ 说话人或音色信息：一些多说话人系统会显示建模说话人表征（speaker embedding）
+ 韵律、风格和情感信息：包含语调、重音、节奏等， 是提高语音表达力的关键信息，也是大部分的研究重点
+ 录音设备或噪声环境：会影响语音的质量，研究主要涉及解耦、可控和降噪

#### 信息建模
提出了许多方法来对不同粒度的不同类型的信息进行建模：
![](image/Pasted%20image%2020230729141713.png)

可以分为显示信息和隐式信息。

对于显示信息，可以直接作为输入来提高模型生成语音的表达力，包括如语言 ID、说话人 ID、风格、韵律等。

对于隐式信息则需要进行建模，方法有：
+ Reference encoder：将韵律定义为语音信号中的变化信息，通过参考编码器对韵律进行建模
+ VAE：利用VAE以高斯先验作为正则化对潜在空间中的方差信息进行建模，这可以实现对合成风格的建模和控制
+ Advanced generative models：使用 advanced generative models 来隐式学习变化信息，这可以更好地对多模态分布进行建模
+ 文本预训练：可以通过使用预训练的 word embedding 或模型参数来提供更好的文本表征

#### 信息粒度
信息可以以不同的粒度进行建模：
+ 语言级别和说话人级别
+ 段落级别
+ Utterance level：从参考语音中提取单个隐向量，以表示该话语的音调/风格/韵律
+ Word/syllable level：对话语级别信息无法覆盖的细粒度风格/韵律信息进行建模
+ Character/phoneme level：如持续时间、音高、韵律
+ Frame level

覆盖不同粒度的层次结构有助于表达性的合成。

#### 解耦、控制和迁移
![](image/Pasted%20image%2020230729143341.png)

使用对抗训练进行解耦：当多种风格或韵律信息纠缠在一起时，有必要在训练中进行解耦：
+ 有通过 对抗-协作 增强了内容风格的解耦力和可控性
+ 也有利用VAE框架和对抗训练，将噪声从说话人信息中分离出来
+ 还有人提出了使用三种瓶颈重建来理清节奏、音高、内容和音色
+ 还有提出通过帧级噪声建模和对抗训练来从说话人中分离噪声

循环一致性/反馈损失来实现可控：
+ 有过添加具有 feedback cycle 的情绪风格分类器来进行可控的情绪迁移，其中分类器促使 TTS 模型合成具有特定情绪的语音
+ 也有使用风格分类器获得 feedback loss，实现给定风格的语音合成。同时结合了不同风格分类器之间的对抗学习，以确保从多个参考音频中保留不同风格
+ 还有使用 ASR 提供 feedback loss 来训练不匹配的文本和语音，目的是减少训练和推理之间的不匹配，因为在推理中使用随机选择的音频作为参考
+ 其他论文利用 feedback loss 来确保风格和 speaker embedding 等方面的可控性

半监督学习实现可控：如果有每个属性的标签，可以很容易地控制合成语音，然而，当没有可用的标记/标签时就很困难，而当部分标签可以用时：
+ 有人提出半监督学习方法来学习VAE模型的潜在表征，以控制情感或说话速率等
+ 当没有标签可用时，提出了高斯混合 VAE 模型来区分不同的属性，也有利用梯度反转或对抗训练来区分说话人音色和噪声，从而为有噪声的说话人 合成干净的语音

改变信息用于迁移：可以通过将信息转换为不同的风格来迁移合成语音的风格，可以从一下三种方式获得信息：
+ 从参考语音中提取
+ 从文本进行预测
+ 从潜在空间进行采样

### 自适应 TTS
自适应 TTS 可以合成任意说话人的语音，也被称为 voice adaptation、voice cloning、custom voice 等。源TTS模型（通常在多说话人语音数据集上训练）对每个目标语音进行自适应，而自适应数据很少。从两个角度分析：
+ General Adaptation：改进源 TTS 模型使其支持新的说话人
+ Efficient Adaptation：减少每个目标说话人的适应数据和参数

![](image/Pasted%20image%2020230729153101.png)

#### General Adaptation

Source Model Generalization：提高源模型的泛化能力。由于训练源模型时的数据有限，TTS 模型很容易对训练数据过拟合，因而对新的说话人泛化能力较差，一些解决方法有：
+ 声学条件建模以提供必要的声学信息作为模型输入，从而更好地学习文本到语音的映射
+ 增加训练数据的数量和多样性

Cross-Domain Adaptation：自适应语音与用于训练源TTS模型的语音数据具有不同的声学条件或风格，因此需要考虑特殊的设计来提高源TTS模型的泛化能力，支持目标说话者的风格：
+ AdaSpeech 进行声学条件建模，以更好地建模声学条件，如录音设备、环境噪声、口音、说话人语速、音色等。模型倾向于泛化而不是记忆声学条件，可以很好地适应不同声学条件的语音数据
+ AdaSpeech 3 设计特定的 filled pauses adaptation, rhythm adaptation, and timbre adaptation，使阅读风格的 TTS 模型适应自发风格
+ 其他工作考虑了跨说话风格的自适应，如跨语言

#### Efficient Adaptation
更多的自适应数据将导致更好的语音质量，但会产生较高的数据收集成本。

对于自适应参数，可以对整个TTS模型、或部分模型（例如，解码器）、或仅 speaker embedding 进行微调。微调的参数越多，产生的语音质量越好，但会增加内存和部署成本。

目标是尽可能减少数据和参数以实现高质量的语音，可以分为以下几种方面的技术：
+ few data adaption
+ few parameter adaption
+ untranscribed data adaptation
+ zero-shot adaptation

Few data adaptation：仅使用少量配对文本和语音数据进行自适应，研究发现，当数据量较小（少于20句）时，语音质量随着适应数据的增加而快速改善，而当适应语句数为数十句时，语音质量则缓慢改善

Few parameter adaptation：在保持自适应质量的同时，尽可能少地减少自适应参数

Untranscribed data adaptation.：很多情况只有语音数据而没有配对的文本：
+ AdaSpeech 2 在语音重建和潜在对齐的帮助下利用未记录的语音数据进行语音自适应
+ 也有使用ASR模型来转录语音数据，使用转录的配对数据进行语音自适应

Zero-shot adaptation：其利用 speaker encoder 来提取给定参考音频的 speaker embedding，不需要额外的数据和参数就可以实现自适应，但是效果不太好

## 未来研究方向

主要分为两类
1. 高质量语音合成
	1. 更强大的生成模型
	2. 更好的表征学习：如一些文本预训练模型
	3. 鲁棒语音合成：针对长文本、不同语言的文本等
	4. 表达性/可控/可迁移语音合成
	5. 更像人的语音合成：随意性、有感情、自发性，而不是听起来很刻意的对话
2. 高效语音合成，降低合成成本，包括：数据、训练、部署成本等
	1. 数据高效
	2. 参数高效
	3. 能量高效