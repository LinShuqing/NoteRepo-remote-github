> EURASIP Journal on Audio, Speech, and Music Processing，2024，Ozyegin University
<!-- 翻译&理解 -->
<!-- Speech synthesis has made significant strides thanks to the transition from machine learning to deep learning mod- els. Contemporary text-to-speech (TTS) models possess the capability to generate speech of exceptionally high qual- ity, closely mimicking human speech. Nevertheless, given the wide array of applications now employing TTS models, mere high-quality speech generation is no longer sufficient. Present-day TTS models must also excel at producing expressive speech that can convey various speaking styles and emotions, akin to human speech. Consequently, researchers have concentrated their efforts on developing more efficient models for expressive speech synthesis
in recent years. This paper presents a systematic review of the literature on expressive speech synthesis models pub- lished within the last 5 years, with a particular emphasis on approaches based on deep learning. We offer a compre- hensive classification scheme for these models and provide concise descriptions of models falling into each category. Additionally, we summarize the principal challenges encountered in this research domain and outline the strategies employed to tackle these challenges as documented in the literature. In the Section 8, we pinpoint some research gaps in this field that necessitate further exploration. Our objective with this work is to give an all-encompassing over- view of this hot research area to offer guidance to interested researchers and future endeavors in this field. -->
1. 本文给出了最近五年内发表的关于表达性语音合成模型的文献综述，重点在基于深度学习的方法
2. 提供了一个全面的分类方案，且对不在每个类别中的模型进行了详细描述
3. 总结了这个研究领域遇到的主要挑战，并概述了文献中用于解决这些挑战的策略
4. 在第 8 节中，指出了这一领域的一些研究空白，可以进一步探索的点

## Introduction
<!-- Since the late 1950s, computer-based text-to-speech systems (TTS) have undergone significant advance- ments [1], culminating in the production of models that generate speech almost indistinguishable from that of a human. This progress has followed a path consisting of several stages, beginning with conven- tional methods named as concatenative synthesis and progressing to more advanced approaches known as statistical parametric speech synthesis (SPSS). Advanced approaches are mainly based on machine learning algo- rithmslikehiddenMarkovmodels(HMMs)andgaussian mixture models (GMMs). Despite this progress, speech generated by these methods was still noticeably artificial. However, the emergence of deep learning (DL) as a new branch under machine learning (ML) in 2006 has led to significant improvements. Speech synthesis researchers, like many in other research fields, started incorporating deep neural networks (DNN) in their models. Initially, DNNs replaced HMMs and GMMs in SPSS models while the main structure still follows the primary framework of SPSS models as shown in Fig. 1. As discussed in [2], the deep learning-based models have overcome many limi- tations and problems associated with machine learning- based models. -->
1. 早期的 TTS 模型主要还是 statistical parametric speech synthesis (SPSS)，其基本结构如图：![](image/Pasted%20image%2020240401155020.png)
<!-- Researchers continue to aim for improved speech qual- ity and more human-like speech despite past advance- ments. Additionally, they seek to simplify the framework of the text-to-speech models due to the intricate nature of the SPSS structure, which limits progress in this field to those with extensive linguistic knowledge and exper- tise. Deep learning advancements have brought about the simple encoder-decoder structure for TTS models as sequence-to-sequence (Seq2Seq) approaches. The pro- posed approaches have simplified the structure of con- ventional TTS with multiple components into training a single network that converts a set of input text char- acters/phonemes into a set of acoustic features (mel- spectrograms). A main concern in these advanced TTS models is the mapping process between the input and output sequences, which is a one-to-many problem, as the single input text can have multiple speech variations as output. In fact, there are two groups of recent TTS models, as shown in Fig. 2. The first group generates mel- spectrograms in a sequential (autoregressive) manner using soft and automatic attention alignments between input and output sequences, such as the Tacotron model [3, 4]. The second group utilizes hard alignments between the phonemes/characters and mel-spectro- grams, and thus its speech generation process is parallel (non-autoregressive), as in the FastSpeech model [5, 6]. This improvement in the structure of the TTS model has encouraged rapid development in the field within the last few years, during which the proposed models produced speech that is nearly indistinguishable from human speech. -->
2. 高级一点的 TTS 的一个主要 concern 是输入和输出序列之间的一对多映射问题，因为单个输入文本可以有多个输出的语音变化。最近的 TTS 模型分为两组：
    1. 在文本和输出之间采用 soft 和 automatic attention alignments 自回归生成 mel-spectrograms，如 Tacotron 模型
    2. 在音素/字符和 mel-spectrograms 之间采用 hard alignments，语音生成过程是并行的，如 FastSpeech 模型
<!-- Human speech is highly expressive and reflects vari- ous factors, such as the speaker’s identity, emotion, and speaking style. In addition, there are many applications in which speech synthesis can be utilized, especially expres- sive speech synthesis. For instance, audiobooks and pod- cast applications that create audio versions of eBooks and podcasts, translation applications which provide real-time translation of foreign language text, dubbing applications that generate an alternative audio track for a video with different content, speaker, or language, and content creation applications which help produce audio versions of textual content, such as blogs and news arti- cles. E-learning applications that allow for adding voice- over audio to e-learning courses, and conversational AI applications enable machines to communicate with users in a human-like manner, such as AI chatbots and virtual assistants. -->
3. 人类语音具有高度表现力，反映了说话人身份、情感和说话风格。表达性语音合成有很多应用场景
<!-- As spoken language is a crucial component in such applications, users must feel as if they are communicating with a real human rather than a machine. Therefore, the speech generated by these applications should convey appropriate emotion, into- nation, stress, and speaking style to match the ongoing conversation or the content type and context of the text being read. -->
4. 合成的语音应该传达情感、语调、重音和说话风格，以匹配正在进行的对话或文本和上下文
<!-- As a result, there has been a recent attention towards building efficient expressive speech synthesis models as another step forward in achieving human-like speech. Therefore, many studies have been devoted to expressive speech synthesis (ETTS) as a hot research area, particu- larly over the last 5 years. In this work, we present the findings of our systematic literature review on ETTS field from the past 5 years. Firstly, we suggest a classifi- cation schema of deep learning-based ETTS models that are proposed during this period, based on structures, and learning methods followed in each study. A sum- mary is then provided for each category in the classifi- cation schema and main papers related to this category. After that, we outline the main challenges in the ETTS area and solutions that have been proposed to solve them from literature. Finally, we conclude with a discussion of the implications of our work and a highlight of some gaps that require further research in this area. -->
5. 本文综述了过去 5 年的表达性语音合成模型的文献，给出了基于深度学习的 ETTS 模型的分类方案，总结了每个类别的主要论文，概述了 ETTS 领域的主要挑战和解决方案，最后讨论一些需要进一步研究的空白
<!-- During our work on this review of expressive speech synthesis literature, we came across several review papers that focus on different stages of development in the speech synthesis field. The majority of these reviews concentrate on DL-based TTS approaches [7–13], while only a few papers [13, 14] cover recent TTS approaches in addition to early conventional ones. However, to the best of our knowledge, there are no review papers that cover the fast growth in the (expressive) speech synthesis area, especially in the last few years. Therefore, our main goal in this review is to provide an overview of research trends, techniques, and challenges in this area during this period. We hope that our work will offer researchers a comprehensive understanding of how and what has been accomplished in this field and the gaps that need to be filled as guidance for their future efforts. -->
<!-- While we were writing this paper, we came across an interesting recent review paper [15] that is similar to our work. However, the review in [15] covers emotional speech synthesis (ESS) as a sub-field of voice transfor- mation while our work is more comprehensive as a sys- tematic literature review that discusses approaches, challenges, and resources. Furthermore, the taxonomy we provide for the reviewed approaches differs from the one given in [15] as elaborated in the next section. -->
<!-- The remaining sections of this paper are structured as follows: Section 2 provides an explanation of the meth- odology employed for conducting this review. Sections 3 and 4 describe the different main and sub-categories of the proposed classification schema for DL-based expres- sive TTS models. Main challenges facing ETTS mod- els and how they have been tackled in the literature are covered in Section 5. We then give a brief description of ETTS datasets and applied evaluation metrics in Sec- tions 6 and 7, respectively. Finally, Section 8 concludes the paper. -->

## 方法
<!-- The last few years have seen rapid growth in expressive and emotional speech synthesis approaches, resulting in a large number of papers and publications in this area. Here, we present the outcomes of a systematic literature review of the last 5 years’ publications within this active research area. This section describes the methodology used to conduct the review, illustrated by Fig. 3, which consists of three main stages: paper selection, paper exclusion, and paper classification. -->
本节描述了进行综述的方法，包括三个主要阶段：论文选择、论文排除和论文分类。

### 论文选择
<!-- For our review, we used the Scopus [16] database to retrieve papers as it encompasses most of the significant journals and conferences pertaining to the speech syn- thesis field. Our query criteria to find relevant papers on Scopus were twofold: (1) the paper title must include at least one of four words (emotion* OR expressive OR prosod* OR style) that denote expressive speech, and (2) the paper title, abstract, or keywords must comprise the terms “speech” AND “synthesis,” in addition to at least one of the above-mentioned words for expressive speech. We considered all papers written in English and pub- lished in journals or conferences since 2018. The search query was conducted in January 2023, and it yielded 356 papers. Scopus provides an Excel file containing all the primary information of the retrieved papers, which we used in the second stage of our review. -->
使用 Scopus 数据库检索论文，检索条件为：
    1. 论文标题至少包含 emotion 或者 expressive 或者 prosod 或者 style 中的一个词
    2. 论文标题、摘要或关键词包含 "speech" 且 "synthesis"，并且至少包含上述表达性语音的一个词

只考虑英文论文，时间从 2018 年开始，检索到 356 篇论文。

### 论文排除
<!-- The exclusion of papers occurred in two phases. In the first phase, we screened the abstract text, while in the second phase, we screened the full text of the paper. Five main constraints were used to exclude papers, includ- ing (1) papers that were not related to the TTS field, (2) papers that were not DL-based models, (3) papers that did not focus on expressive or emotional TTS models, (4) papers that were too specific to non-English languages, and (5) papers that lacked details about the applied method. After screening the paper abstracts, we excluded 180 papers, mostly based on the first exclusion criterion. During the second exclusion phase, in which we read the full text of each paper, we identified another 65 papers that met at least one of the five exclusion criteria. Con- sequently, 111 papers were included in the third stage of our review. Additionally, a group of recently published papers in this area [17–25] was hand-picked and added to the final set of selected papers. While most of the reviewed papers trained their models on English data, a few other papers used data in other languages as listed in Table 1. -->
排除论文的两个阶段：
    1. 检查摘要
    2. 检查全文     

排除的主要约束条件有：
    1. 与 TTS 领域无关
    2. 非基于深度学习的模型
    3. 不关注表达性或情感 TTS 模型
    4. 针对非英语语言的论文
    5. 缺乏关于应用方法的细节

筛选后剩下 111 篇论文，另外手动添加了一些最近发表的论文。

一些不使用英语数据的论文：
![](image/Pasted%20image%2020240401214435.png)

### 论文分类
<!-- After summarizing the approach proposed for generat- ing expressive speech in each selected paper, we catego- rized the papers based on the learning approach applied in each one. Accordingly, papers are divided into two main categories, including supervised and unsuper- vised approaches. Under the supervised category, where labeled data is utilized, we identified three subcategories based on how models are employed expressive speech synthesis. The three proposed subcategories are (1) labels as input features, (2) labels as separate layers or models, and (3) labels for emotion predictors/classifiers. -->
根据论文中的学习方法对论文进行分类，可以分两大类：有监督和无监督。在有监督类别下，根据模型如何应用于表达性语音合成，又可以分为三类：
    1. 把标签作为输入特征
    2. 把标签作为独立层或模型
    3. 情感预测器/分类器的标签

<!-- Papers in the unsupervised approaches category are grouped into four different subcategories based on the main structure or method used for modeling expressivity in these papers. From our observation, most of the proposed meth- ods in the last 5 years are based on three main early works in this field, namely, reference encoder [74], global style tokens [75], and latent features via variational autoencoders (VAE) [76, 77]. Specifically, proposed models in most of the papers under this category can be considered as an extension or enhancement of one of the three previously mentioned methods. Besides, we identify a fourth subcategory that includes the recent TTS models representing the new trend in the TTS area, which utilizes in-context learning. There is one factor common to all these four unsupervised models, which is that they are all based on using an audio reference/ prompt. Additionally, we added a fifth subcategory (named other approaches) in which we include approaches outside the previous four main unsupervised approaches. Figure 4 illustrates the proposed classification schema for the DL- based expressive speech synthesis models. -->
根据建模表达性的结构或方法，无监督的论文分为四个子类：
    1. 参考编码器（reference encoder）
    2. 全局风格标记（global style tokens）
    3. 通过变分自动编码器（VAE）获得的潜在特征（latent features via VAE）
    4. 最近的 TTS 模型，使用上下文学习（in-context learning）
> 上述四种无监督模型的共同点是都基于音频参考/提示。

另外，添加了一个其他方法的子类，包括前四种主要无监督方法之外的方法。

## 有监督方法
<!-- Supervised approaches refer to models that are trained on datasets with emotion labels. Those labels guide model training, enabling it to learn accurate weights. Early deep learning-based expressive speech synthesis systems were primarily supervised models that utilized labeled speech exhibiting various emotions (such as sad- ness, happiness, and anger) or speaking styles (such as talk-show, newscaster, and call-center). Note that the term style has also been used to refer to a set of emotions or a mixture of emotions and speaking styles [59, 68, 78, 79]. Generally, the structure of early conventional TTS models was built upon two primary networks: one for predicting duration and the other for predicting acoustic features. These acoustic features were then converted to speech using vocoders. Both networks receive linguis- tic features extracted from the input text. In supervised ETTS approaches, speech labels (emotions and/or styles) are represented in the TTS model as either input features or as separate layers, models, or sets of neurons for each specific label. The following sections explain these three representations in detail then we provide a general sum- mary of the supervised approaches reviewed in this work in Table 2. -->
有监督方法是指在带有情感标签的数据集上训练的模型。早期的基于深度学习的表达性语音合成系统主要是有监督模型，利用带有各种情感（如悲伤、快乐和愤怒）或说话风格（如脱口秀、新闻播报员和呼叫中心）的标记语音。

早期传统 TTS 模型的结构基于两个网络：一个用于预测持续时间，另一个用于预测声学特征。然后使用声码器将声学特征转为语音。两个网络的输入都是文本中提取的语言特征。

标签（情感和/或风格）在 TTS 模型中，可以是输入特征，也可以作为单独的层、模型或每个特定标签的一组神经元。

概括表格如下：
![](image/Pasted%20image%2020240401221757.png)

## 无监督方法
<!-- Due to the limited availability and challenges associated with collecting or preparing labeled datasets of expres- sive speech, as discussed in Section 6, many researchers tend to resort to unsupervised approaches for generat- ing expressive speech. Within these approaches, models are trained to extract speaking styles or emotions from expressive speech data through unsupervised methods. Unsupervised models typically utilize reference speech as an input to the TTS model, which extracts a style or prosody embedding which is then used to synthesize speech resembling the input style reference. In the lit- erature, three primary structures emerge as baseline models for unsupervised ETTS models: including refer- ence encoders, global style tokens, and variational auto- encoders, which are explained in the following three sections. In addition, we identify the recent TTS mod- els that utilize in-context learning as another group of unsupervised approaches. The last subcategory under the unsupervised approaches involves other individual approaches. We then provide a general summary of all the unsupervised approaches reviewed in this work in Table 3. -->
由于表达性语音的标记数据集有限，许多研究倾向于使用无监督方法生成表达性语音。模型通过无监督方法语音数据中提取说话风格或情感。无监督模型通常使用参考语音作为 TTS 模型的输入，用于提取风格或韵律嵌入，然后用于合成类似于输入风格参考的语音。

无监督 ETTS 模型有三种主要结构：
+ 参考编码器
+ 全局风格标记
+ 变分自动编码器
+ 还有一种使用上下文学习的 TTS 模型

概括表格如下：
![](image/Pasted%20image%2020240401222120.png)

### 直接参考编码
<!-- The main approach, based on a reference or prosody encoder, can be traced back to an early Google paper [74]. The paper suggests using a reference encoder to produce a low-dimensional embedding for a given style reference audio, which is called a prosody embedding. This encoder takes spectrograms as input to represent the reference audio. The generated prosody embedding is then concatenated with the text embedding derived from the text encoder of a Seq2Seq TTS model such as Tacotron [3, 4]. Figure 6 shows reference encoder inte- grated to the TTS model. -->
基于参考或韵律编码器最早可以追溯到 Google 的论文，提出使用 reference encoder 为给定的 style reference audio 生成低维 embedding，称为 prosody embedding。生成的 prosody embedding 与从 Seq2Seq TTS 模型（如 Tacotron）的文本编码器中导出的文本 embedding 拼接，如图：
![](image/Pasted%20image%2020240401222337.png)

<!-- Various features have been employed in the literature as inputs for the reference encoder. For example, in the
work [85], MFCC features extracted using the openS- MILE toolkit [139] are fed into one of the encoders within its style extraction model, which is composed of a multi-modal dual recurrent encoder (MDRE). In another study [31], the reference encoder is proposed as a rank- ing function model, aimed at learning emotion strength at the phoneme level. This model leverages the OpenS- MILE toolkit to extract 384-dimensional emotion-related features from segments of reference audio, derived using a forced alignment model for phoneme bounda- ries. Furthermore, in work [63], a word-level prosody embedding is generated. This is achieved by extracting phoneme-level F0 features from reference speech using the WORLD vocoder [140] and an internal aligner oper- ating with the input text. -->
各种论文中使用了不同的特征作为 reference encoder 的输入：
+ Interactive text-to-speech system via joint style analysis 使用 openSMILE 工具包提取的 MFCC 特征，然后送到其风格提取模型中的一个编码器中，该模型由 multi-modal dual recurrent encoder (MDRE) 组成
+ Fine-grained emotion strength transfer, control and prediction for emotional speech synthesis 中，reference encoder 作为 rank function 模型，用于学习 phoneme level 的情感强度，也使用 openSMILE 工具包从 reference auduo 中提取 384 维情感相关特征，然后根据 phoneme boundary 进行强制对齐
+ Fluenttts: Text-dependent fine-grained style control for multi-style tts 生成 word-level 的 prosody embedding，即从 reference speech 中使用 WORLD vocoder 和内部 aligner 提取 phoneme-level 的 F0 特征
<!-- A prosody-aware module is proposed in [37] which extracts other prosody-related features. The prosody- aware module consists of an encoder, an extractor, and a predictor. The encoder receives the three phoneme-level features including logarithmic fundamental frequency (LF0), intensity, and duration from the extractor as input and generates the paragraph prosody embedding with the assistance of an attention unit. Simultaneously, the predictor is trained to predict these features at inference time based on the input text embedding only. -->
+ Paratts: Learning linguistic and prosodic cross-sentence information in paragraph-based tts 提出了一个 prosody-aware 模块，用于提取其他与韵律相关的特征。其包含 encoder、extractor 和 predictor。encoder 从 extractor 接收三个 phoneme-level 特征（包括 F0、强度和持续时间）作为输入，用 attention 生成 paragraph prosody embedding。同时，predictor 在推理的时候只基于输入文本 embedding 预测这些特征。
<!-- In Daft-Exprt TTS model [118], the prosody encoder receives pitch, energy and spectrogram as input. The prosody encoder then uses FiLM conditioning layers [141] to carry out affine transformations to the inter- mediate features of specific layers in the TTS model. A slightly modified version of the FastSpeech2 [6] model is utilized in this work where the phoneme encoder, prosody predictor and the decoder are the conditioned components. The prosody predictor is similar to the variance adaptor of FastSpeech2 but without the length regulator, and it estimates pitch, energy and duration at phoneme-level. -->
+ Daft-Exprt TTS 模型中，prosody encoder 接收 pitch、energy 和 spectrogram 作为输入，然后使用 FiLM conditioning layers 对 TTS 模型中特定层的中间特征进行仿射变换。其使用 FastSpeech2 作为 backbone，但是 phoneme encoder、prosody predictor 和 decoder 引入了 condition。prosody predictor 类似于 FastSpeech2 的 variance adaptor，但没有 length regulator，其可以在 phoneme-level 上估计 pitch、energy 和 duration
<!-- A pre-trained Wav2Vec model [142] has also been uti- lized for extracting features from the reference waveform. These features serve as input to the reference encoders of the proposed Emo-VITS model [23], which integrates an emotion network into the VITS model [143] to enhance expressive speech synthesis. In fact, the emotion net- work in the Emo-VITS model comprises two reference encoders. The resulting emotion embeddings from these encoders are then combined through a feature fusion module that employs an attention mechanism. Wav2vec 2.0-derived features from the reference waveform in this work are particularly suitable for attention-based fusion and contribute to reducing the textual content within the resulting embeddings [23]. -->
+ 也有采用预训练的 Wav2Vec 模型从 reference waveform 中提取特征，作为 Emo-VITS 模型的 reference encoders 的输入。Emo-VITS 模型将 emotion network 集成到 VITS 模型中，以增强表达性语音合成。Emo-VITS 模型的 emotion network 包括两个 reference encoders，这些 encoders 生成的 emotion embeddings 通过一个 feature fusion module 进行融合

### 基于 VAE 的 latent feature
<!-- The goal of TTS models under this is to map input speech from the higher dimensional space to a well- organized and lower-dimensional latent space utilizing variational auto-encoders (VAEs) [146]. VAE is a genera- tive model that is trained to learn the mapping between observed data x and continuous random vectors z in an unsupervised manner. In detail, VAEs learn a Gaussian distribution denoted as the latent space from which the latent vectors representing the given data x can be sam- pled. A typical variational autoencoder consists of two components. First, the encoder learns the parameters of the z vectors (latent distribution), namely the mean μ(x) and variance σ2(x), based on the input data x. Second, the decoder regenerates the input data x based on latent vectors z sampled from the distribution learned by the encoder. In addition to the reconstruction loss between the model input and the data, variational autoencoders are also trained to minimize a latent loss, which ensures that the latent space follows a Gaussian distribution.-->
目标是利用 VAE 将输入语音从高维空间映射到低维 latent space。
<!-- Utilizing VAEs in expressive TTS models as shown by Fig. 7, allows for mapping the various speech styles within the given dataset to be encoded as latent vectors, often referred to as prosody vectors, within this latent space. During inference, these latent vectors can be sam- pled directly or with the guidance of reference audio from the VAE’s latent space. Furthermore, the latent vectors offer the advantage of disentangling prosody features, meaning that some specific dimensions of these vectors independently represent single prosody features such as pitch variation or speaking rate. Disentangled prosody features allow for better prosody control via manipulat- ing the latent vectors with different operations such as interpolation and scaling [77]. The two early papers, [76, 77], can be regarded as the baseline for latent feature- based approaches. The former study [76] introduces VAE within the VoiceLoop model [147], while the latter [77] incorporates VAE into Tacotron2 [4] as an end-to-end TTS model for expressive speech synthesis. -->
如图：
![](image/Pasted%20image%2020240401230821.png)

使用 VAE 可以将数据集中的各种语音风格编码为 latent vectors，通常称为 prosody vectors。在推理时，直接采样 latent vectors，或者从 VAE 的 latent space 中引导 reference audio 进行采样。latent vectors 也可以解耦 prosody 特征，即 vector 中的某些特定维度表示某个 prosody 特征，如 pitch 或语音速度。解耦得到的 prosody 特征可以通过不同操作（如插值和缩放）来控制 latent vectors 以更好地控制 prosody。
> 两篇论文： Expressive speech synthe- sis via modeling expressions with variational autoencoder 和 Learning latent representations for style control and transfer in end-to-end speech synthesis 可以看作是基于 latent feature 的方法的 baseline，前者在 VoiceLoop 模型中引入 VAE，后者在 Tacotron2 中引入 VAE

<!-- In the same direction of modeling the variation of the prosodic features in expressive speech, studies [109, 110] propose a hierarchical structure for the baseline vari- ational autoencoder, known as Clockwork Hierarchical Variational AutoEncoder (CHiVE). Both the encoder and decoder in the CHiVE model have several layers to cap- ture prosody at different levels based on the input text’s hierarchical structure. Accordingly, linguistic features are also used alongside acoustic features as input to the mod- el’s encoder. The model’s layers are dynamically clocked at specific rates: sentence, words, syllables, and phones. The encoder hierarchy goes from syllables to the sentence level, while the decoder hierarchy is in the reversed order. The CHiVE-BERT model in [110], differs from the main model in [109] as it utilizes BERT [148] features for input text at the word-level. Since the features extracted by the BERT model incorporate both syntactic and seman- tic information from a large language model, CHiVE- BERT model is expected to have improved the prosody generation. -->
同样是建模表达性语音中的韵律特征变化，还有一种基于 hierarchical 结构的 VAE ，称为 Clockwork Hierarchical Variational AutoEncoder (CHiVE)。CHiVE 模型的 encoder 和 decoder 都用多层来捕获不同 level 的 prosody。模型的 encoder 除了声学特征外，还用了语言特征。模型的层以特定的 rate 进行 dynamically clocked：sentence、word、syllable和 phoneme。encoder 从 syllables 到 sentence，decoder 则相反。CHiVE-BERT 在 word level 使用 BERT 特征。由于 BERT 模型提取的特征包含了大型语言模型的句法和语义信息，从而可以改进 prosody 生成。
<!-- Other studies [24, 53] propose Vector-Quantized Variational Auto-Encoder (VQ-VAE) to achieve dis- cretized latent prosody vectors. In vector quantization (VQ) [149], latent representations are mapped from the prosody latent space to a codebook of a limited num- ber of prosody codes. Specifically, during training, the nearest neighbor lookup algorithm is applied to find the nearest codebook vector to the output of the reference encoder and used to condition TTS decoder. To further improve the quality of latent prosody vectors and conse- quently the expressiveness of the generated speech, Diff- Prosody[24] proposes a diffusion-based VQ-VAE model. In the proposed model a prosody generator that utilizes a denoising diffusion generative adversarial networks (DDGANs) [150] is trained to generate the prosody latent vectors based only on text and speaker information. At inference time, the prosody generator is used to produce prosody vectors based on input text and with no need for an audio reference which improves both quality and speed of speech synthesis. -->
还有用 Vector-Quantized Variational Auto-Encoder (VQ-VAE) 来实现离散化的 latent prosody vectors。在 vector quantization (VQ) 中，latent representations 映射到有限数量的 prosody codes 的 codebook。在训练时，使用最近邻查找算法找到最接近 codebook vector 的向量，然后作为 TTS decoder 的 condition。为了进一步提高 latent prosody vectors 的质量，从而提高生成语音的表现力，DiffProsody 提出了基于扩散的 VQ-VAE 模型。prosody generator 使用 denoising diffusion generative adversarial networks (DDGANs) 进行训练，仅基于文本和说话人信息生成 prosody latent vectors。在推理时，prosody generator 根据输入文本生成 prosody vectors，无需 reference audio，从而提高了语音合成的质量和速度。
<!-- While most of the studies in this category follow the baseline model and use mel-spectrograms to represent the reference audio, other studies extract correlated pros- ody features as input to the VAE. For instance, frame- level F0, energy, and duration features are extracted from the reference speech as basic input for the hierar- chical encoder of the CHiVE model [109]. These same features are also used as input for the VAE encoder in work [35], but at the phoneme level. In work [68], multi- resolution VAEs are employed, each with acoustic and linguistic input vectors. The acoustic feature vectors for each encoder include 70 mel-cepstral coefficients, log F0 value, a voiced/unvoiced value, and 35 mel-cepstral anal- ysis aperiodicity measures. -->
大多数方法都使用 mel-spectrograms 来表示 reference audio，也有从 reference speech 中提取相关的 prosody 特征作为 VAE 的输入。如 CHiVE 从 reference speech 中提取 frame-level 的 F0、energy 和 duration 特征作为输入。Discourse-level prosody modeling with a variational autoencoder for non-autoregressive expressive speech synthesis 也用了这些特征作 VAE encoder 的输入，但是是在 phoneme level。Hierarchical multi-grained generative model for expressive speech synthesis 中，使用 multi-resolution VAEs，每个 encoder 都有 acoustic 和 linguistic input vectors。每个 encoder 的 acoustic feature vectors 包括 70 mel-cepstral coefficients、log F0 value、voiced/unvoiced value 和 35 mel-cepstral analysis aperiodicity measures。

### 全局风格标记（GST）
<!-- The Global Style Tokens (GST) approach for expressive synthesis was first introduced in [75]. The paper proposes a framework to learn various speaking styles (referred to as style tokens) in an unsupervised manner within an end-to-end TTS model. The proposed approach can be seen as a soft clustering method that learns soft style clusters for expressive styles in an unlabeled dataset. In detail, GST, as shown by Fig. 8, extends the approach introduced in [74] by passing the resulting style embed- ding from the reference encoder to an attention unit, which functions as a similarity measure between the style embedding and a bank of randomly initialized tokens. During training, the model learns the style tokens and a set of weights, where each style embedding is generated via a weighted sum of the learned tokens. In fact, the obtained weights represent how each token contributes to the final style embedding. Therefore, each token will represent a single style or a single prosody-related fea- ture, such as pitch, intensity, or speaking rate. -->
全局风格标记（GST）方法最早在 Unsupervised style modeling, control and transfer in end-to-end speech synthesis 中提出，其提出了一个框架，可以在端到端 TTS 模型中以无监督的方式学习各种说话风格（称为风格标记）。这种方法可以看作是一种软聚类方法，用于在无标签数据集中学习 expressive styles 的 soft style clusters。GST 扩展了 Towards end-to-end prosody transfer for expressive speech synthesis with tacotron 中的方法，将 reference encoder 生成的 style embedding 送到 attention 单元，用于计算 style embedding 和一组随机初始化的 token 之间的相似度。训练时，模型学习 style token 和一组权重，每个 style embedding 则通过 style token 的加权和来生成。这些权重表示每个 token 对最终 style embedding 的贡献。从而每个 token 可以表示单个 style 或单个与 prosody 相关的特征，如 pitch、intensity 或说话速度。
<!-- At inference time, a reference audio can be passed to the model to generate its corresponding style embedding via a weighted sum of the style tokens. Alternatively, each individual style token can be used as a style embedding. In addition, GSTs offer an enhanced control over the speaking style through various operations. These include manual weight refinement, token scaling with different values, or the ability to condition different parts of the input text with distinct style tokens. -->
推理时，通过 reference audio 生成其对应的 style embedding，也可以直接使用每个 style token 作为 style embedding。GST 还可以对说话风格进行增强控制，如手动权重调整、不同值的 token 缩放，或者使用不同的 style token 来 condition 输入文本的不同部分。
<!-- The GST-TTS model can be further enhanced by mod- eling different levels of prosody to improve both expres- siveness and control over the generated speech. For instance, [46] proposes a fine-grained GST-TTS model where word-level GSTs are generated to capture local style variations (WSVs) through a prosody extractor. The WSV extractor consists of a reference encoder and a style token layer, as described in [75], along with an attention unit to produce the word-level style token -->
GST-TTS 模型可以通过建模不同级别的 prosody 来提高生成语音的表现力和控制。如 Extracting and predicting word-level style variations for speech synthesis 提出了一个细粒度的 GST-TTS 模型，生成 word-level GSTs 来捕获局部风格变化（WSVs）。WSV extractor 由 reference encoder 和 style token layer 组成，还有一个 attention unit 用于生成 word-level style token。
<!-- In [133] a hierarchical structure of multi-layer GSTs with residuals is proposed. The model employs three GST layers, each with 10 tokens, resulting in a better interpretation of the tokens of each level. Upon tokens analysis, it was found that the first-layer tokens learned speaker representations, while the second-layer tokens captured various speaking style features such as pause position, duration, and stress. The third-layer tokens, however, were able to generate higher-quality samples with more distinct and interpretable styles. Similarly, in [50], a multi-scale GST extractor is proposed to extract speaking style at different levels. This extractor extracts style embeddings from the reference mel-spectrogram using three style encoders at global, sentence, and sub word levels, and combines their outputs to form the multi-scale style embedding. -->
Learning hierarchical representations for expressive speaking style in end-to-end speech synthesis 提出了一个多层 GST 的 hierarchical 结构。模型使用三个 GST 层，每个有 10 个 token。分析 tokens 发现，第一层 token 学习说话人表征，第二层 tokens 捕获各种说话风格特征，如 pause 位置、持续时间和重音。第三层 tokens 能够生成更高质量的样本，具有更明显和可解释的风格。

Towards expressive speaking style modelling with hierarchical context information for mandarin speech synthesis 也提出了一个多尺度的 GST extractor，从 reference mel-spectrogram 中使用三个 style encoders 在 global、sentence 和 sub word level 提取 style embeddings，并将它们的输出组合成多尺度 style embedding。
<!-- With only a small portion of the training dataset labeled with emotions, [26] proposes a semi-supervised GST model for generating emotional speech. The model applies a cross-entropy loss between the one-hot vectors representing the emotion labels and the weights of GSTs, in addition to the GST-TTS reconstruction loss. The semi-GST model is trained on a dataset in which only 5% of the samples are labeled with emotion classes, while the rest of the dataset is unlabeled. After training, each style token represents a specific emotion class from the train- ing dataset and can be used to generate speech in the corresponding emotion. -->
由于只有小部分训练数据带有情感标签，提出了一种半监督的 GST 模型用于生成情感语音。计算 emotion labels 的 one hot 向量和 GSTs 的权重之间的交叉熵损失。半监督 GST 模型在一个数据集上训练，其中只有 5% 的样本带有情感类别标签，其余数据集是无标签的。训练后，每个 style token 代表训练数据集中的特定情感类别，并可以用于生成相应情感的语音。
<!-- Furthermore, in [92], a speech emotion recognition (SER) model is proposed with the GST-TTS to generate emotional speech while acquiring only a small labeled dataset for training. The paper formulates the training process as reinforcement learning (RL). In this frame- work, the GST-TTS model is treated as the agent, and its parameters serve as the policy. The policy aims to predict the emotional acoustic features at each time step, where these features represent the actions. The pre-trained SER model then provides feedback on the predicted features through emotion recognition accuracy, which represents the reward. The policy gradient strategy is employed to perform backpropagation and optimize the TTS model to achieve the maximum reward. -->
Reinforcement learning for emotional text-to-speech synthesis with improved emotion discriminability 提出了一个 speech emotion recognition (SER) 模型，与 GST-TTS 结合生成情感语音，只需要很小的标记数据集。将训练过程建模为强化学习（RL）。GST-TTS 模型作为 agent，其参数作为 policy。policy 的目标是在每个时间步预测情感声学特征，这些特征代表 actions。预训练的 SER 模型通过情感识别准确性提供对预测特征的反馈，这代表 reward。使用 policy gradient 策略进行反向传播，优化 TTS 模型以获得最大 reward。
<!-- In contrast, the Mellotron model [114] introduces a unique structure for the GSTs, enabling Mellotron to generate speech in various styles, including sing- ing styles, based on pitch and duration information extracted from the reference audio. This is achieved by obtaining a set of explicit and latent variables from the reference audio. Explicit variables (text, speaker, and F0 contour) capture explicit audio information, while latent variables (style tokens and attention maps) capture the latent characteristics of speech that are hard to extract explicitly. -->
Mellotron: Multispeaker expressive voice synthesis by conditioning on rhythm, pitch and global style tokens 提出了一种独特的 GSTs 结构，使 Mellotron 可以基于从 reference audio 中提取的 pitch 和 duration 信息生成各种风格的语音，包括唱歌风格。先从 reference audio 中获取一组 explicit 和 latent variable。 Explicit variables（文本、说话人和 F0 ）捕获显式音频信息，而 latent variables（包括 style tokens 和 attention maps）捕获难以显式提取的语音的潜在特征。

### 基于上下文学习的模型
<!-- These is a group of recent TTS models that are trained on a large amounts of data using in-context learning strategy. During in-context learning (also called prompt engineering), the model is trained to predict missing data based its context. In other words, the model is trained with a list of input-output pairs formed in a way that rep- resents the in-context learning task. After training, the model should be able to predict the output based on a given input. -->
使用上下文学习策略在大量数据上进行训练。在上下文学习中，模型根据上下文预测缺失数据。在训练时使用 输入-输出对来构成某种上下文学习任务。训练后，模型根据给定的输入预测输出。
<!-- For the TTS task, the provided style reference (referred to as prompt) is considered as part of the entire utter- ance to be synthesized. The TTS model training task is to generate the rest of this utterance following the style of the provided prompt as shown by Fig. 9. By employing this training strategy, recent TTS models such as VALL-E [22], NaturalSpeech 2 [18], and Voicebox [25] are capable of producing zero-shot speech synthesis using only a sin- gle acoustic prompt. Furthermore, these models demon- strate the ability to replicate speech style/emotion from a provided prompt [18, 22] or reference [25] to the synthe- sized speech. -->
对于 TTS 任务，style reference（称为 prompt）会被作为整个合成语音的一部分。TTS 模型的训练任务是根据提供的 prompt 风格生成剩余的语音。从而 TTS 模型（如 VALL-E、NaturalSpeech 2 和 Voicebox）能够使用单个 acoustic prompt 进行 zero-shot 语音合成。而且这些模型还可以从 prompt 或 reference 中复制语音风格/情感到合成的语音中。
<!-- In VALL-E [22], a language model is trained on tokens from Encodec [151], and the input text is used to condi- tion the language model. Specifically, the Encodec model tokenizes audio frames into discrete latent vectors/codes, where each audio frame is encoded with eight codebooks. VALL-E employs two main models: the first one is an auto-regressive (AR) model that predicts the first code of each frame, and the second is non-auto-regressive (NAR) model that predicts the other seven codes of the frame. -->
在 VALL-E 中，使用 Encodec 中的 tokens 训练了一个 language model，输入文本作为 language model 的 condition。Encodec 将音频帧标记为离散的 latent vectors/codes，每个音频帧用八个 codebooks 编码。VALL-E 使用两个主要模型：第一个是 auto-regressive (AR) 模型，预测每个帧的第一个 code，第二个是 non-auto-regressive (NAR) 模型，预测帧的其他七个 code。
<!-- Instead of discrete tokens used in VALL-E, Natural- Speech 2 [18] represents speech as latent vectors from a neural audio codec with residual vector quantizers. The latent vectors are then predicted via a diffusion model, conditioned on input text, pitch from a pitch predictor, and input speech prompt. -->
NaturalSpeech 2 使用了基于 residual vector quantizers 的 音频 codec 将语音表示为 latent vectors。然后通过一个 diffusion model 基于 input text、pitch 和 input speech prompt 预测 latent vectors。
<!-- Another example of in-context training is Voicebox [25] which is a versatile generative model for speech trained on a large amount of multilingual speech data. The model is trained on a text-guided speech infilling task, which gives it the flexibility to perform various speech tasks such as zero-shot TTS, noise removal, content editing, and diverse speech sampling. Voicebox is modeled as a non-autoregressive (NAR) flow-matching model with the ability to consider future context.-->
Voicebox 为多功能的语音生成模型。其在 text-guided speech infilling 任务上训练，从而可以实现各种语音任务，如 zero-shot TTS、去噪、内容编辑和多样化语音采样。Voicebox 是一个 non-autoregressive (NAR) flow-matching 模型，可以考虑未来的上下文。

### 其他方法
<!-- This category containes reviewed papers that propose individual techniques or methods which cannot be cate- gorized under any of the previously mentioned unsuper- vised approaches. For instance, in [121], a neural encoder is introduced to encode the residual error between the predictions of a trained average TTS model and the ground truth speech. The encoded error is then used as a style embedding that conditions the decoder of the TTS model to guide the synthesis process. Raitio and Seshadri [128] improves prosody modeling of FastSpeech2 model [6] with an additional variance adaptor for utterance- wise prosody modeling. -->
Rapid style adaptation using residual error embed- ding for expressive speech synthesis 引入 neural encode 来编码训练好的平均 TTS 模型的预测和真实语音之间的残差误差。编码后的 error 作为 style embedding 用于 TTS 模型的 decoder 的 condition 来引导合成过程。

Hierarchical prosody modeling and control in non-autoregressive parallel neural tts 提出了一个额外的 variance adaptor 来改进 FastSpeech2 的 prosody modeling 来实现 utterance-wise prosody modeling。
<!-- As context information is strongly related to speech expressivity, [45] proposes using multiple self-attention layers in Tacotron2 [4] encoder to better capture the con- text information in the input text. The outputs of these layers in the encoder are combined through either direct aggregation (concatenation) or weighted aggregation using a multi-head attention layer. Additionally, there are some papers that propose using only input text to obtain prosody-related representations/embeddings without any style references, and those are further discussed in Section 5.2.4. -->
由于上下文信息与语音表现力密切相关，Exploiting deep sentential context for expressive end-to-end speech synthesis 提出在 Tacotron2 encoder 中使用多个 self-attention layers 来更好地捕获输入文本中的上下文信息。encoder 中这些层的输出通过直接聚合（concatenation）或加权聚合（使用 multi-head attention layer）进行组合。

## ETTS 模型的主要挑战
<!-- In this section, we list and explain the most important challenges that face expressive TTS models and the main solutions that have been proposed in the literature to overcome these challenges. We then provide a summary of papers addressing each challenge in Table 5. -->
本节列出 ETTS 模型面临的挑战，以及文献中提出的解决方案。概括表格如下：
![](image/Pasted%20image%2020240402150530.png)

<!-- Irrelevant information leakage -->
### 无关信息泄漏
<!-- One main problem in unsupervised approaches that rely on having a style reference or a prompt, is the leak- age of irrelevant information, like speaker or text related information, into the generated style or prosody embed- ding. This irrelevant information within the speech style can lead to degradation in the quality of the synthesized speech. As a result, many studies have investigated this problem, and several solutions have been proposed as outlined below. -->
依赖于 style reference 或 prompt 的无监督方法中的一个主要问题是无关信息的泄漏，如说话人或文本相关信息泄漏到生成的风格或 prosody embedding 中。这种无关信息会导致合成语音质量下降。

#### 对抗训练
<!-- Adversarial training [90] is one of the widely used tech- niques to confront the information leakage problem. Typically, a classifier is trained to distinguish the type of unwanted information (such as speaker or content information) that is leaking from the prosody reference audio into the generated prosody embedding. During the training process, the weights of the employed prosody encoder/extractor from the reference audio are modi- fied with gradient inversion of the proposed classifier. In other words, the classifier penalizes the prosody encoder/ extractor for any undesired information in its output. A gradient reversal layer (GRL) is usually used to achieve the inversion of the classifier gradients. -->
训练分类器来区分从 prosody reference audio 泄漏到生成的 prosody embedding 中的不需要的信息（如说话人或内容信息）。在训练过程中，通过梯度反转来修改 reference audio 中的 prosody encoder/extractor 的权重。换句话说，分类器惩罚 prosody encoder/extractor 的输出中的任何不需要的信息。通常使用梯度反转层（GRL）来实现分类器梯度的反转。
<!-- Several studies utilize adversarial training to prevent the flow of either speaker or content-related information from the given reference audio to the resulting prosody embedding. For instance, the VAE-TTS model proposed in [47] learns phoneme-level 3-dimensional prosody codes. The VAE is conditioned on speaker and emotion embeddings, besides the tone sequence and mel-spectro- gram from the reference audio. Adversarial training using a gradient reversal layer (GRL) is applied to disentan- gle speaker and tone from the resulting prosody codes. Similarly, adversarial training is introduced to the style encoder of the cross-speaker emotion transfer model proposed in [19] to learn a speaker-independent style embedding, where the target speaker embedding is pro- vided from a separate speaker encoder. -->
有很多研究用对抗训练来防止说话人或内容相关信息从 reference audio 流向 prosody embedding：
+ VAE-TTS 模型学习 phoneme-level 的 3 维 prosody codes，VAE 除了把 speaker 和 emotion embeddings 作为 condition，还有 tone sequence 和 mel-spectrogram。然后用 GRL 进行对抗训练，来从 prosody codes 中解耦 speaker 和 tone
+ Cross-speaker emotion transfer by manipulating speech style latents 将对抗训练引入到 cross-speaker emotion transfer model 的 style encoder 中，来学习 speaker-independent style embedding，其中目标说话人 embedding 由另一个说话人 encoder 提供
<!-- The STYLER model in [97] employs multiple style encoders to decompose the style reference into several components, including duration, pitch, speaker, energy, and noise. Both channel-wise and frame-wise bottleneck layers are added to all the style encoders to eliminate content-related information from the resulting embed- dings. Furthermore, as noise is encoded individually by a separate encoder in the model, other encoders are constrained to exclude noise information by employing either domain adversarial training or residual decoding. -->
+ STYLER 使用多个 style encoders 将 style reference 分解为几个分布，包括 duration、pitch、speaker、energy 和 noise。所有 style encoders 都添加了 channel-wise 和 frame-wise 的 bottleneck layers 来消除 embeddings 中的内容相关信息。此外，由于 noise 由模型中的单独 encoder 单独编码，其他 encoders 通过使用 domain 对抗训练或 residual decoding 来排除 noise 信息