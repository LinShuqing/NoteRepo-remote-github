> EURASIP Journal on Audio, Speech, and Music Processing，2024，Ozyegin University
<!-- 翻译&理解 -->
<!-- Speech synthesis has made significant strides thanks to the transition from machine learning to deep learning mod- els. Contemporary text-to-speech (TTS) models possess the capability to generate speech of exceptionally high qual- ity, closely mimicking human speech. Nevertheless, given the wide array of applications now employing TTS models, mere high-quality speech generation is no longer sufficient. Present-day TTS models must also excel at producing expressive speech that can convey various speaking styles and emotions, akin to human speech. Consequently, researchers have concentrated their efforts on developing more efficient models for expressive speech synthesis
in recent years. This paper presents a systematic review of the literature on expressive speech synthesis models pub- lished within the last 5 years, with a particular emphasis on approaches based on deep learning. We offer a compre- hensive classification scheme for these models and provide concise descriptions of models falling into each category. Additionally, we summarize the principal challenges encountered in this research domain and outline the strategies employed to tackle these challenges as documented in the literature. In the Section 8, we pinpoint some research gaps in this field that necessitate further exploration. Our objective with this work is to give an all-encompassing over- view of this hot research area to offer guidance to interested researchers and future endeavors in this field. -->
1. 本文给出了最近五年内发表的关于表达性语音合成模型的文献综述，重点在基于深度学习的方法
2. 提供了一个全面的分类方案，且对不在每个类别中的模型进行了详细描述
3. 总结了这个研究领域遇到的主要挑战，并概述了文献中用于解决这些挑战的策略
4. 在第 8 节中，指出了这一领域的一些研究空白，可以进一步探索的点

## Introduction
<!-- Since the late 1950s, computer-based text-to-speech systems (TTS) have undergone significant advance- ments [1], culminating in the production of models that generate speech almost indistinguishable from that of a human. This progress has followed a path consisting of several stages, beginning with conven- tional methods named as concatenative synthesis and progressing to more advanced approaches known as statistical parametric speech synthesis (SPSS). Advanced approaches are mainly based on machine learning algo- rithmslikehiddenMarkovmodels(HMMs)andgaussian mixture models (GMMs). Despite this progress, speech generated by these methods was still noticeably artificial. However, the emergence of deep learning (DL) as a new branch under machine learning (ML) in 2006 has led to significant improvements. Speech synthesis researchers, like many in other research fields, started incorporating deep neural networks (DNN) in their models. Initially, DNNs replaced HMMs and GMMs in SPSS models while the main structure still follows the primary framework of SPSS models as shown in Fig. 1. As discussed in [2], the deep learning-based models have overcome many limi- tations and problems associated with machine learning- based models. -->
1. 早起的 TTS 模型主要还是 statistical parametric speech synthesis (SPSS)，其基本结构如图：![](image/Pasted%20image%2020240401155020.png)
<!-- Researchers continue to aim for improved speech qual- ity and more human-like speech despite past advance- ments. Additionally, they seek to simplify the framework of the text-to-speech models due to the intricate nature of the SPSS structure, which limits progress in this field to those with extensive linguistic knowledge and exper- tise. Deep learning advancements have brought about the simple encoder-decoder structure for TTS models as sequence-to-sequence (Seq2Seq) approaches. The pro- posed approaches have simplified the structure of con- ventional TTS with multiple components into training a single network that converts a set of input text char- acters/phonemes into a set of acoustic features (mel- spectrograms). A main concern in these advanced TTS models is the mapping process between the input and output sequences, which is a one-to-many problem, as the single input text can have multiple speech variations as output. In fact, there are two groups of recent TTS models, as shown in Fig. 2. The first group generates mel- spectrograms in a sequential (autoregressive) manner using soft and automatic attention alignments between input and output sequences, such as the Tacotron model [3, 4]. The second group utilizes hard alignments between the phonemes/characters and mel-spectro- grams, and thus its speech generation process is parallel (non-autoregressive), as in the FastSpeech model [5, 6]. This improvement in the structure of the TTS model has encouraged rapid development in the field within the last few years, during which the proposed models produced speech that is nearly indistinguishable from human speech. -->
2. 