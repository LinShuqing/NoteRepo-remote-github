> 论文 - A Survey on Neural Speech Synthesis

> TTS：语音合成

1. 综述神经TTS，包括文本分析、声学模型和声码器
2. 综述高阶主题，包括快速 TTS，低资源 TTS、鲁棒 TTS、表达性 TTS 和 自适应 TTS

## Introduction

### TTS 历史

1. 发音合成：模拟人类发音器官的行为来产生语音
2. 共振峰合成：通过控制简化的源滤波器模型的一组规则来生成语音
3. 拼接合成：依赖于存储在数据库中的语音片段的拼接
4. 统计参数合成（SPSS）：首先得到合成语音所需的声学参数，使用算法从参数中恢复语音，包括：
	1. 文本分析
	2. 声学模型
	3. 声码器
5. 神经语音合成：从 WaveNet 开始，它采用（深度）神经网络作为语音合成的模型主干
## 文章组织

如图：![[Pasted image 20230128114328.png]]
TTS 中的关键技术：
+ 文本分析
+ 声学模型
+ 声码器
还介绍了完全端到端的 TTS，以及一些其他的分类方法。

TTS 进阶：
+ 加快生成速率（快速 TTS）
+ 提高合成语音的可理解性和自然度
+ 低资源场景下构建高效的 TTS 模型
+ 提高语音合成的鲁棒性
+ 生成富有表现力的语音
+ 调整TTS模型以支持任何目标说话者的语音（自适应数据和参数）

## TTS 中的关键技术

![[Pasted image 20230128115036.png]]

### 主要分类

分离式：
1. 文本分析将字符转换为音素或语言特征
2. 声学模型从语言特征或字符/音素生成声学特征
3. 声码器根据语言特征或声学特征生成波形

全端到端式：直接将字符/音素转换为波形

TTS 过程中的几种数据格式：
1. 字符，文本的原始格式
2. 语言特征，分析文本获得，音素是语言特征中最重要的元素之一，通常在基于神经网络的 TTS 模型中单独用于表示文本
3. 声学特征，如 LSP、MCC、MGC、F0、BAP等，在基于神经网络的端到端 TTS 模型中，mel谱图或线性谱图通常用作声学特征
4. 波形：语音的最终格式

几种从文本到波形的数据流：
+ 字符→ 语言特征→ 声学特征→ 波形
+ 字符→ 音素→ 声学特征→ 波形
+ 字符→ 语言特征→ 波形
+ 字符→ 音素→ 声学特征→ 波形
+ 字符→ 音素→ 波形
+ 字符→ 波形

### 文本分析
> 将输入文本转换为包含丰富的语音和韵律信息的语言特征

在端到端的神经TTS中，由于基于神经的模型的建模能力大，字符或音素序列被直接作为合成的输入，因此文本分析模块被大大简化，但仍然需要文本规范化来从字符输入中获得标准单词格式，需要字形到音素的转换来从标准单词格式中获得音素。

文本分析的典型任务：![[Pasted image 20230128152747.png]]
+ 文本规范化：原始书面文本（非标准单词）通过文本规范化转换为可以发音的单词，如 ”1989“ 转换为 ”nineteen eighty nine“
+ 分词：对于汉语等语言，需要从文本中检测单词边界
+ 词性标注：标注每个单词的词性
+ 韵律预测：韵律信息，如语音的节奏、重音和语调，对应于音节持续时间、响度和音高的变化，在人类交流中起着重要的感知作用
+ 字素到音素转换（Grapheme-to-phoneme (G2P)）：如，单词“speech”被转换为“s p iy ch”

### 声学模型
> 从语言特征或直接从音素或字符 生成 声学特征

可以分为：
+ SPSS中的声学模型，通常根据语言特征预测如MGC、BAP和F0等声学特征
+ 基于神经的端到端 TTS 中的声学模型，根据音素或字符预测如mel谱图等声学特征
![[Pasted image 20230128154654.png]]
#### SPSS 中的声学模型

主要的方法有：
+ HMM，在改变说话者身份、情绪和说话风格方面更为灵活，但质量不行
+ DNN，提高了 HMM 的质量
+ LSTM，更好地建模长时间跨度的上下文
+ CBHG、VoiceLoop、GAN 等等

#### 端到端 TTS 中的声学模型

相比于 SPSS，端到端的 TTS 中的声学模型的优点有：
+ 无需语言和声学特征的对齐
+ 建模能力更强

##### 基于 RNN 的模型

Tacotron 系列：
+ 第一版，输入字符，输出线性谱，encoder-attention-decoder 结构，GriffinLim 生成波形
+ 第二版，生成 Mel 谱，使用额外的 WaveNet 生成波形
+ 改进：
	+ GST Tacotron 和 Ref-Tacotron：使用参考编码器和样式标记来增强语音的表现力
	+ DurIAN 和 非注意力Tacotron：去除注意力机制，使用持续时间预测因子进行自回归预测
	+ Parallel Tacotron 1、2：将Tacotron中的自回归生成改为非自回归生成
	+ Wave Tacotron：基于Tacotron构建端到端文本到波形模型

##### 基于 CNN 的模型

DeepVoice 系列：
+ 第一版：是一个用卷积神经网络增强的SPSS系统，通过神经网络获得语言特征后，DeepVoice利用基于 WaveNet 的声码器生成波形
+ 第二版：通过改进的网络结构和多说话人建模来增强 DeepVoice，采用       Tacotron+WaveNet，使用 Tacotron 生成线性频谱图，然后使用WaveNet生成波形
+ 第三版，使用 全卷积网络结构进行语音合成，从字符生成mel谱图，可拓展到多说话人

ClariNet：全端到端的方式从文本生成波形
ParaNet：全卷积非自回归模型
DCTTS：利用基于卷积的 encoder-attention-decoder 网络从字符序列生成mel谱。使用谱超分辨率网络获得线性谱，使用 GriffinLim 合成波形

##### 基于 Transformer 的模型

TransformerTTS：利用基于 Transformer 的 encoder-attention-decoder 架构从音素生成mel谱图
MultiSpeech：通过编码器归一化、解码器瓶颈和对角注意约束提高注意机制的鲁棒性
RobuTrans：利用持续时间预测来增强自回归生成中的鲁棒性

> 以前的基于神经的声学模型，如Tacotron 1/2、DeepVoice 3和TransformerTTS 都采用自回归生成，这存在几个问题：1）推理速度慢。2） 鲁棒性问题。生成的语音通常具有大量的单词跳过和重复以及问题，这主要是由于基于 encoder-attention-decoder 的自回归生成中文本和mel谱之间的注意力对齐不准确造成的。
> FastSpeech可以解决这些问题：
> 1. 采用前馈 Transformer 网络并行生成mel谱，这可以大大加快推断速度
> 2. 取消文本和语音之间的注意力机制，以避免单词跳过和重复问题，提高了鲁棒性

FastSpeech 系列：
+ 第一版：利用显式持续时间预测器来扩展音素隐藏序列，以匹配mel谱的长度
+ 第二版：使用 GT-mel谱 作为训练目标而非从自回归教师模型中提取mel谱；提供更多的信息，如音调、持续时间和能量作为解码器输入
+ 改进：
	+ FastPitch：使用音调信息作为解码器输入来改进FastSpeech

##### 其他模型

基于 flow 的模型，如：Flowtron、flow TTS、Glow TTS
基于 VAE 的模型：GMVAE Tacotron、VAE-TTS、BVAE-TTS
基于 GAN 的模型：GAN exposure、TTS-Stylization、Multi-SpectroGAN
基于 Diffusion 的模型：Diff-TTS、Grad-TTS、PriorGrad

### 声码器
> 声码器的发展可以分为两个阶段：SPSS中使用的声码器和基于神经网络的声码器
> SPSS中一些流行的声码器包括 STRIGHT 和 WORLD
> 下面主要介绍基于神经的声码器

早期的神经声码器如WaveNet、Char2Wav、WaveRNN直接将语言特征作为输入并生成波形。

后来使用各种生成模型进行波形生成，可以分为：
+ 自回归声码器
+ 基于 flow 的声码器
+ 基于 GAN 的声码器
+ 基于 VAE 的声码器
+ 基于 Diffusion 的声码器
![[Pasted image 20230129102533.png]]
#### 自回归声码器

WaveNet 是第一个基于神经的声码器，利用 dilated convolution 自回归生成波形，不需要任何先验知识，但是其生成速度慢。
SampleRNN 利用分层递归神经网络进行波形生成。
WaveRNN 使用递归神经网络，并利用 dual softmax层、权重修剪和subscaling 技术在内的多种设计来减少计算。
LPCNet 将传统的数字信号处理引入神经网络，使用线性预测系数来计算下一个波形点，同时利用轻量级RNN来计算残差。

#### 基于 flow 的声码器

归一化流是一种生成模型，利用一系列可逆映射变换概率密度。在采样期间，它通过这些变换的逆运算从标准概率分布生成数据。TTS 中基于流的模型可以分为：
+ 自回归变换：Parallel Wavenet、ClariNet，但是依赖于复杂的师生训练
+ 二分变换：WaveGlow、FloWaveNet，实现了高语音质量和快速生成
![[Pasted image 20230129103954.png]]
#### 基于 GAN 的声码器

包括：![[Pasted image 20230129104033.png]]

从生成器角度：大多数都使用 dilated convolution 来增加感受野以模拟波形序列中的长期依赖，使用转置卷积（反卷积）对 Mel 谱进行上采样以匹配波形序列长度。

判别器角度：主要集中于如何设计模型来捕获波形的特征：
+ GAN-TTS 使用随机窗口判别器
+ MelGAN 提出多尺度判别器，每个尺度聚焦于不同频率范围的特征
+ HiFiGAN 提出多周期判别器，通过观察不同周期中输入音频的不同部分来捕获不同的隐式结构
+ VocGAN 使用层级判别器，以从粗粒度到细粒度的不同分辨率判断生成的波形

从损失函数角度：除了常规的 GAN 损失，也使用了如 STFT 损失、特征匹配损失等来提高对抗训练的稳定性和训练效率。

#### 基于 Diffusion 的声码器
> 最近兴起的工作，用扩散过程和逆过程建立数据与潜在分布之间的映射

优点在于可以生成非常高质量的语音，但迭代过程漫长，生成速度慢。

常见的模型有：DiffWave、WaveGrad、PriorGrad

#### 其他声码器

使用基于神经网络的 source-filter 模型来进行可控的高质量语音生成。

#### 总结

![[Pasted image 20230129110024.png]]
+ 在数学简单性方面，基于自回归（AR）的模型比其他生成模型（如VAE、Flow、Diffusion和GAN）更容易
+ 除AR之外的所有生成模型都可以支持并行生成
+ 除AR模型外，所有生成模型都可以在某种程度上支持潜在变量
+ 基于GAN的模型无法估计数据样本的似然，而其他模型有这优势


### 全端到端的 TTS

直接从字符或者音素序列生成语音波形，优点为：
+ 需要较少的人工注释（如对齐信息）和特征
+ 联合优化可以避免传播误差
+ 降低训练、开发和部署成本

端到端 TTS 的发展：![[Pasted image 20230129113536.png]]

stage 0：统计参数合成使用三个基本模块进行语音合成

stage 1：在统计参数合成将文本分析和声学模型结合到端到端声学模型中，该模型直接从音素序列生成声学特征，然后使用声码器生成波形

stage 2：WaveNet 从 语言特征直接生成语音波形（但是输入为 Mel 谱啊？？），可以被视为声学模型和声码器的组合，但仍需文本分析模块

stage 3：Tacotron 使用 encoder-attention-decoder 模型直接预测字符/音素的线性谱，并使用Griffin Lim将线性谱转换为波形，其他如 DeepVoice 3、Tacotron 2 等先从字符/音素预测Mel谱，然后使用神经声码器生成波形

stage 4：直接从文本到波形：
+ Char2Wav 利用基于 RNN 的 encoder-attention-decoder 模型从字符中生成声学特征，然后使用 SampleRNN 生成波形，两个模型联合优化
+ ClariNet 联合优化 自回归声学模型和非自回归声码器，从而直接生成波形
+ FastSpeech 2s 直接从具有完全并行结构的文本生成语音
+ EATS 直接从字符/音素生成波形，利用持续时间插值和软动态时间包裹损失进行端到端对齐学习
+ Wave Tacotron 在 Tacotron 上构建了一个基于流的解码器，以直接生成波形

### 其他分类法

1. 自回归非自回归
2. 不同的生成模型
3. 不同的网络结构，如 CNN、RNN、CNN+RNN、self-attention 等
具体分类见原论文。

整个 TTS 的发展流程图（按照arxiv上的时间，截止到 2021，论文中有一个更详细的表格）：![[Pasted image 20230129115144.png]]

## TTS 进阶

