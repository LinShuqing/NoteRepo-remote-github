> Disentangled Representation Learning，IEEE Fellow 团队，清华

<!-- Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further categorize the methodologies for DRL into four groups, i.e., Traditional Statistical Approaches, Variational Auto-encoder Based Approaches, Generative Adversarial Networks Based Approaches, Hierarchical Approaches and Other Approaches. We also analyze principles to design different DRL models that may benefit different tasks in practical applications. Finally, we point out challenges in DRL as well as potential research directions deserving future investigations. We believe this work may provide insights for promoting the DRL research in the community. -->
1. Disentangled Representation Learning (DRL) 旨在，学习一种模型，其能够以表征的形式识别和解耦隐藏在观测数据中的潜在因素
2. 本文回顾了 DRL 的各个方面，包括动机、定义、方法、评估、应用和模型设计
3. 将 DRL 方法分为四组：
    1. 传统统计方法
    2. VAE-based 方法
    3. GAN-based 方法
    4. Hierarchical 方法和其他
4. 分析了设计不同任务下 DRL 模型的原则，以及 DRL 面临的挑战和未来的研究方向

## Introduction

<!-- When humans observe an object, we seek to understand the various properties of this object (e.g., shape, size and color etc.) with certain prior knowledge. However, existing end-to-end black-box deep learning models take a shortcut strategy through directly learning representations of the object to fit the data distribution and discrimination crite- ria [1], failing to extract the hidden attributes carried in representations with human-like generalization ability. To fill this gap, an important representation learning paradigm, Disentangled Representation Learning (DRL) is proposed [2] and has attracted an increasing amount of attention in the research community. -->
1. 现有的 black box 深度学习模型，直接学习 object 的 representation 来拟合数据分布，而没有提取 representation 中的隐藏属性
<!-- DRL is a learning paradigm where machine learning models are designed to obtain representations capable of identifying and disentangling the underlying factors hid- den in the observed data. DRL always benefits in learning explainable representations of the observed data that carry semantic meanings. Existing literature [2], [3] demonstrates the potential of DRL in learning and understanding the world as humans do, where the understanding towards real-world observations can be reflected in disentangling the semantics in the form of disjoint factors. The disentan- glement in the feature space encourages the learned repre- sentation to carry explainable semantics with independent factors, showing great potential to improve various machine learning tasks from the three aspects: i) Explainability: DRL learns semantically meaningful and separate representa- tions which are aligned with latent generative factors. ii) Generalizability: DRL separates the representations that our tasks are interested in from the original entangled input and thus has better generalization ability. iii) Controllability: DRL achieves controllable generation by manipulating the learned disentangled representations in latent space. -->
2. DRL 使得模型可以学习去识别和解耦隐藏在观测数据中的潜在因素的表征，这些表征通常是可解释性的，包含一定的语义意义，从而可以在多方面提高很多机器学习任务的性能：
    1. 可解释性：DRL 学习到的表征是语义上有意义的，且是独立的
    2. 泛化能力：DRL 将特定的表征从原始的混合输入中分离出来，从而具有更好的泛化能力
    3. 可控性：DRL 通过操纵学习到的表征，实现可控的生成
<!-- Then a natural question arises, What are disentangled representations supposed to learn? The answer may lie in the concept of disentangled representation proposed by Bengio et al. [2], which refers to factor of variations in brief. As shown by the example illustrated in Figure 1, Shape3D [4] is a frequently used dataset in DRL with six distinct factors of variation, i.e., object size, object shape, object color, wall color, floor color and viewing angle. DRL aims at separating these factors and encoding them into independent and dis- tinct latent variables in the representation space. In this case, the latent variables controlling object shape will change only with the variation of object shape and be constant over other factors. Analogously, it is the same for variables controlling other factors including size, color etc. -->
![](image/Pasted%20image%2020240125112247.png)
3. 在上图的例子中，DRL 的目标是将这些 factors 分离，并将它们编码为表征空间中的独立且不同的 latent variables。此时控制 object shape 的 latent 只会随着 object shape 的变化而变化，并且在其他 factors 上保持不变
<!-- Through both theoretical and empirical explorations, DRL benefits in the following three perspectives: i) In- variance: an element of the disentangled representations is invariant to the change of external semantics [5], [6], [7], [8], ii) Integrity: all the disentangled representations are aligned with real semantics respectively and are capable of generating the observed, undiscovered and even counter- factual samples [9], [10], [11], [12], and iii) Generalization: representations are intrinsic and robust instead of capturing confounded or biased semantics, thus being able to general- ize for downstream tasks [13], [14], [15]. -->
4. DRL 从理论和实践上都有以下三个优势：
    1. 不变性：disentangled representations 的一个元素对外部语义的变化是不变的
    2. 完整性：所有的 disentangled representations 都与真实的语义对齐，能够生成观测到的、未发现的甚至是反事实（counter- factual）的样本
    3. 泛化能力：表征是内在的和鲁棒的，而非混淆或有偏见的，因此能够泛化到下游任务
<!-- Following the motivation and requirement of DRL, there have been numerous works on DRL and its applications over various tasks. Most typical methods for DRL are based on generative models [6], [9], [16], [17], which initially show great potential in learning explainable representations for visual images. In addition, approaches based on causal inference [14] and group theory [18] are widely adopted in DRL as well. The core concept of designing DRL architecture lies in encouraging the latent factors to learn disentangled representations while optimizing the inherent task objec- tive, e.g., generation or discrimination objective. Given the efficacy of DRL at capturing explainable, controllable and robust representations, it has been widely used in many fields such as computer vision [8], [19], [20], [21], [22], natural language processing [23], [24], [25], recommender systems [26], [27], [28], [29] and graph learning [29], [30] etc., boosting the performances of various downstream tasks.-->
5. DRL 的核心概念是，鼓励 latent factors 学习 disentangled representations，同时优化内在的任务目标，例如生成或判别目标
<!-- Contributions. In this paper, we comprehensively re- view DRL through summarizing the theories, methodolo- gies, evaluations, applications and design schemes, to the best of our knowledge, for the first time. In particular, we present the definitions of DRL in Section 2 and comprehen- sively review DRL approaches in Section 3. In Section 4, we discuss popular evaluation metrics for DRL implementa- tion. We discuss the applications of DRL for various down- stream tasks in Section 5, followed by our insights in de- signing proper DRL models for different tasks in Section 6. Last but not least, we summarize several open questions and future directions for DRL in Section 7. Existing work most related to this paper is Liu et al.’s work [31], which only focuses on imaging domain and applications in medical imaging. In comparison, our work discusses DRL from a general perspective, taking full coverage of definitions, taxonomies, applications and design scheme. -->
6. 本文的贡献：
    1. 从理论、方法、评估、应用和设计方案等方面，全面回顾了 DRL
    2. 讨论了 DRL 在各种下游任务中的应用
    3. 总结了 DRL 的几个开放问题和未来的研究方向

## DRL 定义

### Intuitive Definition
<!-- Intuitive Definition. Bengio et al. [2] propose an intuitive definition about disentangled representation: -->
Bengio 提出了一个直观的定义：
<!-- Definition 1. Disentangled representation should separate the distinct, independent and informative generative factors of varia- tion in the data. Single latent variables are sensitive to changes in single underlying generative factors, while being relatively invariant to changes in other factors. -->
定义 1：**disentangled representation 应该将数据中的不同、独立和信息量大的 generative factors 分离开来。单个 latent 变量对单个潜在 generative factors 的变化敏感，但对其他 factors 的变化相对不变**
<!-- The definition also indicates that latent variables are statistically independent. Following this intuitive definition, early DRL methods can be traced back to independent component analysis (ICA) and principal component anal- ysis (PCA). Numerous Deep Neural Network (DNN) based methods also follow this definition [5], [6], [7], [9], [32], [33], [34], [35], [36], [37]. Most models and metrics hold the view that generative factors and latent variables are statistically independent. -->
这个定义也暗示了 latent variables 是统计独立的。许多 DNN-based 的方法也遵循这个定义。大多数模型和评估指标都认为 generative factors 和 latent variables 是统计独立的。

### Group Theory Definition
<!-- Group Theory Definition. For a more rigorous mathemati- cal definition, Higgins et al. [18] propose to define DRL from the perspective of group theory, which is later adopted by a series of works [38], [39], [40], [41]. We briefly review the group theory-based definition as follows: -->
Higgins 提出了一个更严格的数学定义，从 group theory 的角度出发：
<!-- Definition 2. Consider a symmetry group G, world state space W (i.e., ground truth factors which generate observations), data space O, and representation space Z. Assume G can be decom- posed as a direct product G = G1×G2×· · ·×Gn. Representation Z is disentangled with respect to G if:
(i)ThereisanactionofGonZ:G×Z →Z.
(ii)ThereexistsamappingfromW toZ,i.e.,f :W →Z which is equivariant between the action of G on W and Z. This condition can be formulated as follows:
g · f (w) = f (g · w), ∀g ∈ G, ∀w ∈ W (1)
which can be illustrated as Figure. 2.
(iii) The action of G on Z is disentangled with respect to
the decomposition of G. In other words, there is a decomposition Z = Z1 ×...×Zn or Z = Z1 ⊕...⊕Zn such that each Zi is affected only by Gi and invariant to Gj , ∀j ̸= i -->
定义 2：考虑一个 symmetry group $G$，world state space $W$（即生成观测的 ground truth factors），data space $O$ 和 representation space $Z$。假设 $G$ 可以分解为 $G = G_1 \times G_2 \times \cdots \times G_n$。如果满足以下条件，那么 representation $Z$ 相对于 $G$ 是 disentangled 的：
1. $G$ 在 $Z$ 上有一个 action：$G \times Z \rightarrow Z$
2. 存在一个从 $W$ 到 $Z$ 的映射 $f: W \rightarrow Z$，它在 $G$ 对 $W$ 和 $Z$ 上的 action 之间是等变的。这个条件可以表示为： $g \cdot f(w) = f(g \cdot w), \forall g \in G, \forall w \in W$
3. $G$ 在 $Z$ 上的 action 相对于 $G$ 的分解是 disentangled 的。换句话说，存在一个分解 $Z = Z_1 \times \cdots \times Z_n$ 或 $Z = Z_1 \oplus \cdots \oplus Z_n$，使得每个 $Z_i$ 只受 $G_i$ 的影响，对 $G_j, \forall j \neq i$ 不变
<!-- Definition 2 is mainly adopted by DRL approaches orig- inating from the perspective of group theory in VAE (Group theory based VAEs in Section 3.1.1). -->
定义 2 主要被 VAE-based 的 DRL 方法采用。

#### 讨论
<!-- Discussions. All the two definitions hold the assumption that generative factors are naturally independent. However, Suter et al. [14] propose to define DRL from the perspec- tive of the structural causal model (SCM) [42], where they additionally introduce a set of confounders which causally influence the generative factors of observable data. Yang et al. [11] and Shen et al. [43] further discard the independence assumption by considering that there might be an under- lying causal structure which renders generative factors. For example, in Figure 3, the position of the light source and the angle of the pendulum are both responsible for the position and length of the shadow. Consequently, instead of the in- dependence assumption, they use SCM which characterizes the causal relationship of generative factors as prior. We refer to these works holding the assumption of causal factors as causal disentanglement methods, which will be discussed in detail in Section 3.4. -->
1. 所有的定义都假设 generative factors 是独立的。然而，Suter 提出了一个更一般的定义，从 SCM（structural causal model） 的角度出发，引入了一组 confounders，它们会影响可观测数据的 generative factors
![](image/Pasted%20image%2020240125113453.png)
2. Yang 和 Shen 进一步放弃了独立性假设，认为可能存在一个潜在的因果结构来生成 generative factors。例如，上图中，光源的位置和 pendulum 的角度都会影响 shadow 的位置和长度
3. 因此，他们使用了 SCM 来描述 generative factors 的因果关系。将这些方法称为 causal disentanglement methods

## DRL 分类
<!-- In this section, we categorize DRL approaches (Figure 4) i) from the perspective of representation structure, i.e., dimension-wise vs. vector-wise and flat vs. hierarchical, ii) from the perspective of learning schemes, i.e., unsupervised vs. supervised, and iii) from the independence assumption of generative factors, i.e., independent vs. causal. For each group, We will elaborate on specific models, show their advantages and disadvantages, as well as analyze their ap- plication scenarios. Moreover, we also discuss Capsule Net- works and Object-centric Learning, given that the two learning paradigms also employ the idea of disentanglement and thus can be regarded as particular instances of DRL. We overview existing DRL methods and present inspirations on how DRL can be incorporated into specific tasks. -->
DRL 分类如下：
![](image/Pasted%20image%2020240125164938.png)
1. 从 representation structure 的角度，分为 dimension-wise vs. vector-wise 和 flat vs. hierarchical
2. 从 learning schemes 的角度，分为 unsupervised vs. supervised
3. 从 generative factors 的独立性假设的角度，分为 independent vs. causal

同时还讨论了 Capsule Networks 和 Object-centric Learning，因为这两种学习范式也采用了 disentanglement 的思想，可被视为 DRL 的特殊例子。

### Dimension-wise DRL vs. Vector-wise DRL
<!-- According to the structure of disentangled representa- tions, we can categorize DRL methods into two groups, i.e., dimension-wise and vector-wise methods. For dimension- wise methods, generative factors are fine-grained and a sin- gle dimension (or several dimensions) represents one gen- erative factor. For vector-wise methods, generative factors are coarse-grained and different vectors represent different types of semantic meanings. The comparisons of dimension- wise and vector-wise methods are shown in Figure 5 and Table 2. Dimension-wise methods are always experimented on synthetic and simple datasets, while vector-wise meth- ods are always used in real-world scenes such as identity swapping, image classification, subject-driven generation, and video understanding. Synthetic and simple datasets usually have multiple fine-grained latent factors, leading to the applicability of dimension-wise disentanglement. In contrast, on real-world datasets and applications, we usu- ally concentrate on two or several coarse-grained factors (e.g., identity and pose), making it more suited for vector- wise disentanglement. Dimension-wise methods are mostly early theoretical explorations of DRL, and usually rely on certain model architectures, e.g., VAE or GAN. Vector-wise methods are more flexible, for example, we can design task- specific encoders to extract disentangled vectors and design appropriate loss functions to ensure disentanglement. In this section, we will discuss a series of representatives of dimension-wise and vector-wise DRL methods. -->
根据 disentangled representations 的结构，可将 DRL 分为两组：dimension-wise 和 vector-wise：
+ dimension-wise 中，generative factors 是细粒度的，单个 dimension（或者多个 dimension）表示一个 generative factor，通常用于 synthetic 和 simple datasets，且通常有多个细粒度的 latent factors；早期的 DRL 方法大多属于 dimension-wise，通常依赖于某种模型结构，例如 VAE 或 GAN
+ vector-wise 中，generative factors 是粗粒度的，不同的 vectors 表示不同类型的语义意义，通常用于 real-world scenes，会将两个或多个粗粒度的 factors 拼接；vector-wise 更加灵活，例如可以设计 task-specific 的 encoder 来提取 disentangled vectors，设计合适的 loss function 来保证 disentanglement

两个方法的比较如下图和下表所示：
![](image/Pasted%20image%2020240125165649.png)
![](image/Pasted%20image%2020240125165854.png)

#### VAE-based Dimension-wise DRL 

<!-- Vanilla VAE-based methods. A typical architecture that can be used to achieve dimension-wise disentanglement is the Variational auto-encoder (VAE) [16]. Variational auto- encoder (VAE) [16] is a variant of the auto-encoder, which adopts the idea of variational inference. VAE is originally proposed as a deep generative probabilistic model for image generation. Later researchers find that VAE also has the po- tential ability to learn disentangled representation on simple datasets (e.g., FreyFaces [16], MNIST [44]). To obtain better disentanglement performance, researchers design various extra regularizers to combine with the original VAE loss function, resulting in the family of VAE-based Approaches. One of the most important characteristics of various VAE- based methods is the dimension-wise structure of the dis- entangled representations, i.e., different dimensions of the latent vector represent different factors. -->
VAE 是一种典型的 dimension-wise DRL 方法。VAE 最初用于图像生成，后来发现 VAE 也有潜在的能力来学习 simple datasets 上的 disentangled representations。为了获得更好的 disentanglement 性能，设计了各种额外的 regularizers，再与原始的 VAE loss function 结合，从而形成了 VAE-based 方法。这些方法最重要的特征之一是 disentangled representations 的 dimension-wise 结构，即 latent vector 的不同 dimension 表示不同的 factors。
<!-- The general VAE model structure is shown in Figure 6. The fundamental idea of VAE is to model data distributions from the perspective of maximum likelihood using varia- tional inference, i.e., to maximize log pθ (x). This objective can be written as Eq.(2) in the following, -->
VAE 的一般模型结构如下图所示：
![](image/Pasted%20image%2020240125171046.png)

VAE 的基本思想是，从 variational inference 的角度，使用最大似然来建模数据分布，即最大化 log p(x)。这个目标可以写成下式：
$$\log p_\theta(\mathbf{x})=D_{KL}(q_\phi(\mathbf{z}|\mathbf{x})\|p_\theta(\mathbf{z}|\mathbf{x}))+\mathcal{L}(\theta,\phi;\mathbf{x},\mathbf{z}),$$
<!-- where q represents variational posterior distribution and z represents the latent representation in hidden space. The key point of Eq.(2) is leveraging variational posterior dis- tribution qφ(z|x) to approximate true posterior distribu- tion pφ(z|x), which is generally intractable in practice. The detailed derivation of Eq.(2) can be found in the original paper [16]. The first term of Eq.(2) is the KL divergence between variational posterior distribution qφ(z|x) and true posterior distribution pθ(z|x), and the second term is de- noted as the (variational) evidence lower bound (ELBO) given that the KL divergence term is always non-negative. In practice, we usually maximize the ELBO to provide a tight lower bound for the original log(pθ(x)). The ELBO can also be rewritten as Eq.(3) in the following, -->
其中，$q$ 表示 variational posterior distribution，$z$ 表示 hidden space 中的 latent representation。上式的关键在于用 variational posterior distribution $q_\phi(z|x)$ 来近似真实的 posterior distribution $p_\theta(z|x)$，后者通常难以计算的。式中第一项是 variational posterior distribution $q_\phi(z|x)$ 和 true posterior distribution $p_\theta(z|x)$ 之间的 KL 散度，第二项是 evidence lower bound（ELBO），即 KL 散度项总是非负的。实际用的时候，通常最大化 ELBO 确定 log(p(x)) 的下界。ELBO 也可以写成下式：
$$\begin{aligned}\mathcal{L}(\theta,\phi;\mathbf{x},\mathbf{z})&=-D_{KL}\big(q_\phi(\mathbf{z}|\mathbf{x})\|p_\theta(\mathbf{z})\big)\\&+\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\big[\log p_\theta(\mathbf{x}|\mathbf{z})\big],\end{aligned}$$
<!-- where the conditional logarithmic likelihood Eqφ (z|x) [log pθ (x|z)] is in charge of the reconstruction, and the KL divergence reflects the distance between the variational posterior distribution qφ(z|x) and the prior distribution pθ(z). Generally, a standard Gaussian distribution N (0, I ) is chosen for pθ (z) so that the KL term actually imposes independent constraints on the representations learned through neural network [5], which may be the reason that VAE has the potential ability of disentanglement. -->
其中，条件对数似然 $q_\phi(z|x) [\log p_\theta(x|z)]$ 负责重构，KL 散度反映了 variational posterior distribution $q_\phi(z|x)$ 和 prior distribution $p_\theta(z)$ 之间的距离。通常，选择标准的高斯分布 $N(0, I)$ 作为 $p_\theta(z)$，这样 **KL 项实际上对通过神经网络学习到的 representations 施加了独立的约束**，这可能是 VAE 具有 disentanglement 能力的原因。
<!-- Although having the potential ability to disentangle, it has been observed that the vanilla VAE shows poor disen- tanglement capability on relatively complex datasets such as CelebA [45] and 3D Chairs [46] etc. To tackle this problem, a large amount of improvement has been proposed through adding implicit or explicit inductive bias to enhance disen- tanglement ability, resorting to various regularizers (e.g., β- VAE [6], DIP-VAE [35], and β-TCVAE [5] etc.). Specifically, to strengthen the independence constraint of the variational posterior distribution qφ(z|x), β-VAE [6] introduces a β penalty coefficient before the KL term in ELBO, where the updated objective function is shown in Eq.(4). -->
尽管 VAE 有 disentanglement 的潜力，但是在相对复杂的数据集上（例如 CelebA 和 3D Chairs）表现不佳。为了解决这个问题，通过添加 implicit 或 explicit 的 inductive bias 来增强 disentanglement 能力，使用各种 regularizers（如 β-VAE、DIP-VAE 和 β-TCVAE 等）。具体来说，为了加强 variational posterior distribution $q_\phi(z|x)$ 的独立性约束，β-VAE 在 ELBO 中的 KL 项前引入了一个 β penalty coefficient，更新后的目标函数如下：
$$\mathcal{L}(\theta,\phi,\mathbf{x},\mathbf{z},\beta)=\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\big[\log p_\theta(\mathbf{x}|\mathbf{z})\big]-\beta D_{KL}\big(q_\phi(\mathbf{z}|\mathbf{x})\|p_\theta(\mathbf{z})\big).$$
<!-- When β=1, β-VAE degenerates to the original VAE formula- tion. The experimental results of β-VAE [6] show that larger values of β encourage learning more disentangled represen- tations while harming the performance of reconstruction. Therefore, it is important to select an appropriate β to con- trol the trade-off between reconstruction accuracy and the quality of disentangling latent representations. To further investigate this trade-off phenomenon, Chen et al. [5] gives a more straightforward explanation from the perspective of ELBO decomposition. They prove that the penalty tends to increase dimension-wise independence of representation z but decrease the ability of z in preserving the information from input x -->
当 $\beta=1$ 时，β-VAE 退化为原始的 VAE。β-VAE 实验结果表明，较大的 $\beta$ 值鼓励学习更多的 disentangled representations，但会损害重构的性能。因此，需要权衡选择合适的 $\beta$ 值来控制重构精度和 disentangling latent representations。Chen 证明了 penalty 倾向于增加 representation $z$ 的 dimension-wise 独立性，但会降低 $z$ 保留来自输入 $x$ 信息的能力。
<!-- However, it is practically intractable to obtain the opti- mal β that balances the trade-off between reconstruction and disentanglement. To handle this problem, Burgess et al. [34] propose a simple modification, such that the quality of disentanglement can be improved as much as possible with- out losing too much information of the original data. They regard β-VAE objective as an optimization problem from the perspective of information bottleneck theory, whose objective function is shown in Eq.(5) as follows, -->
但是实际上很难获得最优的 $\beta$ 值来实现重构和 disentanglement 之间的 trade-off。Burgess 提出简单的修改，可在不丢失太多原始数据信息时尽可能提高 disentanglement 的质量，通过将 β-VAE 目标视为信息瓶颈理论（information bottleneck theory,）的优化问题，目标函数如下：
$$\max[I(Z;Y)-\beta I(X;Z)],$$
<!-- where X represents the original input to be compressed, Y represents the objective task, Z is the compressed repre- sentations for X , and I (; ) stands for mutual information. Recall the β-VAE framework, we can regard the first term in Eq.(4), Eqφ (z|x) [log pθ (x|z)] as I (Z ; Y ), and approximately treat the second term, DK L (qφ (z|x)∥pθ (z) as I (X ; Z ). To be specific, qφ(z|x) can be considered as the information bot- tleneck of the reconstruction task max Eqφ (z|x) [log pθ (x|z)]. DKL qφ(z|x)∥pθ(z) can be seen as an upper bound over the amount of information that qφ(z|x) can extract and preserve for original data x. The strategy is to gradually increase the information capacity of the latent channel, and the modified objective function is shown in Eq.(6) as follows, -->
其中，$X$ 表示要压缩的原始输入，$Y$ 表示目标任务，$Z$ 是 $X$ 的压缩表征，$I(;)$ 表示 mutual information。β-VAE 中，可以将式中的第一项 $\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\big[\log p_\theta(\mathbf{x}|\mathbf{z})\big]$ 视为 $I(Z;Y)$，将第二项 $D_{KL}\big(q_\phi(\mathbf{z}|\mathbf{x})\|p_\theta(\mathbf{z})\big)$ 视为 $I(X;Z)$。具体来说，$q_\phi(z|x)$ 可以被视为重构任务的信息瓶颈，$D_{KL}\big(q_\phi(\mathbf{z}|\mathbf{x})\|p_\theta(\mathbf{z})\big)$ 可以被视为 $q_\phi(z|x)$ 可以提取和保留原始数据 $x$ 的信息的上界。通过逐渐增加 latent channel 的信息容量，修改后的目标函数如下：
$$\begin{aligned}
\mathcal{L}(\theta,\phi,C;\mathbf{x},\mathbf{z})=& \mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\log p_\theta(\mathbf{x}|\mathbf{z})\boldsymbol{-}  \\
&\gamma\big|D_{KL}\big(q_{\phi}(\mathbf{z}|\mathbf{x})\|p_{\theta}(\mathbf{z})\big)-C\big|,
\end{aligned}$$
<!-- where γ and C are hyperparameters. During the training process, C will gradually increase from 0 to a value large enough to guarantee the expressiveness of latent repre- sentations, or in other words, to guarantee satisfactory re- construction quality when achieving good disentanglement quality. -->
其中，$\gamma$ 和 $C$ 是超参数。在训练过程中，$C$ 会从 $0$ 逐渐增加到足够大的值，以保证 latent representations 的表达能力，或者换句话说，当达到良好的 disentanglement 质量时，保证较好的重构质量。
<!-- Furthermore, DIP-VAE [35] proposes an extra regularizer to improve the ability to disentangle, with objective function shown in Eq.(7) as follows, -->
此外，DIP-VAE 提出了一个额外的 regularizer 来提高 disentanglement 能力，目标函数如下：
$$\max_{\theta,\phi}\mathbb{E}_{\mathbf{x}}\left[\mathbb{E}_{\mathbf{z}\sim q_\phi(\mathbf{z}|\mathbf{x})}[\left.\log p_\theta(\mathbf{x}|\mathbf{z})\right]-D_{KL}\left(q_\phi(\mathbf{z}|\mathbf{x})\|p_\theta(\mathbf{z})\right)\right]-\lambda D\big(q_\phi(\mathbf{z})\|p_\theta(\mathbf{z})\big),$$
<!-- where D(·∥·) represents distance function between qφ(z)
and pθ(z). The authors point out that qφ(z) should equal to Qj qj (zj) to guarantee the disentanglement. Given the assumption that pθ(z) follows the standard Gaussian dis- tribution N(0,I), the objective imposes independence con- straint on the variational posterior cumulative distribution qφ(z). In order to minimize the distance term, Kumar et al. match the covariance of qφ (z) and pθ (z) by decorrelating the dimensions of z ∼ qφ(z) given pθ(z) ∼ N(0,I), i.e., they force Eq.(8) to be close to the identity matrix, -->
其中，$D(·∥·)$ 表示 $q_\phi(z)$ 和 $p_\theta(z)$ 之间的距离函数。作者指出，为了保证 disentanglement，$q_\phi(z)$ 应该等于 $\prod_jq_j\left(\mathbf{z}_j\right)$。假设 $p_\theta(z)$ 服从标准高斯分布 $N(0, I)$，目标函数对 variational posterior cumulative distribution $q_\phi(z)$ 施加了独立性约束。为了最小化距离项，Kumar 等人通过 decorrelating $z \sim q_\phi(z)$ 的 dimensions 来匹配 $q_\phi(z)$ 和 $p_\theta(z)$ 的协方差，即强制式中的 $D_{KL}\big(q_\phi(\mathbf{z})\|p_\theta(\mathbf{z})\big)$ 接近单位阵：
$$\mathrm{Cov}_{q_\phi(\mathbf{z})}[\mathbf{z}]=\mathbb{E}_{p(\mathbf{x})}\left[\boldsymbol{\Sigma}_\phi(\mathbf{x})\right]+\mathrm{Cov}_{p(\mathbf{x})}\left[\boldsymbol{\mu}_\phi(\mathbf{x})\right],$$
<!-- where
VAE
N (μφ (x), Σφ (x)). Finally, they propose two variants, DIP- VAE-I and DIP-VAE-II, whose objective functions are shown in Eq.(9) and Eq.(10) respectively as follows, -->
其中，$\boldsymbol{\Sigma}_\phi(\mathbf{x})$ 和 $\boldsymbol{\mu}_\phi(\mathbf{x})$ 分别是 $q_\phi(\mathbf{z}|\mathbf{x})$ 的方差和均值（即 $q_\phi(\mathbf{z}|\mathbf{x})\sim N(\boldsymbol{\mu}_\phi(\mathbf{x}), \boldsymbol{\Sigma}_\phi(\mathbf{x}))$）。最后提出了两个变体，DIP-VAE-I 和 DIP-VAE-II，目标函数如下：
$$\begin{aligned}
\max_{\begin{array}{c}\theta,\phi\\\end{array}}\text{ELBO}(\theta,\phi)& -\lambda_{od}\sum_{i\neq j}\left[\mathrm{~Cov}_{p(\mathbf{x})}[\boldsymbol{\mu}_\phi(\mathbf{x})]\right]_{ij}^2  \\
&-\lambda_d\sum_i\left(\left[\operatorname{Cov}_{p(\mathbf{x})}[\boldsymbol{\mu}_\phi(\mathbf{x})]\right]_{ii}-1\right)^2, \\
\max_{\theta,\phi}\text{ELBO}(\theta,\phi)& -\lambda_{od}\sum_{i\neq j}\left[\mathrm{~Cov}_{q_\phi(\mathbf{z})}[\mathbf{z}]\right]_{ij}^2  \\
&-\lambda_d\sum_i\left(\left[\operatorname{Cov}_{q_\phi(\mathbf{z})}[\mathbf{z}]\right]_{ii}-1\right)^2,
\end{aligned}$$
<!-- where λd and λod are hyperparameters. DIP-VAE-I regular- izes Covp(x) μφ(x), while DIP-VAE-II directly regularizes Covqφ(z)[z]. -->
其中，$\lambda_d$ 和 $\lambda_{od}$ 是超参数。DIP-VAE-I 正则化了 $\operatorname{Cov}_{p(\mathbf{x})}[\boldsymbol{\mu}_\phi(\mathbf{x})]$，而 DIP-VAE-II 直接正则化了 $\operatorname{Cov}_{q_\phi(\mathbf{z})}[\mathbf{z}]$。

<!-- Kim et al. [7] propose FactorVAE which imposes inde- pendence constraint according to the definition of indepen-dence, as shown in Eq.(11), -->
Kim 等人提出了 FactorVAE，根据独立性的定义，对独立性约束进行了改进，如下：
$$\begin{aligned}
\begin{aligned}\frac1N\sum_{i=1}^N\left[\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x}^{(i)})}[\log p_\theta(\mathbf{x}^{(i)}|\mathbf{z})]\right.\end{aligned}& \left.-D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}^{(i)})\|p_\theta\left(\mathbf{z}\right)\right]  \\
&-\gamma D_{KL}\left(q_\phi(\mathbf{z})\|\bar{q}_\phi(\mathbf{z})\right)
\end{aligned}$$
<!-- where x i represent i-th sam
ple. DKL qφ(z)∥Qj qφ(zj) is called Total Correlation which evaluates the degree of dimension-wise independence in z. -->
其中，$\bar{q}_\phi(\mathbf{z})=\prod_jq_\phi\left(\mathbf{z}_j\right)$，，$\mathbf{x}^{(i)}$ 表示第 $i$ 个样本。$D_{KL}\big(q_\phi(\mathbf{z})\|\bar{q}_\phi(\mathbf{z})\big)$ 为 Total Correlation，用于评估 $z$ 的 dimension-wise 独立性程度。
<!-- Chen et al. [5] propose to elaborately decompose DK L  qφ (z|x)||pθ (z) into three terms, as is shown in Eq.(12). i) The first term demonstrates the mutual informa- tion which can be rewritten as Iq(z;x), ii) the second term denotes the total correlation and iii) the third term is the dimension-wise KL divergence. -->
Chen 等人提出将 $D_{KL}\big(q_\phi(\mathbf{z}|\mathbf{x})\|p_\theta(\mathbf{z})\big)$ 拆分为三个 term，如下：
$$\begin{aligned}
D_{K L}\left(q_\phi(\mathbf{z} \mid \mathbf{x}) \| p_\theta(\mathbf{z})\right)= & \underbrace{D_{K L}\left(q_\phi(\mathbf{z}, \mathbf{x}) \| q_\phi(\mathbf{z}) p_\theta(\mathbf{x})\right)}_{\text {(i) Mutual Information }} \\
& +\underbrace{D_{K L}\left(q_\phi(\mathbf{z}) \| \prod_j q_\phi\left(z_j\right)\right)}_{\text {(ii) Total Correlation }} \\
& +\underbrace{\sum_j D_{K L}\left(q_\phi\left(z_j\right) \| p_\theta\left(z_j\right)\right)}_{\text {(iii) Dimension-wise K L Divergence }} .
\end{aligned}$$
<!-- From Eq.(12), we can straightforwardly obtain the expla- nation of the trade-off in β-VAE, i.e., higher β tends to decrease Iq(z;x) which is related to the reconstruction quality, while increasing the independence in qφ(z) which is related to disentanglement. As such, instead of penalizing DKL qφ(z|x)||pθ(z) as a whole with coefficient β, we can penalize these three terms with three different coefficients respectively, which is referred as β-TCVAE and is shown in Eq.(13) as follows. -->
从上式可以直观地看出 β-VAE 中 trade-off 的解释，即较高的 $\beta$ 倾向于降低与重构质量相关的 $I_q(z;x)$，同时增加与 disentanglement 相关的 $q_\phi(z)$ 的独立性。因此，可以分别使用三个不同的系数对这三个 term 进行惩罚，即 β-TCVAE，如下：
$$\begin{aligned}\mathcal{L}=&\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})p_\theta(\mathbf{x})}\big[\log p_\theta(\mathbf{x}|\mathbf{z})\big]-\alpha I_q(\mathbf{z};\mathbf{x})\\&-\beta D_{KL}\big(q_\phi(\mathbf{z})\|\prod_jq_\phi(z_j)\big)-\gamma\sum_jD_{KL}\big(q_\phi(z_j)\|p_\theta(z_j)\big).\end{aligned}$$

<!-- To further distinguish between meaningful and noisy factors of variation, Kim et al. [33] propose Relevance Fac- tor VAE (RF-VAE) through introducing relevance indicator variables that are endowed with the ability to identify all meaningful factors of variation as well as the cardinality. The aforementioned VAE based methods are designed for continuous latent variables, failing to model the discrete variables. Dupont et al. [32] propose a β-VAE based frame- work, JointVAE, which is capable of disentangling both continuous and discrete representations in an unsupervised manner. The formulas of the two methods are supplemented in Tabel 1. -->
为了进一步区分 meaningful 和 noisy factors，Kim 等人通过引入 relevance indicator variables 来提出了 RF-VAE，这些 variables 具有识别所有 meaningful factors 的能力。上述 VAE-based 方法是为连续的 latent variables 设计的，无法建模离散的 variables。Dupont 等人提出了一个基于 β-VAE 的框架 JointVAE，能够以无监督的方式 disentangling 连续和离散的 representations。两种方法的公式如下表所示：