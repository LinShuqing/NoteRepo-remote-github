> 来自论文 - ChatGPT is not all you need. A State of the Art Review of large Generative AI models

1. 描述生成式 AI 如何部分的影响生成模型，同时对当前已发布的主要的生成模型做了分类

## Introduction

1. 专家系统基于知识库和推理引擎，且这种推理是通过 if else 实现的
2. 生成式 AI 则基于语料库或数据库生成判别器和生成器，判别器能够将输入映射到潜在的高维空间，生成器能够生成随机行为，实现无监督、半监督或有监督的生成行为。注意生成式 AI 和预测性机器学习如回归和分类不同。
3. 生成式模型的应用非常广泛。

## 生成式 AI 模型分类

根据输入和生成的形式对模型进行分类如下：![[Pasted image 20230218110937.png]]
一共有9类，而且包含的模型都是 21 年之后的，很新。

目前只有 6 家公司有能力做这种大型的生成式模型：google、openai、deepmind、meta ai、runway、nvidia。

### 文本生成图像

1. DALL-E2 ，来自 OpenAI，使用 CLIP 模型来预测给定图像下最相关的文本片段。为了生成图像，将一个先验模型和 CLIP 解码器组合，从而给定文本后，可以生成和 CLIP 得到的 embedding 最接近的图像。
2. IMAGEN：Google 提出，是一个由大型的 Transformer 语音模型组成的 文本生成图像 的 Diffusion 模型。Imagen 证明 增加语言模型的大小比增加图像 Diffusion 模型的大小更能提高样本保真度和图像文本对齐。
3. Stable Diffusion：开源 Diffusion 模型，使用 latent diffusion 模型，可以在 latent 空间中进行修改从而修改图像，主要包含两个部分：Text encoder 和 Image generator。
4. MUSE：基于 Transformer，比自回归或者 Diffusion 的生成效率更高，因为是在离散空间中基于掩码建模任务训练的。最大的优点就是速度快。

### 文本生成 3D 图像

1. DreamFusion：由 Google 开发，使用预训练的 2D 文本生成图像的Diffusion 模型来进行文本到 3D 图像的合成。Dreamfusion 用来自2D Diffusion 模型蒸馏的损失取代了先前的CLIP技术
2. Magic3D：来自 NVIDIA。Dreamfusion 模型存在两个问题：时间长、质量低。Magic3D 使用两阶段优化框架解决了这些问题。首先构建一个低分辨率的 diffusion prior，然后使用稀疏的3D哈希网格结构加速。然后进一步通过渲染优化 3D网格模型。

### 图像生成文本

1. Flamingo：DeepMind 创建的视觉语言模型，采用 few shot learning，只需要一些简单的输入提示（如对图像中的内容进行提问）就可以用在很多视觉和语言任务中。具体而言，Flamingo的输入包含具有视觉条件的自回归文本生成模型，能够提取与图像或视频交织的文本标记序列，并生成文本作为输出
2. VisualGPT：来自 OpenAI，利用了来自预训练语言模型GPT2的知识。为了弥合不同模态之间的语义差距，设计了一种新的编码器-解码器注意机制。模型的最大优点是它不需要像其他图像到文本模型那样多的数据。可以从 github访问 api。

### 文本生成视频

1. Phenaki：来自 Google，给定提示文本来合成视频。Phenak i是第一个可以从开放域下的可变时间提示生成视频的模型。为了解决数据问题，它在大型 图像-文本 对数据集以及少量 视频-文本 样本中进行联合训练。模型由三部分组成：C-ViViT encoder、transformer和 video generator。生成的视频可以长达几分钟，而模型则在1.4秒的视频上训练。可以从 github访问 api。
2. Soundify：Soundify是由Runway开发的一个将声音效果与视频相匹配的系统。

### 文本生成音频

1. AudioLM：来自 Google，AudioLM将输入音频映射到离散 的 token 序列中，并将音频生成看成是表征空间中的语言建模任务。通过对大量原始音频的进行训练，AudioLM学习在较短的 prompt 下生成自然连贯的音频。可以从 github访问 api。
2. Jukebox：OpenAI 开发，能够生成原始波形域下的音乐。通过分层VQ-VAE架构来解决 原始音频建模时的长距离依赖关系，将音频压缩到离散空间，设计损失函数来保留最多的信息量。仅限于英文歌曲。训练数据集来自LyricWiki的120万首歌曲。VQ-VAE有50亿个参数，在9秒音频剪辑上训练3天。可以从 github访问 api。
3. Whisper：OpenAI开发，音频生成文本，可以实现多语言语音识别、翻译和语言识别。基于680000小时的标记音频数据进行训练，encoder-deccoder transformer 结构。

### 文本生成文本

1. ChatGPT：来自 OpenAI，对话交互模型，基于 Transformer，通过强化学习进行训练。
2. LaMDA：Google，对话训练，基于 Transformer，137B个参数，并根据1.56T个单词的公共对话数据和web文本进行预训练。单个模型可以执行多个任务，可以生成好几个回答，然后根据一些安全规则进行过滤
3. PEER：Meta AI，协作式语言模型，用于帮助写作。基于四个步骤：Plan, Edit, Explain and Repeat，重复这四个步骤直到不再需要修改为止。主要采用 Wikipedia edit histories 数据进行训练。训练模型填充缺失的数据，然后根据这些合成数据再去训练其他模型。这样做的缺点是噪声很大，而且缺乏引用，于是通过一个并不总是有效的检索系统来弥补。整个框架是一个迭代过程。
4. Meta AI Speech from Brain：Meta AI开发的模型，用于帮助无法通过语音、打字或手势进行交流的人。模型试图从非侵入性大脑记录中直接解码语言。采用 wave2vec 2.0 自监督模型来识别听有声读物时志愿者大脑中复杂的言语表达，采用脑电图和脑磁图测量神经元活动

### 文本生成代码

1. OpenAI创建的AI系统，是一个通用编程模型。一般来说，编程可以分成两个部分：将问题分解为更简单的问题；将问题映射到已有的代码。第二部分是程序员最需要时间的部分，也是Codex最擅长的地方。模型从GPT-3进行了 fine tune，在openai网站上有 api。
2. Alphacode：是一个为需要更深入推理的问题生成代码的系统。模型受益于三个部分：广泛的数据集、基于 Transformer 的大型高效架构 和 大规模模型采样。通过 GitHub 库进行预训练，总计715.1GB代码（比 codex 多），架构方面，使用 encoder-decoder transformer 架构。

### 文本生成其他科研内容（如化学公式、蛋白质结构，解数学题等）

1. Galactica：由Meta AI和Papers with Code开发的自组织科学的模型
2. Minerva：使用逐步推理解决数学和科学问题的语言模型。

### 其他

1. Alphatensor：张量计算
2. GATO：机器人控制

## 总结和未来的工作

生成式大模型的构建限制有：
+ 大量的参数和数据集
+ 大量的计算时间和算力需求
+ 数据偏差
+ 道德约束
