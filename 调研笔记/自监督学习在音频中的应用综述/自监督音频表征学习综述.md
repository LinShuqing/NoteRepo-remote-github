
来自论文 - [[Self-Supervised Speech Representation Learning- A Review.pdf]]

1. 语音表征学习主要经历了三个相似的过程：生成、对比和预测  
2. 本文综述了自监督语音表征学习的方法及其与其他领域的联系



## Introduction

  
自监督学习（SSL）是无监督学习方法的一个子类，是一种利用从输入数据本身提取的信息作为标签来学习表征的技术。

在下游任务中使用自监督语音表征学习的框架如图：![[Pasted image 20221123205930.png]]框架分为两个阶段，第一阶段使用 SSL 预训练表征模型（上游模型），在第二阶段，下游任务直接使用 来自于frozen model 的表征，或在监督学习下微调预训练模型。

语音表征应该是 disentangled, invariant, 和 hierarchical 的：
+ 语音包含的信息很多，如 说话者身份、风格、情绪、周围噪音和通信信道噪音 等，因此学习 区分这些因素的表征非常重要
+ 特征要对背景噪声和通信信道中的变化的具有不变性
+ 声学、词汇和语义级别的学习特征层次结构可以支持不同需求的应用

## 表征学习的历史

### 聚类和混合模型

最初使用 EM 算法来学习语音表征。

早期的工作使用聚类方法，如 k-mean 聚类，通过寻找最接近数据的类来识别孤立词。

使用 GMM 来建模 subword， 可以对数据中的更多可变性进行建模。GMM最初用于上下文相关的音素建模；状态聚类算法的出现产生了上下文相关音素的GMM。

隐马尔可夫模型（HMM）这样的动态模型则允许处理连续语音。

也有研究侧重从生成模型中提取语音特征
+ Feature vectors：从 GMM模型的参数中得到
+ Fisher vectors：特征是对数似然相对于高斯混合模型参数（权重、均值和方差）的归一化梯度
他们可以用于语音识别和说话人识别。

### 层叠神经模型
> 神经模型能够提供更强大的分布式表征

早期技术包括 restricted Boltzmann machines、denoising autoencoders、noise contrastive estimation、sparse coding、energy-based methods 等

通过堆叠多个神经网络层可以逐步构建更高级的表征，但是这也增加了网络的复杂性。同时 NN 的目标函数非凸，也导致了更长的训练时间。

### 通过预设任务优化进行学习

最近的趋势是，学习某种网络来解决预设任务从而将输入映射到表征，其特点有：
+ 采用端到端而非逐层训练
+ 层数很深
+ 可以在各种任务上进行评估模型（如 NLP 的 GLUE，语音的 SUPERB）

预设任务的设计也是研究重点，任务应具有足够的挑战性，使模型能够学习高级抽象表征：
+ 早期包括：恢复黑白图像颜色、联合学习 潜在表示及其聚类分配、预测图像块的相对位置
+ 其他方法有：使用 VAE 重构输入 

在 SSL 和上下文相关的工作中，与 AE 相关的是根据部分信息生成对象。如ELMo、GPT系列、Megatron 和 基于BERT系列用句子中的前一个token来预测下一个token。另一个预设任务是对比学习，这个在 CV 中很流行。本文主要关注语音的预设任务优化技术。

### 其他工作

半监督学习：
+ 伪标签生成 PL

SSL和PL都利用未标记的纯语音数据。区别在于，PL中的是在模型预训练期间针对特定任务使用监督数据，这将模型的重点限制在单个（或几个）下游任务上。SSL 旨在学习与任务无关的表征，可以用于各种任务中。

迁移学习 TL：在一项任务上训练的模型获得的知识转移到不同但相关的任务；将其扩展到多任务学习可以获得更广泛的表征。
SSL可以被视为TL的一种类型，因为从预训练中学到的知识用于不同的下游任务。

## 语音表征学习 范式

相比于 NLP 和 CV，语音的特点有：
1. 语音为变长序列，用于 CV 的预设任务不能直接用在语音
2. 语音是没有段边界的长序列：相比于文本，语音的序列非常长，而且对于无标记的语音，没有明显的分段边界
3. 语音是连续的：对于文本，有 vocabulary 词汇表，训练预设任务时可以进行mask，但是语音是连续的，没有特定的 token
4. 语音处理的任务是多样的：为不同的语音处理任务构建泛化的自监督表征模型很有挑战

> 符号注记
> $f(\cdot)$ 为 encoder，$g(\cdot)$  为 decoder，或者也有，$f(\cdot)$ 为 预训练后的表征模型，$g(\cdot)$ 仅为支持预设任务的那部分。
> 给定声学输入 $X=\left\{x_1, x_2, \ldots, x_T\right\}$ ，encoder 输出表征 $H=\left\{h_1, h_2, \ldots, h_T\right\}$，通常使用输出表征 $H$，但是也有用 $f(\cdot)$ 的 hidden layer 的输出作为表征的。输入 $X$ 可以是原始音频，也可以是各种谱特征。

模型汇总：![[Pasted image 20221127105248.png]]

### 生成方法

#### Motivation
生成方法的预设任务是，基于有限的视图生成或重建输入数据，包括：
+ 过去预测未来
+ 预测 mask 的输入

> 和生成模型不一样，生成模型是生成新的数据。

#### 方法

##### Autoencoding
最常见的 AE 使用比输入特征更少的隐藏单元为潜在表征带来信息的 bottleneck。

其他模型则在潜在空间中增加正则以提高表征的质量：
+ 去噪自编码器（DAE）通过从被噪声破坏的输入中重建来学习潜在表征
+ 矢量量化变分自动编码器（VQ-VAE）用离散的潜在表征的后验分布扩展了原始变分自动编码器，强制使用离散的潜在表征 促使模型关注显著的输入特征。
	+ 将编码器输出的连续表征向量 $h_t$ 预设到 codebook 向量来量化
	+ 由于离散操作不可微，VQ-VAE 使用 straight-through estimator 来近似梯度，即编码器输出的梯度等于解码器输入的梯度，待学习的 codebook $A \in \mathbb{R}^{K \times D}$ （$K$ 为 codebook 大小，$D$ 为 $a_k$ 的特征维度），$h_t$ 的量化表征 $q_t$ 为：$q_t=a_k, \text { where } k=\arg \min _j\left\|h_t-a_j\right\|_2$ ，decoder $g(\cdot)$ 输入为 $q_t$ ，输出为 $x_t$。
	+ VQ-VAE 的损失为：$$\mathcal{L}=\underbrace{\log p\left(x_t \mid q_t\right)}_{\text {encoder }+\text { decoder }}+\underbrace{\left\|\operatorname{sg}\left[h_t\right]-A\right\|_2^2}_{\text {codebook }}+\beta \underbrace{\left\|h_t-\operatorname{sg}[A]\right\|_2^2}_{\text {encoder }}$$其中，$\operatorname{sg}[x]=x$ 为 stop-gradient 操作（也就是不计算这个部分的偏导）。

##### Autoregressive prediction
Autoregressive predictive coding（APC）使用过去信息预测未来信息。encoder $f(\cdot)$ 输入为 $X_{[1,t]}$ 输出潜在表征 $H_{[1,t]}$，decoder $g(\cdot)$ 为 prediction head，输入 $H_{[1,t]}$ 输出 $x_{t+c}$ ，$c$ 代表模型向前预测多少个 timestamp。联合学习 encoder 和 decoder 来最小化 $\hat{x}_{t+c}=g\left(h_t\right)$ 和 $x_{t+c}$ 的距离，用公式来说，APC 为：$$\begin{aligned}
H_{[1, t]} &=f\left(X_{[1, t]}\right) \\
\hat{x}_{t+c} &=g\left(h_t\right) \\
\mathcal{L}_t &=\left\|\hat{x}_{t+c}-x_{t+c}\right\|_1
\end{aligned}$$
在基于文本的语言模型中，$c=1$，但是在语音中邻近声学特征相似，需要学习所谓的慢特征（slow feature），因此简单地预测下一帧对于APC来说是一个很简单的任务；原始工作发现 $c=3$ 表现良好。

APC 的一个缺点是它只编码来自先前时间步的信息，而不是整个输入。DeCoAR 结合了ELMo 的双向性和 APC 的重建目标，允许对来自整个输入的信息进行编码，它使用前向LSTM $f_1(\cdot)$ 编码 $X_{[1,t]}$，后向 LSTM $f_2(\cdot)$ 编码 $X_{[t+k,T]}$：$$\begin{gathered}
H_{[1, t]}=f_1\left(X_{[1, t]}\right), \\
H_{[t+k, T]}^{\prime}=f_2\left(X_{[t+k, T]}\right), \\
\hat{X}_{[t+1, t+k-1]}=g\left(h_t, h_{t+k}^{\prime}\right) .
\end{gathered}$$
最后拼接 $h_t$ 和 $h_t^\prime$ 用于下游任务。

##### Masked Reconstruction
> mask 也可以用在对比和预测模型中，不过这里只讲生成模型

mask 重建在很大程度上受到BERT掩蔽语言模型（MLM）任务的启发。从高层次的角度来看，mask 重建的模型的训练阶段可以表示为：$$\begin{aligned}
H &=f(m(X)) \\
\hat{x}_t &=g\left(h_t\right) \\
\mathcal{L}_t &=\left\|\hat{x}_t-x_t\right\|_1
\end{aligned}$$函数 $m(\cdot)$ 定义为随机 mask 策略，$f(\cdot)$ 通常是 Transformer 的 encoder，也有用 RNN 的，decoder 是 prediction head。损失 $\mathcal{L}_t$ 通常仅针对被 mask 的时间步计算，以阻止模型学习恒等映射。

NLP 中的 mask 策略可以直接被用在语音中，但不够：
+ 在标准 BERT mask 中，每个 token 都是随机独立 mask 的。然而，对于语音来说，mask 单个帧的重构任务太简单了。由于音频信号的平滑性，模型可以学习简单地内插以重建 mask 帧。因此，通常 mask 连续帧。
+ pMPC 不仅 mask 固定数量的连续帧，而是根据话语中的语音分割来选择 mask 的语音帧，使得 mask 的区域对应于语言单元。
+ 当使用频谱输入特征时，语音也可以沿频率维度 mask，这种 mask 已被证明可以改善用于说话人分类的表征。

在非自回归预测编码（NPC）中，通过 masked convolution block 引入时间 masking。也有从 XLNet 的 shuffled version 中重建输入，以解决基于 mask 的方法的预训练和微调之间的差异。

正则化方法可以进一步改进掩模重建方法：
+ DeCoAR 2.0 使用矢量量化；
+ 在TERA模型中引入了 attention dropout 和 layer dropout 两种正则化方法。

##### 其他生成方法

PASE and PASE+  使用多个目标特征进行重构。  学习语音片段的 acoustic embeddings 的模型以未来和过去的频谱图片段、相位信息以及两个片段之间的时间间隔为目标。

#### 挑战
语音比文本有更多的信息，使得生成更加困难。一个可以完美重构输入的模型可能不一定学习了感兴趣的特征。

在设计预设任务时有许多选择。例如，要 mask 的连续帧数。这种选择会影响模型学习的内容。然而，关于任务设计与学习表征中编码的信息之间的关系的研究很少。

### 对比方法

#### Motivation
语音包含很多 entangle 的特征，直接建模语音可能不是发现contextualized latent factors of variations 的最佳方式，对比模型通过区分目标样本（正样本）和干扰样本（负样本）来学习自监督表征。

预设任务是最小化 anchor 和正样本之间的潜在距离，同时最大化到负样本的距离。

#### 方法
##### CPC
Contrastive Predictive Coding (CPC) 中，anchor representation $h_t$ 是通过直到时刻 $t$ 的上下文来获得的，正负样本是从模型学到的表征 $z_t$ 中采样得到：$$\begin{aligned}
Z &=f_1(X) \\
h_t &=f_2\left(Z_{[1, t]}\right) \\
\hat{z}_{t, k} &=g_k\left(h_t\right)
\end{aligned}$$其中，$f_1(\cdot)$ 为卷积网络，$g_k$ 为 step wise 的转换（线性回归层）。损失函数为最大化 $h_t$ 和 $z_t$ 互信息的下界，也被称为 InfoNCE：$$\mathcal{L}_{t, k}=-\log \left(\frac{\exp \left(\hat{z}_{t, k}^{\mathrm{T}} z_{t+k}\right)}{\sum_{n \sim \mathcal{D}} \exp \left(\hat{z}_{t, k}^{\mathrm{T}} z_n\right)}\right)$$其中，$\mathcal{D}$ 为一组索引，包括目标索 $t+k$ 和从 proposal distribution 中提取的负样本，其可以被认为是集合 $\{1, \ldots, T\}$ 上的均匀分布。损失索引 $k$ 表示CPC使用不同的投影层以多个偏移为目标。

wav2vec模型拓展 CPC，修改了损失以考虑二元预测任务：$$\mathcal{L}_{t, k}=-\log \left(\sigma\left(\hat{z}_{t, k}^{\mathrm{T}} z_{t+k}\right)\right)-\sum_{n \sim \mathcal{D}} \log \left(\sigma\left(-\hat{z}_{t, k}^{\mathrm{T}} z_n\right)\right)$$其中，$\sigma$ 为 sigmoid 函数。

##### wav2vec 2.0
wav2vec 2.0 模型结合了 mask 和对比学习。作为CPC模型，它使用InfoNCE损失来最小化上下文化表征和量化目标向量之间的距离。它将波形作为输入，并使用convolutional feature encoder和 transformer。Masking 应用于 transformer 输入处的卷积特征：$$\begin{aligned}
Z &=f_1(X) \\
H &=f_2(m(Z)) \\
q_t &=g\left(z_t\right)
\end{aligned}$$其中，$m(\cdot)$ 为 mask 策略，$f_1(\cdot)$ 为卷积网络，$f_2(\cdot)$ 为 transformer encoder，$g(\cdot)$ 为量化模块。
  
在wav2vec 2.0中，anchor 仅在 masked timesteps 取 $h_t$，正样本为在相同 timestep 的量化向量 $q_t$，负样本从其他 masked timesteps 中采样。量化目标允许在负样本采样时排除正样本 $q_t$ 。损失是：$$\mathcal{L}_t=-\log \left(\frac{\exp \left(S_{\mathrm{c}}\left(h_t, q_t\right)\right)}{\sum_{n \sim \mathcal{D}} \exp \left(S_{\mathrm{c}}\left(h_t, q_n\right)\right)}\right)$$其中，$S_{\mathrm{c}}(\cdot)$ 为余弦相似度。

由于学习表征的质量取决于量化的质量，wav2vec 2.0结合了两种技术来学习高质量的 codebook。首先，wav2vec 2.0在每个 time step 拼接来自多个 codebook 的量化表示，即所谓的乘积量化（PQ）。此外，训练损失还增加了辅助项，能够平等使用所有 codebook entries 。

wav2vec-C 方法在损失中添加一致性扩展了wav2vec 2.0，其目的是从学习的量化表征中重建输入特征。

#### 挑战
1. 用于定义正样本和负样本的策略也可以间接地将不变量施加到学习的表征上。
2. 由于语音输入不具有声学单元的显式分割，因此负样本和正样本并不代表整个语言单元。
3. 由于语音输入是平滑的，并且缺乏自然分割，因此很难定义一种采样策略能够保证以可靠的方式提供始终与 annchor 相关的样本

### 预测方法

#### Motivation
预测方法 mask 一段输入话语，然后输出被 mask 部分的概率分布

预测损失仅应用于 被 mask 的区域。

预测方法最开始来自于 BERT 及其相关研究的成功，语音表征学习中的预测方法从自监督DeepCluster方法中获得灵感

#### 方法

##### Discrete BERT
由于语音的连续性，将BERT用于语音是一个挑战。

DiscreteBERT方法受益于 vq-wav2vec 方法，给定输入的话语，首先从vq-wav2vec模型中提取离散单元序列，然后将其用于标准BERT模型。在学习过程中，仅使用 mask 预测损失来训练BERT模型，而不对vq-wav2vec模型进行梯度更新。

##### HuBERT
Hidden Unit BERT (HuBERT) 输入为连续波形而非离散单元。HuBERT 预测被 mask 的连续语音特征的 predetermined k-means cluster assignment。与wav2vec 2.0类似，HuBERT模型使用卷积编码器建模连续波形。然后在输入到 transformer 之前进行 mask。

由于训练的 target 是 cluster identities，HuBERT 采用正确的 k-means cluster 和预测的来计算交叉熵，损失在 mask 和 未 mask 的 time step 上进行计算：$$\begin{aligned}
\mathcal{L}_m &=\sum_{t \in M}-\log p\left(c_t \mid X\right) \\
\mathcal{L} &=\alpha \mathcal{L}_m+(1-\alpha) \mathcal{L}_u
\end{aligned}$$其中， $c_t$ 是 time step $t$ 的 k-means cluster，$M$ 是所有 mask 的 time step 的集合，$\mathcal{L}_u$ 代表为mask 的部分的损失，其计算和 $\mathcal{L}_m$ 差不多，只是 time step 的范围为 $M$ 的补集。

HuBERT模型被迫从连续输入中学习声学和语言模型。首先，模型需要将未 mask 的输入建模为有意义的连续潜在表征，这映射到经典的基于帧的声学建模问题。第二，该模型需要捕获学习表征之间的长距离时间关系，以减少预测误差。

  
HuBERT方法使用学习到的表征进一步 refines 目标单元。在训练初始 transformer 模型之后，使用 k-means 量化潜在的HuBERT特征，以提供用于第二次迭代训练的 target。HuBERT方法只需要两次迭代即可超过 SOTA。

##### WavLM
基于 HuBERT，WavLM 强调语音内容建模，同时保留说话人的一致性。

与 HuBERT 和 wav2vec 2.0 模型中使用的 context embedding 相比，WavLM 通过 content-based gated relative position bias 扩展了 transformer 的 self-attention，这提高了模型在识别任务上的能力。gates 基于当前语音内容自适应地调整 relative position bias。直观地说，如果某一帧无声，而另一帧属于语音片段，则两帧之间的相同距离偏移往往会发挥不同的作用。WavLM在可能的偏移范围内使用320个 bucketed positions。相对位置嵌入参数在所有层中共享。

同时 WavLM 框架提出了一种话语混合策略，通过构建来自不同说话者的部分重叠信号以增强训练数据。在 masking 预测预训练期间，将和主说话人对应的内容信息作为 target。

WavLM在语音分离、说话人验证和聚类分割方面实现了SOTA。

##### data2vec
> 这是一个可以用在语音、图像、文本的 general 的框架

data2vec 在给定输入的 mask 的情况下预测上下文潜在输入表征。预测目标 $y_t$ 是，使用相同模型的EMA对未 mash 的输入进行编码而生成的连续表征。通过平均 teacher network 的前k个 transformer 块的实例归一化（instance-normalized）表征来计算。Data2vec使用平滑的L1损失，该损失对异常值不太敏感，且可以避免梯度爆炸：$$\mathcal{L}\left(y_t, h_t\right)= \begin{cases}\frac{1}{2}\left(y_t-h_t\right)^2 / \beta, & \left|y_t-h_t\right| \leq \beta \\ \left|y_t-h_t\right|-\frac{1}{2} \beta, & \text { otherwise }\end{cases}$$其中，$h_t$ 的计算与wav2vec 2.0和HuBERT 类似，$\beta$ 为超参数。
  
data2vec方法对于语音表征学习以及视觉和文本表征都非常有效。

#### 挑战
1. HuBERT 和 wavLM 不适用于处理大量数据（因为有两个步骤）
2. 尽管 mask 预测损失使SSL模型不受其初始目标单元质量的限制，但实验表明，高质量的初始 teacher 能够实现更好的性能
3. 对于大模型，two-tower 方法 有一定的计算量

### 从多模态数据中学习

#### Motivation
> 本小节只关注从多模态数据中进行无监督表征学习

多模态的好处有：
+ 可以减少噪声的影响
+ 从语音数据中学习图像或视频等信号可以学习到编码了更多语义信息的表征

#### 方法
> 定义两大类方法，根据涉及的数据类型，分为：内在模态和外在模态

##### 内在模态（Intrinsic modalities）
内在模态指由语音源直接产生的模态，包括说话者面部的图像或视频、嘴唇运动、关节点等。从多个内在模态进行学习可以提高对噪声的鲁棒性，因为声学噪声可能与其他模态不相关。

这种类型的表征学习通常被称为“多视图学习”，因为多个内在模态可以被视为同一内容的多个视图。一些典型方法包括：
+ Multi-view autoencoders
+ Multi-modal DBM
+ Canonical correlation analysis (CCA)
+ Multi-view contrastive losses
+ audio-visual extensions of masked prediction methods（特别是 Audio-Visual HuBERT）

##### 外在模态（Extrinsic modalities）
外在模态 指 不是由同一来源产生但仍然为彼此提供上下文的模态，一个典型的例子是一幅图像及其字幕。（最近有很多相关的数据集）

典型的方法包括，为每个模态生成表征，然后使用对比损失，使得两种模态中 paired 样本尽可能相似，unpaired 数据尽可能远离。

对这些模型的分析发现，尽管将语音与相应的图像（或其他上下文模态）匹配是 high-level 的学习目标，但这些模型通常从最浅到最深的模型层学习多个 level 的语言表征。

也有研究使用 paired 和 unpaired 的数据联合学习音频和文本表征，如 SLAM。

#### 挑战
多模态方法的一个挑战是，它们所依赖的多模态数据通常比单模态少。此外，多模态数据通常来自特定领域，尚不清楚是否可以用在其他领域。

### Acoustic Word Embeddings
前面的大多数技术都旨在学习帧级表征，然而，有时候明确表示任意持续时间的长跨度语音音频（如电话、单词或短语级段）可能是有用的，有工作已经开始解决语音跨度的表征学习问题，特别是单词段，称为声学单词嵌入。

早期方法基于模板，后来使用了自编码器。确定两个单词段是否对应于同一个单词。

在自监督中，一个问题是单词（或更一般的片段）嵌入是否可以通过 pooling 自监督的帧级表征来得到。目前并不成功，需要进一步的工作。


## 自监督方法的一些 benchmark 

> 本节调查了可用于学习和评估语音表征的数据集

### 数据集

下表总结了 SSL 的数据集。
![[Pasted image 20221127105210.png]]

### 用于评估 SSL 的实验设置
对 SSL 进行测试的方法是，在下游任务上进行监督学习来微调预训练的 SSL 模型。下表给出了 SSL 的论文中实验设置：![[Pasted image 20221129152445.png]]
![[Pasted image 20221129152513.png]]
总结：
+ LS and WSJ  是最常见的预训练数据集
+ 数据集的大小在不断增加
+ 大多数侧重在英语，而汉语和多语言也越来越受到关注
+ 用于微调的数据集更为多样，涉及ASR、PR、PC、SID、AED、情绪、ST和LID等下游任务


### Benchmark 结果
在 ASR 中各种预训练模型的结果（WER 为评估指标）：![[Pasted image 20221129153344.png]]
结论：
+ 随着模型增大，性能增加
+ 与流行的半监督方法相比，wav2vec 2.0和HuBERT通过1小时甚至10分钟的有标签样本实现了较低的WER，非常适合低资源的场景

在其他任务的表现如下表：![[Pasted image 20221129153707.png]]

一些共享数据集、任务和参数配置的 benchmark：
+ SUPERB
+ LeBenchmark
+ ZeroSpeech

除了上面这些， 还有音频的 benchmark：
+ HEAR
+ NOSS
+ HARES

## 自监督表征分析

> 本节深入研究自监督表征，尝试回答以下问题：
> + 表征中的编码到底是什么
> + 表征对分布的鲁棒性、对数据的依赖性如何
> + 表征是否跨语言通用
> + 如何训练出强大的表征

### 信息内容
有论文针对不同网络层的声学语言信息内容，分析了wav2vec 2.0表征。使用了三种方法：
+ 典型相关分析（CCA）基于两个连续向量的线性投影的最大相关性来计算两个连续矢量之间的相似度得分，可以用于判断不同层的 embedding 彼此之间的相似性
+ 对连续表征向量进行聚类，计算 cluster ID和 phone 或 word labels 之间的离散互信息
+ 从网络中提取的表征向量用于执行简单的下游任务，特别是确定两个声学段是否对应于同一单词
最后发现，经过预训练的模型显示出自编码器风格的行为，早期层显示出与输入特征的强烈相似性，中间层发散更多，最终层恢复到与输入特征和早期层的更高相似性。

通常，wav2vec 2.0模型中的早期层编码声学信息。下一层对语音 类信息 进行编码，然后是词义信息，然后再返回到对语音/声学信息进行编码。

因此，从 需要语音或单词相关信息的任务 的最后一层提取表征可能不是最佳选择。

在不同的自监督学习和不同的模态中，中间层的类相关信息似乎很常见。
  
自监督表征可以编码除了语音类或单词之外的其他信息，例如信道、语言、说话者和情感信息；研究分析了 BERTphone 模型中的 speaker 和 language 信息，最终发现，language recognition 来自于高层的表征，而  speaker recognition 受益于第 6, 9, 和 12 层的信息，这表明，language 依赖于 high-level 的 phonetic，而 speaker 的信息组合使用 acoustic 和 phonetic 的信息。

在 WavLM模型 的类似研究中，小模型再次证实了较低层编码了 speaker diarization and verification 所需的说话人相关信息，而较高层编码语音和语义信息。
  
另一项研究在HuBERT模型的中间层而不仅是输出层使用自监督损失，以便更好地学习语音信息。最终的模型在 需要语音内容信息 的下游任务（如 phone recognition、ASR和 keyword spotting ）上确实更好，但在与说话人相关的任务（如 speaker diarization and verification ）上则更差。

### 训练准则
不同的训练准则（mask 预测、对比预测、自回归预测编码）对表征的性能也有影响。有研究表明，自回归预测编码损失与两项任务的下游性能表现出最强的相关性。

有论文比较了vq-vae和vq-wav2vec表征语音单位的能力，发现predictive vq-wav2vec模型比类似于 自编码器的vq-vae模型表现得更好，这很可能是由于其对时间动态的建模能力更强。

### 模型大小的影响
一些研究表明，当使用更大的数据集时，下游性能更好。

虽然数据大小和性能之间的精确关系尚未量化，但我们可以假设它遵循收益递减规律（或幂律），这与大多数数据密集型机器学习任务的观察结果类似。

除了数据集的大小，数据的多样性似乎也发挥了作用。

### 鲁棒性和可迁移性
MFCC 这种传统的语音特征缺乏对环境的鲁棒性，一个问题是，经过预训练的表征是否能提供更大的鲁棒性来抵抗分布变化。

有研究将 CPC模型的预训练表征与MFCC进行了比较，发现预训练表征对训练和测试数据之间的不匹配更为鲁棒。这里的分布变化（也就是不匹配的来源）可能源于声学（麦克风、室内混响）以及与主题和风格相关的词汇效应，以及说话人特征（如口音）的差异。

有文章使用来自六个不同领域的数据集更详细地研究了领域效应，结论是，在更多和不同的域上进行预训练是可取的：在保留的域上测试时，在更多的域上预训练的模型比在更少的域上预先训练的模型表现得更好。添加未标记数据可显著提高性能；即使是域外未标记的数据也是有帮助的。

有论文表明，自监督的预训练使模型能够学习跨语言通用的语境化语音特征。
对于训练数据集中不存在的语言，多语言表示也实现了竞争性能（比单语表示更低的字符错误率），再次表明无监督的预训练表示可以学习在不同语言之间通用的语音信号的通用特征

最后一个问题是，对从自监督模型的不同层提取的表征的解释 是否也适用于多语言环境。由论文基于多语言wav2vec 2.0 XLSR-53模型，在八种语言的音素识别实验表明，情况确实如此：音素错误率与单语（英语）场景相同，中间层的音素错误率低于前/后层。

## 从表征学习到零资源（略）


## 讨论 & 总结

李宏毅推荐的研究方向：
+ 探索更多在语音中使用表征学习的方法，如 adapter-based method、prompt/instruction learning 等
+ 提高表征模型的效率，更大的模型会带来更好的性能，但是需要大量的内存和高昂的训练成本，可以通过研究网络剪枝或者知识提取来对模型压缩；同时也可以考虑流式的表征模型以应用到流式场景中
+ 更鲁棒的模型。模型是否会在某些下游任务中失效，是否可以抵御对抗攻击等研究很少
+ 使用文本表征模型来改进语音表征。用于训练语音表征模型的语料库中的内容信息量远远少于文本。

SSL 模型有很大的发展空间，当SSL模型从语音中学习一般知识时，很容易在此基础上开发新的语音处理应用程序，其作用类似于操作系统，是很多应用的基石！！！
