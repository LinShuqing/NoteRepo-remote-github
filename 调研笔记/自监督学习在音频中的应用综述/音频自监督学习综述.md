# 音频和语音处理领域的自监督学习综述
> 大部分来自于论文   [[Audio Self-supervised Learning- A Survey.pdf]]

自监督学习（SSL）旨在从大规模数据中发现一般的表征，而无需人工标注。本文为 用于音频和语音处理的 SSL 方法综述，同时还总结了多模态 SSL 框架中 音频模态的工作，以及现有的评估基准。

## Introduction
1. SSL 旨在学习能够通过上游任务产生通用 representation 的模型。即直接从数据属性中学习 representation，而无需人工注释。
2. 下游任务是指，使用在上游任务上生成的预训练模型，提取特征 representation 以理解新数据，即，通过泛化预训练模型来理解新的上下文。
3. SSL 缓解了目前限制深度学习应用的两个困难
	1. 大量的人工注释：SSL 无需标签
	2. 需要为特定任务设计有效网络架构：只要上游任务可以产生合适的 representation，就可以被用于各种下游任务；同时还可以通过添加 “负样本” 来实现所谓的 对比学习
4. 语音的不确定性和多变性导致了 SSL 在音频领域的效果要差于 CV和 NLP
5. 好的 SSL 模型提取的 representation 应该满足以下特点：
	1. 分布式的，随着维度的增加其表征能力增强
	2. 抽象的，能够聚合更抽象的特征
	3. 可解耦的，向量中的每个元素都是有表征意义的

## 自监督学习概述
SSL 旨在通过解决上游任务而非使用人工注释来从大规模的数据中学习潜在的 representation。 SSL 模型训练后可在一定程度上概括其在潜在高维空间中的表示。在训练过程中通过比较相同的目标和其他目标（也称负样本），SSL 模型可以产生更具有区分性的 representation。

根据训练过程中是否采用负样本，可以将 SSL 分为两类：
1. 预测
2. 对比

同时由于预测模型可以采用对比损失作为目标函数，本文也考虑了基于regressive predictive codding （APC）和masked predictive coding （MPC）的contrastive predictive coding （CPC）。

### SSL 的视图生成

> 如何构造自监督学习任务

对于音频，可采用的音频视图生成方法有：
1. 来自于多传感器和跨模态
2. 采用数据增强技术创建音频信号的一个视图
3. 音频的时空相关性，如音频序列的帧
4. 局部特征和全局特征之前的关系，通过最大化互信息来实现SSL。

### 自监督学习框架
#### 预测模型
1. 预测 SSL 模型优化两个视图的表示之间的相似性或相关性，而不考虑它们与训练目标中负样本的相似性。如自编码器、孪生神经网络（Siamese Network）和聚类。

下面详细描述三个典型的预测 SSL 模型。

框架对比：![[Pasted image 20221122102534.png]]

##### Auto-encoding
基于 auto-encoder。标准的 AE 学习潜在的 representation，同时希望从潜在 representation 中重构原始输入。在AE中，潜在 representation 的维度选取很重要。

在AE-SSL中，模型可以不用预测整个原始样本，而仅仅预测失真部分，如 Word2Vec ，其中的 CBOW和 Skip-grams 模型分别用于预测给定上下文预测单词和给定单词预测上下文，用到语音中，这两种方法都可以被用于学习语音片段的固定长度向量表示，如 Audio2Vec和Speech2Vec。

AE 还可以用于根据已有的数据预测未来数据。自回归预测编码（APC）用于波形预测，采用L1损失基于上下文向量来预测下一个音频representation。掩膜预测编码（MPC）直接训练一个双向架构（如采用 Transformer结构），通过mask输入信号的某一部分，双向调整上下文来预测该部分的信号。非自回归预测编码（NPC）同样也对输入进行mask，但是基于局部而非全局来进行 representation 表征学习。

##### 孪生网络模型
模型具有“双塔”结构，每个“塔”处理数据的一个视图，目标是让同一个样本的视图之间的相似性尽可能的高，同时两个“塔”的架构相似或相同，参数可以共享也可以独立，如果引入负样本，引入对比损失使得同一个样本的不同视图应该尽可能的接近，负样本应该远离。

在不使用负样本时，孪生模型很容易出现模式崩塌的问题。但是这一部分还是主要讲述不采用负样本训练的孪生模型。

Boostrap Your Own Latent（BYOL）模型分别训练一个 online network 和一个 target network 网络。online network 有一个额外的pred层，通过优化（减少）pred 层输出和 target network 的 embedding 之间的距离，两个网络异步迭代更新。通过最小化MSE准则（两个网络的输出）进行优化：$$\begin{aligned}
L &=\left\|\overline{q_\theta}-\overline{p_{\xi}}\right\|_2^2 \\
&=2-2 \frac{<q_\theta, p_{\xi}>}{\left\|q_\theta\right\|_2 \cdot\left\|z_{\xi}\right\|_2}
\end{aligned}$$

SimSiam 模型和 BYOL 结构相似，但是删除了网络的投影层（Prj），target network 也没有使用反向传播进行优化，而是直接复制 online network ，同时还添加了额外的 learnable predictor 和 stop-gradient 操作以防止模式崩塌。

Barlow Twins（BT）是一种神经网络架构，BT 模型处理同一样本的两个 distorted 版本以生成其 representation。该模型测量了两个学习 representation 之间的互相关矩阵，目标是使得该矩阵接近于单位阵，从而使得样本的两个 distorted 版本的 representation 尽可能的接近。相比于 BYOL 和 SimSiam，BT 不需要 predictor layer、EMA 等操作。

三种孪生模型的对比如下：
![[Pasted image 20220929162314.png]]
##### 聚类
如 K-mean 聚类这一类的聚类方法为 SSL 提供了一种生成伪标签的方法。

1. Deep Cluster 迭代地执行两个步骤，首先，利用 K-means 聚类方法对 representation 进行分组，并为每个样本生成伪标签。然后，将创建的伪标签分配给每个样本，通过最小化分类损失（例如交叉熵损失）来优化编码器网络。
2. 除了采用K-mean 全局聚类，局部聚合（Local Aggregation）允许通过分别识别每个样本的近邻来对更灵活的统计结构进行建模。
3. SwAV 为另一种聚类方法，将在线聚类思想引入孪生模型中，避免了因两步训练导致的时间消耗，online clustering 提供伪标签。

#### 对比模型
在 SSL 模型中考虑负样本可以使 representations 更具区分性。背后的思想是，拉近两个相似输入（正样本对）的潜在表示，使不同输入（负样本对）的潜在表示远离。
##### 对比损失
1. 最大裕度对比损失计算如下：
$$\begin{aligned}
&L\left(x, x^{+}\right)=\sum_{x \in \mathscr{X}}\left\|f(x)-f\left(x^{+}\right)\right\|_2^2 \\
&L\left(x, x^{-}\right)=\sum_{x \in \mathscr{X}} \max \left(0, \epsilon-\left\|f(x)-f\left(x^{-}\right)\right\|_2\right)^2,
\end{aligned}$$其中，$x^+$ 表示和 $x$ 相似的样本（正样本），$x^-$ 表示负样本。$\epsilon$ 为超参数。
2. Triplet loss：用于计算正负对样本之间的偏移距离：
$$\begin{aligned}
&L\left(x, x^{+}, x^{-}\right) \\
&\quad=\sum_{x \in \mathscr{X}} \max \left(0,\left\|f(x)-f\left(x^{+}\right)\right\|_2^2-\left\|f(x)-f\left(x^{-}\right)\right\|_2^2+\epsilon\right)
\end{aligned}$$
3. Multi-Class N-pair ：在 Triplet loss 中允许在多个负样本之间进行联合比较（公式有点类似于 softmax 计算）：
$$\begin{aligned}
&L\left(x, x^{+}, x_{n \in[1,2 N-1]}^{-}\right) \\
&\quad=\log \left(1+\sum_{n=1}^{2 N-1} e^{f(x)^T f\left(x_n^{-}\right)-f(x)^T f\left(x^{+}\right)}\right) \\
&\quad=-\log \frac{e^{f(x)^T f\left(x^{+}\right)}}{e^{f(x)^T f\left(x^{+}\right)}+\sum_{n=1}^{2 N-1} e^{f(x)^T f\left(x_n^{-}\right)}} .
\end{aligned}$$
4. NT-Xent（normalised temperature-scaled crossentropy loss）引入一个额外的 temperature 参数来控制负样本的惩罚：$$\begin{aligned}
&L\left(x, x^{+}, x_{n \in[1, N-1]}^{-}\right) \\
&=\mathbb{E}\left[-\log \frac{e^{f(x)^T f\left(x^{+}\right)}}{e^{f(x)^T f\left(x^{+}\right)}+\sum_{n=1}^{N-1} e^{f(x)^T f\left(x_n^{-}\right)}}\right] \\
&
\end{aligned}$$分母项包括一个正样本和 $N-1$ 个负样本，对于正样本输出的数大，负样本输出小。

##### 孪生网络中的对比自监督学习

1. SimCLR：最开始用在图像中，对图像进行随机裁剪、resize、改色和模糊，然后把转换后的图像编码成 representation，采用 NT-Xnet 作为目标函数进行训练
2. Momentum Contrast（MoCo）：采用一个额外的 momentum encoder（和 encoder 共享参数），encoder 的更新和 SimCLR 差不多，momentum encoder 采用线性差值来更新，两者属于同步更新。
3. MoCo v2、SimCLR v2 把网络的规模加大、采用更强的数据增强等
4. MoCo v3 移除了 memory queue，在投影层之后采用了 prediction 层（类似于 BYOL），进一步提高了对表征的获取能力。
5. SSL 还可以从数据的多视图中获取高质量的 representation，在简单的下游模型中保持预测性能。
6. 对比损失的设计需要合理考虑 temperature 系数 $\tau$ 的选择，既要使特征可分离，又要有一定的裕度。

#### 对比预测编码（CPC）

CPC 利用自回归预测模型，根据过去帧中聚合的全局上下文信息来预测未来信息。同时利用负样本来提高 representation 的区分性，把 $x$ 用上下文向量 $c_t$ 来替代：$$L\left(c_t, z_{t+\tau}, z_{n \in[1, N-1]}^{-}\right)=\mathbb{E}\left[-\log \frac{e^{c_t^T z_{t+\tau}}}{e^{c_t^T z_{t+\tau}}+\sum_{n=1}^{N-1} e^{c_t^T z_n^{-}}}\right]$$
在 CPC v2 中，增加了模型的大小，使用 layer normalization 替代 batch normalization，同时引入 patch-based augmentation。

在掩膜预测模型中，也可以使用对比损失，目标函数为：$$L\left(c_t, z_t, z_{n \in[1, N-1]}^{-}\right)=\mathbb{E}\left[-\log \frac{e^{c_t^T z_t}}{e^{c_t^T z_t}+\sum_{n=1}^{N-1} e^{c_t^T z_n^{-}}}\right]$$

### 训练时 wi/wo 负样本

在 SSL 模型中，目标函数比网络结构更重要，且通过提高模型大小和 representation 大小可以提高 representation 的性能。

在使用对比学习时，负样本的质量和数量也很重要。不适用对比学习的通过减少数据视图之间的距离来优化，使用对比学习的旨在对比正负样本之间的距离。两种方法的优劣在于：
+ 没有负样本时，SSL 难以学习样本之间的区分性特征，但是可以有效的编码样本的完整信息
+ 有负样本时，SSL 可以学到更多的区分性特征，但是可能丢弃一些共有的属性（对于特征的区分性贡献不打，但是对于特征本身可能很重要）

所以，没有负样本的 SSL 更有用，但是有负样本的 SSL 非常适用于上游和下游任务接近时的情况，而且后期 fine-tune 也可以进一步改进模块。但是效果可能还是不如没有负样本的 SSL。

典型的 SSL 模型总结：![[Pasted image 20221122113849.png]]

## 音频自监督学习

LIM、COLA、CLAR 模型将 SimCLR 拓展到音频 representation 学习。
+ LIM 模型直接处理语音，最大化相同话语的语音块的 representation 的互信息
+ COLA 随机获取音频段，同时采用了一些数据增强方法，提出了 mix-back 方法来进行额外的增强
+ CLAR 使用原始音频和和增强后的音频作为模型输入的”视图对“，同时使用少量标注数据可以大幅提高收敛速度。

BYOL-A 在不使用负样本的情况下从单个音频中学习 representation。

Audio2Vec 和 Speech2Vec 使用 CBoW 和 skip-gram 的形式来学习音频 representation，不同之处在于：
+ Speech2Vec 强制音频段和单词进行对齐，这个过程中可能涉及监督学习；Audio2Vec 是完全无监督的
+ Speech2Vec 基于 RNN encoder-decoder；Audio2Vec 由 CNN 堆叠而成
+ Speec2Vec 输入是 Mel 谱，Audio2Vec 输入是 MFCC
+ Audio2Vec 中引入了 TemporalGap 公式，用于估计两个音频段之间的时间距离

有研究提出，通过训练模型来重新排列打乱后的音频 patch（也就是将排序作为任务）。

使用 auto-encoder ，mask一部分的音频输入来实现重构任务，通过最小化重构误差来优化模型。

Mockingjay 输入为 Mel 谱，利用 Transformer 学习随机 mask 的帧的编码表征。通过最小化 L1 重构损失来进行优化。

Audio ALBERT 和 Mockingjay 结构相同，但是 Transformer 的参数在所有层共享，可以实现更快的推理，且提高了训练速度。

TERA（Transformer Encoder Representations from Alteration） 拓展了 mask 的过程，包括随机替换、沿 channel 轴进行 mask、应用高斯噪声等。

DAPC 通过仅预测沿频率轴和时间轴的缺失分量来最小化 mask 部分的重构损失。

PASE 将 CNN encoder 和 多个 neural decoders（所谓的 workers） 结合，workers 从 encoder 中通过回归或者二分类任务来学习 representation；PASE+ 采用了额外的数据增强和更高效的 workers。同时把 CNN encoder 和 QRNN 进行组合来获取长时依赖。

对于音频 CPC，CNN 将原始音频编码成 representation，然后使用 GRU-RNN 聚合信息得到上下文向量，最重要的是使用了对比学习来获取更具区分性的特征。

CPC2 使用 LSTM-RNN 替换 GRU-RNN，使用 multi-head transformer 替换线性预测网络。

![[Pasted image 20221122194020.png]]
Wav2vec 调整 CPC 为全卷积结构，一个 CNN 用于从音频中产生 representation，其他的 用于捕获每个帧的全局上下文信息到一个上下文向量。通过最小化每个 step 的对比损失进行优化：$$L_k=-\sum_{i=1}^{T-k}\left(\operatorname { l o g } \sigma \left(z_{i+k}^T h_k\left(c_i\right)+\lambda \mathbb{E}\left[\log \sigma\left(-\tilde{z}^T h_k\left(c_i\right)\right]\right)\right.\right.$$总损失为 $K$ 个 step 的损失之和，欺诈 $h_k(c_i)$ 代表线性层，预训练完成后移除该层即可从原始波形获得表征。

VQ-Wav2vec 在 encoder 之后通过 矢量量化模块来得到离散的 representation，得到的结果送到上下文网络中，和 wav2vec 方法一样进行优化。采用了 Gumbel Softmax 作为 argmax 的近似来解决 argmax 引起的不连续问题。得到的离散表征用于训练 BERT 模型以得到最终的 representation。

Wav2vec 2.0 采用双向的 MPC 模型，使用对比损失进行优化（如 InfoNCE）。原始波形使用多个 1D-CNN 进行编码，对得到的 representation 进行部分 mask，然后送到 transformer 网络中进行预测。

wav2vec 模型及其变体在预训练过程中没有考虑下游任务，而是在训练完成后针对特定任务进行 fine-tune。

Wav2vec-U 在无监督情况下学习从音频表征到音素的映射，基于 GAN网络，生成器是 Wav2vec 2.0 来提取语音 representation，并基于此使用聚类来生成音素序列，生成的因素用于欺骗判别器（真的样本来自于无标签的本文中的音素序列）。

音频 SSL 方法汇总：
![[Pasted image 20221122203802.png]]

## 多模态音频表征

### 音频+视频

1. 音视频对应：L3-Net、AVE-Net 把一段视频分成视频流和音频流，分别输入到两个卷积网络中，预测音频和视频是否是对应的（也就是来自于同一个时间段）：
+ L3-Net 通过拼接 视觉和音频 representation 然后送到全卷积网络来进行预测得分
+ AVE-Net 的视觉和音频表示被设计成维度相同，通过计算两个向量的距离来测量对应度
 同时两个模型都会预测声音是从视频中的哪里产生的。
 因为两者都是二分类任务，所以可以使用 logistics 损失进行优化。
 
2. 音视频 源分离
3. 视频生成

### 音频+文本（略）

### 音频+视频+文本（略）

## 下游音频任务和 benchmark

一些 benchmark：
1. ZeroSpeech 
2. SuperB
3. LeBenchmark
4. Libri-Light
5. HEAR

## 碎碎念

1. SSL 生成的 representation 的质量取决于 预设任务的有效性，其中的关键在于 training targets or objectives