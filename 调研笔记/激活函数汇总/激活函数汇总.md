> https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/


激活函数（Activation Function）是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。类似于人类大脑中基于神经元的模型，激活函数最终决定了是否传递信号以及要发射给下一个神经元的内容。

激活函数可以分为**线性激活函数**（线性方程控制输入到输出的映射）以及**非线性激活函数**（非线性方程控制输入到输出的映射，比如 Sigmoid、Tanh、ReLU、LReLU、PReLU、Swish 等）。

## 为什么要使用 激活函数

因为神经网络中每一层的输入输出都是一个**线性**求和的过程，下一层的输出只是承接了上一层输入函数的线性变换，所以如果没有激活函数，那么无论你构造的神经网络多么复杂，有多少层，最后的输出都是输入的线性组合，纯粹的线性组合并不能够解决更为复杂的问题。而引入激活函数之后，我们会发现常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。

## 常见激活函数

### Sigmoid 函数

Sigmoid 函数也叫 Logistic 函数，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。

函数的表达式如下：
$$f(x)=\frac1{1+e^{-x}}$$
导数为：
$$f^{\prime}(x)=f(x)(1-f(x))$$
![](image/Pasted%20image%2020231103102056.png)


优点：
- Sigmoid 函数的输出范围是 0 到 1。非常适合作为模型的输出函数用于输出一个 0~1 范围内的概率值，比如用于表示二分类的类别或者用于表示置信度
- 梯度平滑，便于求导，也防止模型训练过程中出现突变的梯度

缺点：
+ 容易造成梯度消失
+ 函数计算不是以 0 为中心，降低更新效率


### Tanh函数

Tanh函数表达式为：
$$f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$
其导数为：
$$f^{\prime}(x)=1-{f(x)}^2$$
![](image/Pasted%20image%2020231103102817.png)
与sigmoid不同的是，tanh 是 0 为中心的。因此在实际应用中，tanh会比sigmoid 更好一些。但是在饱和神经元的情况下，tanh还是没有解决梯度消失问题。

优点：
+ 函数以 0 为中心

缺点：
+ 仍然存在梯度饱和的问题

### ReLU 函数

ReLU 激活函数的表达式为：
$$f(x)=\max(0,x)$$
其导数非常简单：
$$f(x)=\left.\left\{\begin{array}{ll}0&\text{if} \quad x<0\\1&\text{if}\quad x>0\end{array}\right.\right.$$
![](image/Pasted%20image%2020231103115351.png)

优点：
+ ReLU解决了梯度消失的问题，当输入值为正时，神经元不会饱和
+ 由于ReLU线性、非饱和的性质，在SGD中能够快速收敛
+ 计算复杂度低，不需要进行指数运算

缺点：
+ 输出不是以 0 为中心
+ 存在 Dead Neuron 的问题，输入为负时梯度为 0

### Leaky Relu函数

数学表达式为：
$$f(x)=\max(\alpha x,x)$$
其中 $\alpha$ 为超参数。

其导数和 ReLU 类似：
$$f(x)=\left.\left\{\begin{array}{ll}\alpha &\text{if} \quad x<0\\1&\text{if}\quad x>0\end{array}\right.\right.$$
![](image/Pasted%20image%2020231103120422.png)
优点：
+ 解决了ReLU输入值为负时神经元出现的死亡的问题

缺点：
+ $\alpha$ 需要调参
+ 有点近似线性，复杂分类中效果不太好

### PReLU 函数

PReLU 数学表达式为：
$$f(x)=\max(\alpha x,x)$$
其中 $\alpha$ 为学习得到的而非认为设定的。

其导数和 Leaky ReLU 一样：
$$f(x)=\left.\left\{\begin{array}{ll}\alpha &\text{if} \quad x<0\\1&\text{if}\quad x>0\end{array}\right.\right.$$
![](image/Pasted%20image%2020231103120723.png)

PReLU 激活函数也是用来解决 ReLU 带来的神经元坏死的问题。与Leaky ReLU激活函数不同的是，PRelu 激活函数负半轴的斜率参数 $\alpha$ 是通过学习得到的，而不是手动设置的恒定值。

### ELU 函数

ELU 函数的表达式为：
$$f(x)=\left.\left\{\begin{array}{ll}\alpha(e^x-1) &\text{if} \quad x<0\\1&\text{if}\quad x>0\end{array}\right.\right.$$
其导数为：
$$f(x)=\left.\left\{\begin{array}{ll}\alpha e^x &\text{if} \quad x<0\\1&\text{if}\quad x>0\end{array}\right.\right.$$
![](image/Pasted%20image%2020231104145158.png)

优点：
+ ELU 在较小的输入下会饱和至负值，从而减少前向传播的变异和信息
缺点：
+ 需要指数计算，效率低

### SELU 函数

SELU 表达式为：
$$f(x)=\lambda\left.\left\{\begin{array}{ll}\alpha(e^x-1) &\text{if} \quad x<0\\1&\text{if}\quad x>0\end{array}\right.\right.$$
其导数为：
$$f(x)=\lambda\left.\left\{\begin{array}{ll}\alpha e^x &\text{if} \quad x<0\\1&\text{if}\quad x>0\end{array}\right.\right.$$
![](image/Pasted%20image%2020231104145809.png)
SELU 允许构建一个映射 $g$，其性质能够实现 SNN（自归一化神经网络）。

SNN 不能通过 ReLU、sigmoid 、tanh 和 Leaky ReLU 实现。这个激活函数需要有：
+ 负值和正值，以便控制均值；
+ 饱和区域（导数趋近于零），以便抑制更低层中较大的方差；
+ 大于 1 的斜率，以便在更低层中的方差过小时增大方差；
+ 连续曲线。后者能确保一个固定点，其中方差抑制可通过方差增大来获得均衡。

通过乘上指数线性单元（ELU）来满足激活函数的这些性质，而且 $\lambda>1$ 能够确保正值净输入的斜率大于 1。

SELU激活函数是在自归一化网络中定义的，通过调整均值和方差来实现内部的归一化，这种内部归一化比外部归一化更快，这使得网络能够更快得收敛。

### Swish 函数

Swish 函数的表达式为：
$$f(x)=x\cdot \operatorname{sigmoid}(x)=\frac x{1+e^{-x}}$$
其导数为：
$$f(x)=\frac{1+e^{-x}+xe^{-x}}{\left(1+e^{-x}\right)^2}$$
![](image/Pasted%20image%2020231104151134.png)

Swish 激活函数**无界性**有助于防止慢速训练期间，梯度逐渐接近 0 并导致饱和；同时，**有界性**也是有优势的，因为有界激活函数可以具有很强的正则化(防止过拟合， 进而增强泛化能力)，并且较大的负输入问题也能解决

Swish激活函数在 $x=0$ 附近更为平滑，而非单调的特性增强了输入数据和要学习的权重的表达能力。

### GELU 函数

GELU 激活函数表达式为：
$$f(x)=0.5x\left(1+\tanh\left(\sqrt{2/\pi}(x+0.044715x^3)\right)\right)$$
> emmmmmmmmm

其导数为：
$$f(x)=0.5\mathrm{tanh}(0.0356774x^3+0.797885x)+0.0535161x^3+0.398942x)\mathrm{sech}^2(0.0356774x^3+0.797885x)+0.5$$

函数图像如图：
![](https://pic4.zhimg.com/80/v2-149a8bc6ee90f65205a8d408f79a0017_720w.webp)
导数图像如图：
![](https://pic1.zhimg.com/80/v2-06d94b711445480094eca59b8ec51804_720w.webp)

优点：
- 似乎是 NLP 领域的当前最佳；尤其在 Transformer 模型中表现最好；
- 能避免梯度消失问题