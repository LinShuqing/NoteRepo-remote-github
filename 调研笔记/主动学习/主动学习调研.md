# Overview of Active Learning for Deep Learning
> 来自博客：[Overview of Active Learning for Deep Learning (jacobgil.github.io)](https://jacobgil.github.io/deeplearning/activelearning)
> 主要是 翻译 + 一点点的个人理解

## Introduction

有监督学习需要：
1. 数据+标签
2. 通过优化模型来拟合数据和标签

现有的机器学习大多 focus 在上面第二点，通过开发算法从数据中学习。但是获取带标签的数据集太难了！耗时耗力耗钱。

作者认为，未来的很多创新将在如何快速开发高质量的带标签的数据集。

active learning 则是一个很好的方法。

## 动机

其实很多时候可以获得大量数据，但是对所有的数据打标签不可行，如：
+ 从网上爬了10亿张图片
+ 行车记录仪一个月的视频
+ 活组织扫描下的一百万个细胞

有两个密切相关的领域可以处理这些情况：
1. 半监督学习：利用未标记的数据学习特征
2. active learning：通过学习来选择将要标记的数据

## active learning：更少的数据标注+更高的精度

一个好的 AL 算法可以选择哪些数据要被标注，使得当在这些数据上训练时，效果会比在其他数据上训练要好。

### The unlabeled pool scenario

AL 中最常见的也是最贴近生活的情况是 unlabeled pool scenario，以图像为例：
+ 有大量的未标记数据
+ 玩一个回合制游戏
+ 每个回合，从 pool 中选择最佳的需要标记的图像，这个过程通常是对图像进行排序，然后选择排名靠前的图片
+ 标记这些图像然后加入训练集
+ 训练模型，通常模型是在新的数据集从零开始训练，实际也可以继续使用前面的模型训练
+ 重复直到达到  annotation budget（也就是最大可标记的图像的数量）

### 如何排序
> 一个很自然的问题就是，如何对图像进行排序（这也算是 AL 的本质 了）。
> 其实，最好的策略是选择那些使得模型预测错误的图像（**因为用这些图像训练模型肯定可以提高最终的性能**），但是问题是，我们并不知道真实的标签（**因为策略就是要选择哪些图片来进行标记**）。

有一个函数，输入是图像，输出是它的排名rank，这个函数被称为 acquisition function。

大多数的 AL 使用两种方法：
+ **Uncertainty sampling**：找到模型不确定的图像，作为错误图像的替代
+ **Diversity sampling**：在未标注的图像中找到那些多样性强的图片

##### Uncertainty sampling 的一些例子

1. 把 熵 作为一种 uncertainty，因为熵越大，代表模型对不同类别的判断能力越弱，此时 rank 会越高。
2. 把 变分比 $1-\max(p)$ 作为 uncertainty 
3. 把 模型最大的输出值和第二大的输出值之间的差异作为 uncertainty
4. 在 SVM 中，把 点 到 超平面的距离作为 uncertainty

### ## Query by committee （QBC）
略

## AL+DL

#### 基于贝叶斯

#### AL 的损失函数

#### AL 中的模式坍塌

