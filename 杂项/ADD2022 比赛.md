> 论文 - ADD 2022: THE FIRST AUDIO DEEP SYNTHESIS DETECTION CHALLENGE

1. ASV spoof 并没有 cover 真实场景下的欺诈情况，ADD 2022 来填补空白了
2. 包含三个赛道：
	1. 低质量虚假语音检测，LF，处理真实场景下的带噪语音
	2. 部分虚假语音检测，PF，只有部分语音是虚假的
	3. audio fake game，FG，竞争游戏，包含音频生成和鉴伪

## Introduction
1. ASVspoof 2021 中的 DF 只涉及 LA 类似的压缩音频，却忽略了真实场景的攻击，如噪声、在真实的音频中存在一部分的 fake clips，还有很多新的 VC 和SS的算法
2. 于是启动 ADD 2022

## 赛道

1. LF 赛道：包含带有各种真实世界的噪声和背景音乐的真实和虚假噪声
2. PF 赛道：目的是区分真假语音，其中部分虚假语音是通过使用真实或合成音频来操纵原始真实语音产生的
3. FG 赛道：两个任务
	1. 生成任务：生成虚假音频来欺骗下一个任务中的检测模型
	2. 检测任务：检测所有的音频，尤其是前面生成的，有两轮评估，第一轮评估包含未知的真实和 DF 音频；第二轮包含上一个任务提交生成的生成音频
> 这么看来，同时参数 FG 赛道的两个任务是更有优势的，可以同时、对抗 训练两个模型，从而提高两个模型的各自性能。

## 数据集

所有的赛道用的是相同的 training 和 dev set，但是用了不同的 adaptation 和 test set。
关于数据集的统计数据：
![[image/Pasted image 20230313192150.png]]

### Training and dev sets
基于 AISHELL-3 ，选了 40 个男性和女性 说话人，两个 set 的说话人不重叠。真实语音直接从这里选，虚假语音通过主流的语音合成和语音转换来生成。

### Adaptation sets
每个任务都有一个 adaption set：
赛道1：包含各种噪音的真实和完全虚假的语音
赛道2：包含由通过真实或合成音频操纵原始真实语音而产生的部分虚假语音
赛道3.2：它包括组织者制作的各种虚假音频

### Test sets
test sets 包含未知的真实和虚假语音：
赛道1：包含未知的各种噪音下的真实和完全虚假的语音
赛道2：包含未知的真实和部分虚假语音
赛道3.1：包含10个说话人 ID（来自 AISHELL3）
赛道3.2：第一轮的和赛道1一样，第二轮的第一轮的 test set 和部分 3.1 提交的音频

## 评估指标
除3.1是欺骗成功率（DSR），其他三个都是 EER。

### EER
ADD 2022 的 EER 是 threshold-free 的，定义为：$$\begin{aligned}
P_{f a}(\theta) & =\frac{\#\{\text { fake trials with score }>\theta\}}{\#\{\text { total fake trials }\}} \\
P_{\text {miss }}(\theta) & =\frac{\#\{\text { genuine trials with score }<\theta\}}{\#\{\text { total genuine trials }\}}
\end{aligned}$$
则，$E E R=P_{f a}\left(\theta_{E E R}\right)=P_{\text {miss }}\left(\theta_{E E R}\right) .$

3.2 任务有两轮，总的加权 EER 定义为：$$W E E R=\alpha * E E R_{-} R 1+\beta * E E R_{-} R 2$$
其中，$\alpha=0.4,\beta=0.6$。

### DSR
3.1 和 3.2 是分开进行评估的，3.1 用的是 DSR（deception success rate），反映了通过生成的语音来欺骗检测模型的程度，其定义如下：$$D S R=\frac{W}{A * N}$$
其中，$W$ 是所有的模型检测错误的总样本数，A 是所有用于评估的样本的数量，$N$ 是模型的数量。

## 挑战的结果

来自15个国家的120多个团队要求提供所有赛道的数据集。

### Baseline
有 6 个 baseline，采用 GMM、LCNN 和 RawNet2 训练 baseline 模型，输入为 LFCC 特征（RawNet2 用的是原始音频）。

只在 training set 或者 adaption set 上训练，仅使用 dev set 进行优化，没有进行任何的数据增强：![[image/Pasted image 20230313203200.png]]

### 结果分析
赛道1：![[image/Pasted image 20230313203309.png]]
所有提交的平均 EER 是31.7，第一是21.7，**第四第五竟然是 baseline，而且是 GMM + LFCC 的 baseline。而且还没比 baseline 好多少！！！而且GMM 的 baseline 比 LCNN 或 RawNet2好很多！！！**

赛道2：
![[image/Pasted image 20230314092952.png]]
平均 EER 为37.9，**最好的竟然能达到 4.8！！！21/27 支队伍优于最佳的 baseline S01！GMM 竟然还是 baseline 中最好的！！！** 当模型直接采用  training and adaptation sets 训练时，所有的 baseline 性能都变差了。

赛道3.2：![[image/Pasted image 20230314093536.png]]
平均 WEER 为34.2，最低的为 10.1，第一轮评估的平均是 20.7，第二轮是 43.1。

赛道3.1：![[image/Pasted image 20230314093756.png]]
有 14 个团队提交了生成音频，最好的为 93.8%，平均为 56.1%，说明语音鉴伪任重而道远！

## 总结

模型泛化性和评估指标是未来研究的重点！