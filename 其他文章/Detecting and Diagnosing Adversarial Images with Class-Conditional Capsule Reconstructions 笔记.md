
1. 提出基于对输入的 class-conditional 重构来进行对抗样本检测
2. 对抗样本比正常样本的重构误差更大，从而基于这个寻找阈值来进行检测
3. 同时提出重构攻击，既要能够误分类，又要使得重构误差小，对于CapsNets，由此产生的扰动会导致图像在视觉上看起来更像目标类
4. 这表明，CapsNets 利用了更符合人类感知的功能，并解决了对抗性示例提出的中心问题

## Introduction

1. 减少网络对对抗样本攻击的脆弱性的最好方法是进行对抗训练，也就是同时在 clean 和 adversarially perturbed 图像上进行训练
2. 还有一种方法是检测对抗输入而非对其进行分类

## 背景（略）

## 预备知识

对抗样本：给定 clean image $x$，对应的 label $y$，分类器 $f(\cdot)$，定义对抗样本为 $x^{\prime}=x+\delta$，且能够欺骗分类器从而实现错误分类 $f\left(x^{\prime}\right) \neq f(x)=y$，其中扰动 $\delta$ 很小以至于在视觉上看起来相似但是能使分类器发生误判。还有一种是要产生特定目标的对抗样本：$f\left(x^{\prime}\right)=t \neq y$。本文这里的 $t$ 是从其他的非 GT lable 种均匀选择的。在评估扰动大小时，采用 $\ell_{\infty}$ 和 $\ell_2$ 作为计算指标（约束 $\delta$ 不至于太大）。

Capsule Network：采用的是 Dynamic Routing 那篇论文的框架。

Threat 模型：之前的工作主要考虑白盒和黑盒两个模型，对于白盒攻击，攻击者完全知道模型本身和参数（也就是可以进行梯度计算），黑盒攻击只知道网络架构但不知参数，一般是重新训练和网络架构相同的替代模型然后攻击替代模型来生成对抗样本。

## 通过重构进行对抗图像检测
![](./image/Pasted%20image%2020230203114447.png)
### 模型

CapNet：输入为 capsule 的 pose ，将除了预测类的 pose 外的所有的值都mask 为0，通过计算输入和重构之间的欧式距离，基于重构网络来检测对抗攻击。具体而言，对于输入 $x$，CapNet 输出预测的类 $f(x)$ 以及 pose，重构网络接受 pose 作为输入，然后选择与预测类相对于的 pose，记为 $v_{f(x)}$，产生重构图像 $g(v_{f(x)})$，然后计算重构距离 $d(x)=\left\|g\left(v_{f(x)}\right), x\right\|_2$，和预先设定的检测阈值 $p$ 进行比较，如果 $d(x)>p$，则认为输入为对抗样本。如图b现实了真实图像和对抗样本的 $d(x)$ 的直方图差异。

CNN+CR：也可以拓展到 CNN，CR 代表 条件重构。

CNN+R：不进行 mask，直接重构

> 其实本文作者更大程度上是提供了一种对抗样本的检测思路，即通过进行重构来计算重构误差，以发现 clean 图像 和 对抗样本的重构误差之间的差异（阈值）来进行检测的。

### 检测阈值

检测阈值 $p$ 的选择设涉及到 FP 和 FN 之间的权衡。

### 评估指标

通过计算 即能够成功骗过模型有没有被发现的对抗样本的比例来衡量不同模型的性能（这个比例越小，说明模型越好）。

### 测试模型和数据集

三个模型参数量相同，训练相同的 epoch，没用进行超参数搜索。

## 结果

![](./image/Pasted%20image%2020230203120255.png)
capnet 好强啊！！！