> 损失函数 是用来度量模型的预测值和真实值的差异程度的函数。
> 在模型的训练阶段，通过优化损失函数，使模型的预测值和真实值尽可能地接近，从而打到学习的目的。

## 0-1 loss 损失

假设样本数为 $N$，模型的真实值为 $y_i$，预测值为 $f(x_i)$，0-1 loss 的思想很简单。如果预测值和实际值一样，则损失为 0，不一样则损失为 1：$$L_i= \begin{cases}1  & y_i\neq f(x_i) \\ 0 . & y_i=f(x_i)\end{cases}$$

## L2 损失（MSE）

假设样本数为 $N$，模型的真实值为 $y_i$，预测值为 $f(x_i)$，则 MSE 损失定义为：$$L = \frac{1}{N}\sum\limits_{i=1}^{N}(y_i-f(x_i))^2$$
## L1 损失（MAE）

假设样本数为 $N$，模型的真实值为 $y_i$，预测值为 $f(x_i)$，L1 损失定义为：$$L = \frac{1}{N}\sum\limits_{i=1}^{N}\vert y_i-f(x_i)\vert$$

## Smooth L1 损失
> 在 目标检测中 提出，使用该loss的模型有 Faster RCNN、SSD 等

在 L1 损失中，函数在零点附近不平滑且其导数为常数，即使在损失很小的时候梯度也可能很大，导致模型振荡不利于收敛，而 smooth L1 损失 可以解决这个问题。

假设样本数为 $N$，模型的真实值为 $y_i$，预测值为 $f(x_i)$，smooth L1 损失 定义为：$$L_i= \begin{cases}\frac{1}{2}(y_i-f(x_i))^2 & \vert y_i-f(x_i)\vert<1 \\ \vert y_i-f(x_i)\vert-\frac{1}{2} & \vert y_i-f(x_i)\vert\ge1\end{cases}$$

## Huber loss 损失
> 原理很简单，就是在误差接近 0 时使用MSE，误差较大时使用MAE

假设预测的输出用 $\hat{y_i}$ 表示。则 huber loss 定义为：$$J_{\text {huber }}=\frac{1}{N} \sum_{i=1}^N \mathbb{I}_{\left|y_i-\hat{y}_i\right| \leq \delta} \frac{\left(y_i-\hat{y}_i\right)^2}{2}+\mathbb{I}_{\left|y_i-\hat{y}_i\right|>\delta}\left(\delta\left|y_i-\hat{y}_i\right|-\frac{1}{2} \delta^2\right)$$
这里的 $\delta$ 代表切换的阈值。



## Hinge loss 损失
> hinge loss 只用于二分类问题中

假设二分类的标签为 $y=\pm1$ ，预测值为 $\hat{y}$，则 hinge loss 定义为：$$L=\max (0,1-y \cdot \hat{y})$$
当 label  为 $1$ 且 $\hat{y}\ge1$ 时，loss 为0，而当 $\hat{y}<1$ 时，loss 为 $1-\hat{y}$ ，对应的损失函数图像为：![[Pasted image 20221225190026.png]]

## KL 散度 和 交叉熵损失

设 $x$ 表示一个事件，$p(x)$ 表示事件发生的概率，则事件的信息量定义为：$$I(x)=-\log (p(x))$$
假设事件有 $n$ 种发生的可能，每种发生的概率为 $p(x_i),\sum_{i=1}^n p\left(x_i\right)=1$，则 熵 定义为：$$H(X)=-\sum_{i=1}^n p\left(x_i\right) \log \left(p\left(x_i\right)\right)$$
### KL 散度 

KL 散度用来衡量两个分布的相似程度，假设数据的真实分布为 $P(x)$，模型预测的分布为 $Q(x)$，则定义KL散度为：$$D_{K L}(p \| q)=\sum_{i=1}^n p\left(x_i\right) \log \left(\frac{p\left(x_i\right)}{q\left(x_i\right)}\right)$$
化简之后为：$$D_{K L}(p \| q)=\sum_{i=1}^n p\left(x_i\right) \log \left(p\left(x_i\right)\right)-p\left(x_i\right) \log \left(q\left(x_i\right)\right)$$
### 交叉熵

定义交叉熵 $H(p,q)$ 为：$$H(p,q)=-\sum_{i=1}^n p\left(x_i\right) \log \left(q\left(x_i\right)\right)$$
结合前面提到的信息熵，KL 损失可以写为：$$D_{K L}(p \| q)=H(p,q)-H(p)$$
所以 KL 散度其实就是交叉熵和信息熵的差，由于 $P(x)$ 为原始数据的分布，通常其信息熵为一个确定的未知数，所以通过最小化交叉熵损失就可以最小化 KL 散度。

实际上，在具体使用时，对于二分类任务，交叉熵为：$$L=\frac{1}{N} \sum_i L_i=\frac{1}{N} \sum_i-\left[y_i \cdot \log \left(p_i\right)+\left(1-y_i\right) \cdot \log \left(1-p_i\right)\right]$$
这里，当样本为正类时，$y_i=1$，当样本为负类时，$y_i=0$。

而对于多分类任务，交叉熵为：$$L=\frac{1}{N} \sum_i L_i=-\frac{1}{N} \sum_i \sum_{c=1}^n y_{i c} \log \left(p_{i c}\right)$$
其中，$n$ 为类别数，当样本 $i$ 的真实类别为 $c$ 时，$y_i=1$，反之 $y_i=0$ 。$p_{ic}$ 表示模型预测过程中的输出概率。

## Dice loss 损失
> 常用于图像分割领域，来自论文 - V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation

Dice系数是一种集合相似度度量函数，通常用于计算两个样本的相似度，取值范围在 $[0,1]$，给定集合 $X,Y$，Dice 系数定义为：$$s=\frac{2|X \bigcap Y|}{|X|+|Y|}$$
其中，$|\cdot|$ 表示取元素的个数，由此定义 Dice loss 如下：$$d=1-\frac{2|X \bigcap Y|}{|X|+|Y|}$$
在二分类问题中，Dice 系数等同于 F1 score，优化 Dice loss 是直接优化 F1 score。

## softmax 损失

softmax 损失函数为：
$$L=-\frac{1}{N} \sum_{i=1}^N \log \frac{e^{W_{y_i}^T x_i}}{\sum_{j=1}^n e^{W_j^T x_i}}$$
其中，$x_i \in \mathbb{R}^d$ 表示第 $i$ 个样本的特征，其真实类别为第 $y_i$ 类， $W \in \mathbb{R}^{d \times n}, W_j \in \mathbb{R}^d$ ，$n$ 为总的类别数，$N$ 为总的样本数。


### L-Softmax 损失
> 来自论文 - Large-Margin Softmax Loss for Convolutional Neural Networks（2017）

在一个二分类任务中，如果样本 $\boldsymbol{x}$ 属于类别 1，则原始的 softmax 要求 $W_1^T \boldsymbol{x}>W_2^T \boldsymbol{x}$，即 $\left\|\boldsymbol{W}_1\right\|\|\boldsymbol{x}\| \cos\left(\theta_1\right)>\left\|\boldsymbol{W}_2\right\|\|\boldsymbol{x}\| \cos \left(\theta_2\right)$。

为了生成更为严格的决策边界（增加判决裕度），要求 $\left\|\boldsymbol{W}_1\right\|\|\boldsymbol{x}\| \cos\left(m\theta_1\right)>\left\|\boldsymbol{W}_2\right\|\|\boldsymbol{x}\| \cos \left(\theta_2\right)$ ，其中 $0 \leq \theta_1 \leq \frac{\pi}{m}$ ，且 $m$ 是一个正整数，从而：
$$\begin{aligned}
\left\|\boldsymbol{W}_1\right\|\|\boldsymbol{x}\| \cos \left(\theta_1\right) & \geq\left\|\boldsymbol{W}_1\right\|\|\boldsymbol{x}\| \cos \left(m \theta_1\right) \\
&>\left\|\boldsymbol{W}_2\right\|\|\boldsymbol{x}\| \cos \left(\theta_2\right) .
\end{aligned}$$
能够有更大的裕度，基于此定义 L-softmax 损失为：
$$L_i=-\log \left(\frac{e^{\left\|\boldsymbol{W}_{y_i}\right\|\left\|\boldsymbol{x}_i\right\| \psi\left(\theta_{y_i}\right)}}{e^{\left\|\boldsymbol{W}_{y_i}\right\|\left\|\boldsymbol{x}_i\right\| \psi\left(\theta_{y_i}\right)}+\sum_{j \neq y_i} e^{\left\|\boldsymbol{W}_j\right\|\left\|\boldsymbol{x}_i\right\| \cos \left(\theta_j\right)}}\right)$$
其中：
$$\psi(\theta)=\left\{\begin{array}{l}
\cos (m \theta), \quad 0 \leq \theta \leq \frac{\pi}{m} \\
\mathcal{D}(\theta), \quad \frac{\pi}{m}<\theta \leq \pi
\end{array}\right.$$
为了简化前向后向传播，定义 $\psi(\theta_i)$ 为：$$\psi(\theta)=(-1)^k \cos (m \theta)-2 k, \quad \theta \in\left[\frac{k \pi}{m}, \frac{(k+1) \pi}{m}\right]$$
其中 $k \in[0, m-1]$ 且 $k$ 为整数。

几何解释如下：![[Pasted image 20221225114332.png]]

不同 margin 学习到的特征可视化如下：![[Pasted image 20221225112314.png]]
可以看到，margin 越大，学习的特征越紧凑。

### A-Softmax 损失
> 来自论文 - SphereFace: Deep Hypersphere Embedding for Face Recognition（2018）
> 其实和 L-softmax 几乎完全一样，唯一的区别是 A-softmax 对权值 $W$ 进行了归一化。而这一归一化导致了其可解释性，即特征点可以映射到单位超球面上，具体见 https://www.cnblogs.com/heguanyou/p/7503025.html

将原始的 softmax 写为：$$\begin{aligned}
L_i & =-\log \left(\frac{e^{\boldsymbol{W}_{y_i}^T \boldsymbol{x}_i+b_{y_i}}}{\sum_j e^{\boldsymbol{W}_j^T \boldsymbol{x}_i+b_j}}\right) \\
& =-\log \left(\frac{e^{\left\|\boldsymbol{W}_{y_i}\right\|\left\|\boldsymbol{x}_i\right\| \cos \left(\theta_{y_i, i}\right)+b_{y_i}}}{\sum_j e^{\left\|\boldsymbol{W}_j\right\|\left\|\boldsymbol{x}_i\right\| \cos \left(\theta_{j, i}\right)+b_j}}\right)
\end{aligned}$$A-softmax 决策界和 softmax 对比如下：
$$\begin{array}{|c|c|}
\hline \text { Loss Function } & \text { Decision Boundary } \\
\hline \hline \text { Softmax Loss } & \left(\boldsymbol{W}_1-\boldsymbol{W}_2\right) \boldsymbol{x}+b_1-b_2=0 \\
\hline \text { Modified Softmax Loss } & \|\boldsymbol{x}\|\left(\cos \theta_1-\cos \theta_2\right)=0 \\
\hline \text { A-Softmax Loss } & \begin{array}{l}
\|\boldsymbol{x}\|\left(\cos m \theta_1-\cos \theta_2\right)=0 \text { for class } 1 \\
\|\boldsymbol{x}\|\left(\cos \theta_1-\cos m \theta_2\right)=0 \text { for class } 2
\end{array} \\
\hline
\end{array}$$
modified softmax 损失就是归一化之后但是没加 margin 的softmax：$$L_{\text {modified }}=\frac{1}{N} \sum_i-\log \left(\frac{e^{\left\|\boldsymbol{x}_i\right\| \cos \left(\theta_{y_i, i}\right)}}{\sum_j e^{\left\|\boldsymbol{x}_i\right\| \cos \left(\theta_{j, i}\right)}}\right)$$
这里由于 $\left\|\boldsymbol{W}_j\right\|=1, \forall j$ ，然后忽略了偏置 $b$。

A-Softmax 对应的损失函数为：
$$L_{\text {ang }}=\frac{1}{N} \sum_i-\log \left(\frac{e^{\left\|\boldsymbol{x}_i\right\| \psi\left(\theta_{y_i, i}\right)}}{e^{\left\|\boldsymbol{x}_i\right\| \psi\left(\theta_{y_i, i}\right)}+\sum_{j \neq y_i} e^{\left\|\boldsymbol{x}_i\right\| \cos \left(\theta_{j, i}\right)}}\right)$$
其中：
$$\psi\left(\theta_{y_i, i}\right)=(-1)^k \cos \left(m \theta_{y_i, i}\right)-2 k$$
为单调递减函数。有 $\theta_{y_i, i} \in\left[\frac{k \pi}{m}, \frac{(k+1) \pi}{m}\right]$ 且 $k \in[0, m-1]$。

> 关于这个 $\psi\left(\theta_{y_i, i}\right)$ 函数的可视化的例子如图：![[Pasted image 20230106151738.png]]
> 可以看到，它是一个完全单调递减的函数（前面的 $(-1)^{a}$ 就是为了确保在 $[0,\frac{\pi}{m}]$ 范围外也单调递减）。

### Additive Margin Softmax 损失
> 来自论文 - Additive Margin Softmax for Face Verification（2018）

前面的那些损失都有一个 $\psi(\theta)$，这里作者直接定义为：$$\psi(\theta)=\cos \theta-m$$
相比于 A-softmax 和 L-softmax，作者给出的定义更简单。

最终给出的损失函数为：$$\begin{aligned}
{L}_{A M S} & =-\frac{1}{n} \sum_{i=1}^n \log \frac{e^{s \cdot\left(\cos \theta_{y_i}-m\right)}}{e^{s \cdot\left(\cos \theta_{y_i}-m\right)}+\sum_{j=1, j \neq y_i}^c e^{s \cdot \cos \theta_j}} \\
& =-\frac{1}{n} \sum_{i=1}^n \log \frac{e^{s \cdot\left(W_{y_i}^T \boldsymbol{f}_i-m\right)}}{e^{s \cdot\left(W_{y_i}^T \boldsymbol{f}_i-m\right)}+\sum_{j=1, j \neq y_i}^c e^{s W_j^T \boldsymbol{f}_i}}
\end{aligned}$$
> 可以看出，这里不是在 $\theta$ 上面做文章，而是直接修改了 $\cos(\theta)$ 。


### Additive Angular Margin 损失

> 来自论文 - ArcFace: Additive Angular Margin Loss for Deep Face Recognition（2018）
> 和前面两个的区别是，这个是加上一个 margin $m$，前面是乘。


传统的 softmax 损失函数没有明确地优化特征嵌入以强制类内样本具有更高相似性和类间样本具有多样性，导致在类内变化较大时性能发生下降。

在 softmax 中，将 logits 写为：$W_j^T x_i=\left\|W_j\right\|\left\|x_i\right\| \cos \theta_j$ ，其中 $\theta_j$ 代表权值 $W_j$ 和 特征 $x_j$ 之间的夹角，同时通过 $l2$ 归一化使得 $\left\|W_j\right\| = 1, \left\|x_i\right\| = s$，从而使得输出的 logits 只和角度 $\theta_j$ 有关，此时特征分布在以 $s$ 为半径的超球面上：
$$L_2=-\frac{1}{N} \sum_{i=1}^N \log \frac{e^{s \cos \theta_{y_i}}}{e^{s \cos \theta_{y_i}}+\sum_{j=1, j \neq y_i}^n e^{s \cos \theta_j}}$$
在此基础上，在 $x_i$ 和 $W_j$ 之间添加一个附加的角裕度惩罚项 $m$，来加强类内的一致性和类间的不一致性：
$$L_3=-\frac{1}{N} \sum_{i=1}^N \log \frac{e^{s\left(\cos \left(\theta_{y_i}+m\right)\right)}}{e^{s\left(\cos \left(\theta_{y_i}+m\right)\right)}+\sum_{j=1, j \neq y_i}^n e^{s \cos \theta_j}}$$

> Geodesic Distance：测地距离，地球表面两点之间的最短距离，也指数学空间的三维网格中，沿网格表面最短路径的距离（注意和欧式距离不一样）。

可以看出，附加角边缘惩罚等于归一化超球中的测地距离边缘惩罚。

结果：![[Pasted image 20221225102733.png]]
	可以看到，采用 AAM 损失的特征分布更为聚集。


### Large Margin Cosine 损失（LMCL）
> 和之前的区别是，这里是先计算 cos 然后减去一个 margin $m$

在 softmax 损失中，同样假设 $\left\|W_j\right\|=1$，则修改后的损失可以写为：$$L_{n s}=\frac{1}{N} \sum_i-\log \frac{e^{s \cos \left(\theta_{y_i, i}\right)}}{\sum_j e^{s \cos \left(\theta_{j, i}\right)}}$$
其中，$\|x\|=s$，$N$ 为样本数。模型学习的是 angular space 中的可分离特征。

定义 LMCL损失为：$$L_{l m c}=\frac{1}{N} \sum_i-\log \frac{e^{s\left(\cos \left(\theta_{y_i, i}\right)-m\right)}}{e^{s\left(\cos \left(\theta_{y_i, i}\right)-m\right)}+\sum_{j \neq y_i} e^{s \cos \left(\theta_{j, i}\right)}}$$
且满足：$$\begin{aligned}
W & =\frac{W^*}{\left\|W^*\right\|} \\
x & =\frac{x^*}{\left\|x^*\right\|} \\
\cos \left(\theta_j, i\right) & =W_j^T x_i
\end{aligned}$$


## Focal loss 损失
> 来自论文 - Focal Loss for Dense Object Detection，作者 何凯明

在二分类的交叉熵损失中，公式：$$L_i=-\left[y_i \cdot \log \left(p_i\right)+\left(1-y_i\right) \cdot \log \left(1-p_i\right)\right]$$又可以写为：$$L_i= \begin{cases}-\log (p_i) & \text { if } y_i=1 \\ -\log (1-p_i) & \text { if }  y_i=0\end{cases}$$
则 focal loss 将上述公式改进为：$$L_{i}= \begin{cases}-(1-p_i)^\gamma \log ({p_i}) & \text { if } {y}_i=1 \\ -{p_i}^\gamma \log (1-{p_i}) & \text { if } {y}_i=0\end{cases}$$
式中，$\gamma>0$ 为可以调节的因子（显然，当 $\gamma=0$ 时就退化成交叉熵损失了）。

考虑这样一种情况，当标签 $y_i=1$，预测的样本 $p_i$ 非常接近 $1$ 时，系数 $(1-p_i)^\gamma$ 会变得非常小，而对于分类不正确的样本也就是 $p_i$ 接近于 $0$ 时，相比于交叉熵损失几乎没有改变，这样相等于变相改变了模型分类的难以程度，对于难分的样本损失较大，对于简单易分的样本损失较小，下图很好地说明了这一情况：![[Pasted image 20221225205706.png]]
即 $\gamma$ 越大，曲线凸得越厉害。

同时添加一个平衡因子 $\alpha$，最终损失变成：$$L_{i}= \begin{cases}-\alpha_i(1-p_i)^\gamma \log ({p_i}) & \text { if } {y}_i=1 \\ -(1-\alpha_i){p_i}^\gamma \log (1-{p_i}) & \text { if } {y}_i=0\end{cases}$$
这里参数 $\alpha$ 的意义是，平衡不同类别之间的权重，实际中可以将其设置为此类出现的频率的倒数。 （也就是出现频率越少的类，分配的损失的权重越高）

## triplet loss 损失
> 在人脸识别领域，triplet loss常被用来提取人脸的embedding，也可以用于一些无监督和自监督任务的训练损失

triplet loss 主要用于训练差异性小的样本，其定义为：$$L_i=\max (d(a, p)-d(a, n)+\operatorname{margin}, 0)$$
输入是一个三元组，其中 $a$ 代表 anchor，$p$ 代表 positive正样本，$n$ 代表 negative  负样本，margin 是一个大于 0 的常数。

通过优化 triplet loss，最终的目标是减少 $a$ 和 $p$ 之间的距离，增大 $a$ 和 $n$ 之间的距离。

可以将 loss 分成三类，如下图：
+ easy triplet：此时 loss 计算为 0，因为 $d(a, p)+\operatorname{margin}<d(a, n)$
+ hard triplet：此时是最坏的情况，$d(a, n)<d(a, p)$ ，模型仍需训练（负样本竟然比正样本距 anchor 还近），这时候损失很大
+ semi-hard triplet：此时正样本比负样本远，但是远的还不够也就是还没达到 margin，$d(a, p)<d(a, n)<d(a, p)+\text { margin }$，这时候计算损失还是大于 0 的

![[Pasted image 20221226101807.png]]

triplet loss 的难点在于采样。

## center loss 损失
> 用于人脸识别，来自论文 - A Discriminative Feature Learning Approach for Deep Face Recognition
> 通常作为 softmax 损失的 正则化项。

center loss 的思想是，为每一个类别提供一个类别中心，最小化min-batch中每个样本与对应类别中心的距离，这样就可以达到缩小类内距离的目的。

其定义为：$$L=\frac{1}{2} \sum_{i=1}^N\left\|\mathbf{x}_i-\mathbf{c}_{y_i}\right\|_2^2$$
这里，$\mathbf{c}_{y_i} \in \mathbb{R}^d$ 代表类别 $y_i$ 的 特征中心。训练时，中心 $\mathbf{c}_{y_i}$ 随着特征的变化而不断变化。

$L$ 对 $\mathbf{x}$ 的梯度和中心 $\mathbf{c}$ 的更新如下：$$\begin{gathered}
\frac{{L}}{\mathbf{x}_i}=\mathbf{x}_i-\mathbf{c}_{y_i} \\
\Delta \mathbf{c}_j=\frac{\sum_{i=1}^m \delta\left(y_i=j\right)\left(\mathbf{c}_j-\mathbf{x}_i\right)}{1+\sum_{i=1}^m \delta\left(y_i=j\right)}
\end{gathered}$$
其中，$\delta(x)= \begin{cases}1, & x \text { is true } \\ 0, & x \text { is false }\end{cases}$ 。


## contrastive loss 损失

对比损失不是在所有的 sample 上计算损失，而是在一对 sample 上计算。设 $x_1,x_2$ 为一对输入样本，$y$ 为二分类的标签，$y=0$ 表示两个样本相似，$y=1$ 表示两个样本不相似，定义两个样本之间的带参距离 $D_W$ 为两个关于样本的函数 $G_W$ 之间的欧式距离：$$D_W\left(x_1, x_2\right)=\left\|G_W\left(x_1\right)-G_W\left(x_2\right)\right\|_2$$
简写为 $D_W$。则损失函数定义为：$$\begin{aligned}
\mathcal{L}(W) & =\sum_{i=1}^P L\left(W,\left(y, x_1, x_2\right)^i\right) \\
L\left(W,\left(y, x_1, x_2\right)^i\right) & =(1-y) L_S\left(D_W^i\right)+Y L_D\left(D_W^i\right)
\end{aligned}$$
其中，$P$ 为训练对的数量，$L_S$ 是相似点的损失函数，$L_D$ 是不相似点的损失函数，这两个损失需要满足对于最小化 $L$，会导致相似样本的 $D_W$ 小，不相似样本的 $D_W$ 大。

一个最常见的损失是：$$\begin{aligned}
& L\left(W, Y, x_1, x_2\right)= \\
& \quad(1-y) \frac{1}{2}\left(D_W\right)^2+(y) \frac{1}{2}\left\{\max \left(0, m-D_W\right)\right\}^2
\end{aligned}$$
其中，$m$ 是阈值。
> 最终的目的是，使得相似样本的距离尽可能地小，不相似样本地距离尽可能地大于阈值 $m$（大于 $m$ 后损失就为 $0$ 了）