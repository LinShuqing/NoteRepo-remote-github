
激活函数（Activation Function）是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式。类似于人类大脑中基于神经元的模型，激活函数最终决定了是否传递信号以及要发射给下一个神经元的内容。

激活函数可以分为**线性激活函数**（线性方程控制输入到输出的映射）以及**非线性激活函数**（非线性方程控制输入到输出的映射，比如 Sigmoid、Tanh、ReLU、LReLU、PReLU、Swish 等）。

## 为什么要使用 激活函数

因为神经网络中每一层的输入输出都是一个**线性**求和的过程，下一层的输出只是承接了上一层输入函数的线性变换，所以如果没有激活函数，那么无论你构造的神经网络多么复杂，有多少层，最后的输出都是输入的线性组合，纯粹的线性组合并不能够解决更为复杂的问题。而引入激活函数之后，我们会发现常见的激活函数都是非线性的，因此也会给神经元引入非线性元素，使得神经网络可以逼近其他的任何非线性函数，这样可以使得神经网络应用到更多非线性模型中。

## 常见激活函数

### Sigmoid 函数

Sigmoid 函数也叫 Logistic 函数，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。

函数的表达式如下：
$$f(x)=\frac1{1+e^{-x}}$$
导数为：
$$f^{\prime}(x)=f(x)(1-f(x))$$

**什么情况下适合使用Sigmoid？**

- Sigmoid 函数的输出范围是 0 到 1。非常适合作为模型的输出函数用于输出一个0~1范围内的概率值，比如用于表示二分类的类别或者用于表示置信度。
    
- 梯度平滑，便于求导，也防止模型训练过程中出现突变的梯度

![](image/Pasted%20image%2020231018115736.png)

### 