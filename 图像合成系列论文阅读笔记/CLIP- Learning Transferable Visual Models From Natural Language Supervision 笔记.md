> CLIP，openai，2021

<!-- 翻译 & 理解 -->
<!-- SOTA computer vision systems are trained to pre- dict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual con- cept. Learning directly from raw text about im- ages is a promising alternative which leverages a much broader source of supervision. We demon- strate that the simple pre-training task of predict- ing which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the inter- net. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study performance on over 30 different computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non- trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. -->
1. 本文证明：通过预测 image 和 caption 的对应关系，可以从 4 亿个（image, text）对中学习到 SOTA 的 image representations
2. 在预训练之后，可以使用文本来引用学习到的 visual concepts，从而实现模型的 zero-shot 迁移
3. 在 30 多个不同的 CV 数据集上测试，包括 OCR、视频 action recognition、定位和多种细粒度 object classification 任务
4. 模型在大多数任务上都可以进行 zero-shot 迁移，并且通常与 fully supervised baseline 相当，而无需任何特定数据集的训练

## Introduction
<!-- We demonstrate that a simplified version of ConVIRT trained from scratch, which we call CLIP, for Contrastive Language- Image Pre-training, is an efficient and scalable method of learning from natural language supervision. We find that CLIP learns to perform a wide set of tasks during pre- training including OCR, geo-localization, action recogni- tion, and outperforms the best publicly available ImageNet model while being more computationally efficient. We also find that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models. -->
1. 本文提出 ConVIRT 的简化模型，称为 CLIP（Contrastive Language-Image Pre-training），可以高效、可扩展地从 natural language supervision 中学习
2. CLIP 在预训练阶段学习了一系列任务，包括 OCR、geo-localization、action recognition，并且在计算效率上优于 ImageNet 模型

## 方法
<!-- At the core of our work is the idea of learning perception from the supervision contained in natural language paired with images. In the following subsections we detail our specific approach. -->
本文的核心思想是：从自然语言和图像配对的有监督数据中学习 perception

<!-- Creating a Sufficiently Large Dataset -->
### 创建足够大的数据集
<!-- Existing work has mainly used three datasets, MS-COCO (Lin et al., 2014), Visual Genome (Krishna et al., 2017), and YFCC100M (Thomee et al., 2016). While MS-COCO and Visual Genome are high quality crowd-labeled datasets, they are small by modern standards with approximately 100,000 training photos each. By comparison, other computer vision systems are trained on up to 3.5 billion Instagram photos (Mahajan et al., 2018). YFCC100M, at 100 million photos, is a possible alternative, but the metadata for each image is sparse and of varying quality. Many images use automati- cally generated filenames like 20160716 113957.JPG as “titles” or contain “descriptions” of camera exposure settings. After filtering to keep only images with natural language titles and/or descriptions in English, the dataset shrunk by a factor of 6 to only 15 million photos. This is approximately the same size as ImageNet. -->
现有工作主要使用了三个数据集：MS-COCO、Visual Genome 和 YFCC100M。
+ MS-COCO 和 Visual Genome 是高质量的众包标注数据集，但是规模较小，每个数据集大约有 10 万张图片。
+ YFCC100M 有 1 亿张图片，但每张图片的 metadata 很稀疏且质量不一，在筛选出只保留英文标题和/或描述的图片后，数据集缩小了 6 倍，只剩下 1500 万张图片
<!-- A major motivation for natural language supervision is the large quantities of data of this form available publicly on the internet. To test this we constructed a new dataset of 400 million (image, text) pairs collected form a variety of publicly available sources on the Internet. To attempt to cover as broad a set of visual concepts as possible, we search for (image, text) pairs as part of the construction process whose text includes one of a set of 500,000 queries. We approximately class balance the results by including up to 20,000 (image, text) pairs per query. The resulting dataset has a similar total word count as the WebText dataset used to train GPT-2. We refer to this dataset as WIT for WebImageText. 1 -->
作者构建了一个新的数据集，包含了 4 亿（image, text）对，数据来自互联网上的各种公开源。
> 为了平衡，在构建过程中搜索包含 50 万个查询中的一个的（image, text）对。每个查询最多包含 2 万个（image, text）。最终的数据集的总字数与用于训练 GPT-2 的 WebText 数据集相似。将这个数据集称为 WIT（WebImageText）。

<!-- Selecting an Efficient Pre-Training Method -->
### 选择高效的预训练方法
<!-- Our initial approach, similar to VirTex, jointly trained an image CNN and text transformer from scratch to predict the caption of an image. However, we encountered difficul- ties efficiently scaling this method. In Figure 2 we show that a 63 million parameter transformer language model, which already uses twice the compute of its ResNet50 im- age encoder, learns to recognize ImageNet classes three times slower than an approach similar to Joulin et al. (2016) that predicts a bag-of-words encoding of the same text. -->
作者尝试了类似 VirTex 的方法，从头开始联合训练 image CNN 和 text transformer 来预测 image 的 caption。但是发现这种方法难以扩展。
<!-- Recent work in contrastive representation learning has found that contrastive objectives can outperform the equivalent predictive objective (Tian et al., 2019). Noting this finding, we explored training a system to solve the potentially eas- ier proxy task of predicting only which text as a whole is paired with which image and not the exact words of that text. Starting with the same bag-of-words encoding baseline, we swapped the predictive objective for a contrastive objective in Figure 2, observed a further 4x efficiency improvement in the rate of zero-shot transfer to ImageNet. -->
对比学习目标优于预测学习目标。于是尝试预测 image 和 caption 的对应关系，而不是 caption 的具体内容。在相同的 bag-of-words 编码基线上将预测目标替换为对比目标，观察到了 zero-shot 迁移到 ImageNet 的速度进一步提高了 4 倍。
<!-- Given a batch of N (image, text) pairs, CLIP is trained to predict which of the N × N possible (image, text) pairings across a batch actually occurred. To do this, CLIP learns a multi-modal embedding space by jointly training an image encoder and text encoder to maximize the cosine similar- ity of the image and text embeddings of the N real pairs in the batch while minimizing the cosine similarity of the embeddings of the N 2 − N incorrect pairings. We optimize a symmetric cross entropy loss over these similarity scores. In Figure 3 we include pseudocode for the core of an imple- mentation of CLIP. This batch construction technique and objective was first introduced as the multi-class N-pair loss Sohn (2016) and was recently adapted for contrastive (text, image) representation learning in the domain of medical imaging by Zhang et al. (2020). -->
CLIP 训练时，预测一个 batch 中 N 个（image, text）对中实际的对应关系。CLIP 通过联合训练 image encoder 和 text encoder 来学习 multi-modal embedding space，最大化 batch 中 N 个真实对的 image 和 text embeddings 的 cosine 相似度，同时最小化 N^2 - N 个错误对的 cosine 相似度。损失函数为对称交叉熵损失。
<!-- Since over-fitting is not a major concern, the details of train- ing CLIP are simplified compared to Zhang et al. (2020). We train CLIP from scratch instead of initializing with pre- trained weights. We remove the non-linear projection be- tween the representation and the contrastive embedding space. We use only a linear projection to map from each en- coder’s representation to the multi-modal embedding space. We also remove the text transformation function tu which samples a single sentence at uniform from the text since many of the (image, text) pairs in CLIP’s pre-training dataset are only a single sentence. We also simplify the image trans- formation function tv . A random square crop from resized images is the only data augmentation used during training. Finally, the temperature parameter which controls the range of the logits in the softmax, τ , is directly optimized during training as a log-parameterized multiplicative scalar to avoid turning as a hyper-parameter. -->
一些训练细节：
+ 从头开始训练 CLIP，而不是使用预训练权重
+ 移除 representation 和 contrastive embedding space 之间的非线性 projection，只使用线性 projection
+ 移除 text transformation function $t_u$，因为 CLIP 的预训练数据集中的许多（image, text）对只有一个句子
+ 简化 image transformation function $t_v$，训练时只使用随机 square crop
+ 温度参数 $\tau$ 直接优化为 log 参数化的乘法标量，以避免作为超参数

<!-- Choosing and Scaling a Model -->
### 选择和扩展模型
<!-- We consider two different architectures for the image en- coder. For the first, we use ResNet50 (He et al., 2016a) as the base architecture for the image encoder due to its widespread adoption and proven performance. We make sev- eral modifications to the original version using the ResNetD improvements from He et al. (2019) and the antialiased rect-2 blur pooling from Zhang (2019). We also replace the global average pooling layer with an attention pooling mechanism. The attention pooling is implemented as a sin- gle layer of “transformer-style” multi-head QKV attention where the query is conditioned on the global average-pooled representation of the image. For the second architecture, we experiment with the recently introduced Vision Transformer (ViT) (Dosovitskiy et al., 2020). We closely follow their implementation with only the minor modification of adding an additional layer normalization to the combined patch and position embeddings before the transformer and use a slightly different initialization scheme. -->
考虑了两种不同的 image encoder 架构：
+ 使用 ResNet50 作为 image encoder 的基础架构，对原始版本进行了一些修改，使用了 ResNetD 的改进和 antialiased rect-2 blur pooling，用 attention pooling 机制替换了 global average pooling 层
+ 使用最近引入的 Vision Transformer（ViT）作为 image encoder，只对其实现进行了微小修改，添加了额外的 layer normalization 到 patch 和 position embeddings 前的 transformer，初始化方法不同
<!-- The text encoder is a Transformer (Vaswani et al., 2017) with the architecture modifications described in Radford et al. (2019). As a base size we use a 12-layer 512-wide model with 8 attention heads. The transformer operates on a lower-cased byte pair encoding (BPE) representation of the text (Sennrich et al., 2015). The text sequence is bracketed with [SOS] and [EOS] tokens and the activations of the highest layer of the transformer at the [EOS] token are used as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space. Masked self-attention was used in the text encoder to preserve the ability to add language modeling as an auxiliary objective, though exploration of this is left as future work. -->
text encoder 是一个 Transformer，采用 12 层 512 宽的模型，8 个 attention heads。输入为 text 的 lower-cased byte pair encoding（BPE）。text sequence 前后加上 [SOS] 和 [EOS] token，模型把 [EOS] token 的输出作为 text 的 feature representation，然后 layer normalized 并线性投影到 multi-modal embedding space。
<!-- While previous computer vision research has often scaled models by increasing the width (Mahajan et al., 2018) or depth (He et al., 2016a) in isolation, for the ResNet image encoders we adapt the approach of Tan & Le (2019) which found that allocating additional compute across all of width, depth, and resolution outperforms allocating it to only one dimension. We use a simple variant which allocates addi- tional compute equally to increasing the width, depth, and resolution of the model. For the text encoder, we only scale the width of the model to be proportional to the calculated increase in width of the ResNet and do not scale the depth at all, as we found CLIP’s performance to be less sensitive to the text encoder. -->
对于 ResNet image encoder，将额外的计算资源平均分配到增加宽度、深度和分辨率上。对于 text encoder，只增加了模型的宽度，与 ResNet 的宽度增加成比例，没有增加深度。

### 预训练
<!-- We train a series of 5 ResNets and 3 Vision Transformers. For the ResNets we train a ResNet50, a ResNet101, and then 3 more which follow EfficientNet-style model scaling and use approximately 4x, 16x, and 64x the compute of a ResNet50. They are denoted as RN50x4, RN50x16, and RN50x64 respectively. For the Vision Transformers we train a ViT-B/32, a ViT-B/16, and a ViT-L/14. The largest ResNet model, RN50x64, took 18 days to train on 592 V100 GPUs while the largest Vision Transformer took 12 days on 256 V100 GPUs. For the ViT-L/14 we also pre-train at a higher 336 pixel resolution for one additional epoch to boost performance similar to FixRes (Touvron et al., 2019). We denote this model as ViT-L/14@336px. Unless otherwise specified, all results reported in this paper as “CLIP” use this model which we found to perform best. Full model hyperparameters and details are in supplementary material. -->
训练了 5 个 ResNets 和 3 个 Vision Transformers：
+ ResNets：ResNet50、ResNet101，以及 3 个遵循 EfficientNet 风格的模型，分别使用了大约 4 倍、16 倍和 64 倍 ResNet50 的计算资源
+ Vision Transformers：ViT-B/32、ViT-B/16 和 ViT-L/14

最大的 ResNet 模型 RN50x64 在 592 个 V100 GPU 上训练了 18 天，最大的 Vision Transformer 在 256 个 V100 GPU 上训练了 12 天。对于 ViT-L/14，还在更高的 336 像素分辨率上额外训练了一个 epoch，以提高性能。

### 使用 CLIP
<!-- CLIP is pre-trained to predict if an image and a text snip- pet are paired together in WIT. To apply CLIP to down- stream tasks, we reuse this capability and study the zero- shot transfer performance of CLIP on standard computer vision datasets. Similar to Radford et al. (2019) we motivate this as a way of measuring the task learning capability of a system (as opposed to its representation learning capability). For each dataset, we use the names of all the classes in the dataset as the set of potential text pairings and predict the most probable (image, text) pair according to CLIP. We addi- tionally experiment with providing CLIP with text prompts to help specify the task as well as ensembling multiple of these templates in order to boost performance. However, since the vast majority of unsupervised and self-supervised computer vision research focuses on representation learning, we also investigate this for CLIP using the common linear probe protocol. -->

## 分析（略）
