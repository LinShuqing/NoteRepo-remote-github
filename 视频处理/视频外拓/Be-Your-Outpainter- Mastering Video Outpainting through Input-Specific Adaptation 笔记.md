> ECCV 2024，MMLab, CUHK，Avolution AI

<!-- 翻译 & 理解 -->
<!-- Abstract. Video outpainting is a challenging task, aiming at generating video content outside the viewport of the input video while maintaining inter-frame and intra-frame consistency. Existing methods fall short in either generation quality or flexibility. We introduce MOTIA (Mastering Video Outpainting Through Input-Specific Adaptation), a diffusion- based pipeline that leverages both the intrinsic data-specific patterns of the source video and the image/video generative prior for effective outpainting. MOTIA comprises two main phases: input-specific adapta- tion and pattern-aware outpainting. The input-specific adaptation phase involves conducting efficient and effective pseudo outpainting learning on the single-shot source video. This process encourages the model to identify and learn patterns within the source video, as well as bridg- ing the gap between standard generative processes and outpainting. The subsequent phase, pattern-aware outpainting, is dedicated to the gen- eralization of these learned patterns to generate outpainting outcomes. Additional strategies including spatial-aware insertion and noise travel are proposed to better leverage the diffusion model’s generative prior and the acquired video patterns from source videos. Extensive evaluations underscore MOTIA’s superiority, outperforming existing state-of-the-art methods in widely recognized benchmarks. Notably, these advancements are achieved without necessitating extensive, task-specific tuning. -->
1. 现有的 video outpainting 方法在生成质量或灵活性方面存在不足
2. 引入 MOTIA (Mastering Video Outpainting Through Input-Specific Adaptation)，一个基于 diffusion 的 pipeline，利用源视频的内在数据特定模式和图像/视频生成先验来进行 outpainting，包含两个主要阶段：
    + input-specific adaptation：在单次 source video 上进行 pseudo outpainting 学习，让模型识别和学习 source video 中的模式
    + pattern-aware outpainting：将学习到的模式泛化，生成 outpainting 结果
3. 提出 spatial-aware insertion 和 noise travel 等策略，更好地利用 diffusion model 的生成先验和从 source videos 中获得的 video patterns
4. MOTIA 超过现有的 SOTA 方法，而无需进行大量的 task-specific tuning

## Introduction
<!-- Currently, there are two primary approaches to video outpainting. The first employs optical flows and specialized warping techniques to extend video frames, involving complex computations and carefully tailored hyperparameters to en- sure the added content remains consistent [6, 8]. However, their results are far from satisfactory, suffering from blurred content. The other type of approach in video outpainting revolves around training specialized models tailored for video inpainting and outpainting with extensive datasets [7, 33]. However, they have two notable limitations: 1) An obvious drawback of these models is their depen- dency on the types of masks and the resolutions of videos they can handle, which significantly constrains their versatility and effectiveness in real-world applica- tions, as they may not be adequately equipped to deal with the diverse range of video formats and resolutions commonly encountered in practical scenarios. 2) The other drawback is their inability to out-domain video outpainting, even intensively trained on massive video data. Fig. 2 shows a failure example of most advanced previous work [7] that the model faces complete outpainting failure, with only blurred corners. We show that a crucial reason behind this is that the model fails at capturing the intrinsic data-specific patterns from out-domain source (input) videos. -->
1. 目前，video outpainting 主要有两种方法：
    + 第一种使用 optical flows 和专门的 warping 技术来扩展视频帧，需要复杂的计算和精心设计的超参数来确保添加的内容保持一致。然而，结果并不理想，内容模糊
    + 另一种方法是训练专门的模型，针对 video inpainting 和 outpainting 进行训练，使用大量数据集。然而，这种方法有两个明显的局限：
        1. 依赖于 mask 的类型和视频的分辨率，限制了其在实际应用中的多样性和有效性
        2. 无法 out-domain video outpainting，即使在大量视频数据上进行了密集训练