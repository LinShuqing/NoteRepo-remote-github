# A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT
> 2023年，主要看了 NLP 相关的部分

1. PFM 是指在不同数据类型、不同下游任务中的基础模型，如 BERT、GPT-3、MAE、DALLE-E和ChatGPT，其基于大规模的数据进行训练，同时广泛应用于各类下游任务
2. 本文全面回顾了最新的进展、挑战和机遇，首先回顾一些基础和现有的预训练方法，然后讨论了其他高级的 PFM 和不同数据模态下的 PFM，研究了一些关于 PFM 的模型效率、模型压缩、安全和隐私的问题，最后阐述了关键意义、未来方向、挑战等

## Introduction
1. Fundation Model：来自 2021 的一篇研究报告，On the Opportunities and Risk of Foundation Models，简单来说就是大模型，翻译为基础模型或者基石模型
2. PFM 主要研究在三个领域：NLP、CV、图学习（Graph Learning）：![[Pasted image 20230301200722.png]]主要思想就是基于大量数据学习通用的语义表征。
3. 多模态 PFM 正在融合，组成一个所谓的 unified PFMs，本文也回顾了其中的一些进展
4. PFM 的优点在于：第一，只要进行轻微的 fine tune 就可以提高下游任务的性能；第二，PFM 在生成质量方面还不错

## 基本组成

PFM 通用的概念性架构：![[Pasted image 20230301201855.png]]

### Transformer
Transformer 是一个纯注意力的模型，不依赖于递归或者卷积网络。

在 NLP 中，Transformer 可以用以解决 顺序输入的数据中长期依赖性问题，如 GPT3。

在 CV 中，ViT 将图像表示为成一系列的 patch ，类似于词嵌入。

在 GL 中，Graph Transformer Networks（GTN）在没有领域知识的情况下学习新的图结构和节点表征。


### PFM 中的学习机制
1. 监督学习：给定训练数据和标签，最小化目标函数来学习一个 function。
2. 半监督学习：除了有标签数据，还有无标签数据，通过数据的类间距和代理任务从数据中产生伪标签
3. 弱监督学习：处于监督学习和自监督学习（SSL）之间的，数据集中存在部分不正确或者不精确的标签
4. 自监督学习：利用数据本身中的信息来学习不同任务的表征，如 VAE 或者 GAN 是两种生成式 SSL 方法，对比学习是一种判别式的方法等
5. 强化学习：将学习过程建模为代理和环境之间的有序交互，agent 的目标是每个 状态 的长期回报的期望

### PFM 的预训练任务

### NLP 中的预训练
1. 掩码语言建模：mask 一部分单词然后训练模型来预测这些单词
2. 去噪自编码器：对数据集添加噪声，然后用带噪的数据来重构原始数据
3. 替换单词检测：用于判断语言模型是否替换了当前的 token
4. 下一句预测：输入两个句子判断是不是上下句
5. 顺序预测：将两个连续的句子作为正样本，然后交换其顺序作为负样本来建模句子之间的相关性

#### CV 中的预训练
1. 特定代理任务：创建代理任务，借助代理任务的标签来训练，如 inpainting 用于修复图像中的缺失部分
2. 帧顺序学习：从视频中学习帧之间的顺序
3. 数据生成：通过 GAN 将数据映射到隐空间
4. 数据重构：借鉴 NLP 中的，对掩码进行预测
5. 其他：如基于对比学习

#### GL 中的预训练
1. 图信息补全：mask 图中的部分信息，基于剩下的来恢复这部分的信息
2. 图属性预测：通过挖掘图中的潜在特征来作为自监督的标签
3. 图一致性分析：最大化具有相似语义信息的样本之间的一致性
4. 其他：将多个代理任务整个到一个统一的框架中等等

总之：SSL 很有前途，RL 是一种很新颖的 fine tune 方法。

## 用于 NLP 的 PFM
首先介绍学习词嵌入的模型，包括 autoregressive language model、contextual LM、permuted LM。总结了模型增强、多任务学习和不同下游任务的增强方法，最后介绍了  instruction-aligning 技术。

### 词嵌入方法

#### Autoregressive Language Model
基于前面的单词预测下一个单词，或者基于后面的单词预测上一个单词。对于词序列 $T=\left[w_1, w_2, \ldots, w_N\right]$，序列出现的概率为：$$p\left(w_1, w_2, \ldots, w_N\right)=\prod_{i=1}^N p\left(w_i \mid w_1, w_2, \ldots, w_{i-1}\right)$$
GPT 就采用了 自监督预训练+有监督 fine tune 的方法，使用 Transformer 作为解码器。

GPT2 增加了 Transformer 的层数，同时引入多任务学习。其模型容量很大， 可以针对不同的任务进行调整而非 fine tune。不过特定的下游任务仍需特定的数据集进行 fine tune。

GPT3 进一步增加模型参数和训练数据，无需针对下游 任务进行 fine tune就可以实现很好的性能。

#### Contextual Language Model
Autoregressive 仅仅使用了 单边的信息，而不能同时使用上下文的。如 ELMO 仅使用 双向 LSTM。contextual LM 基于上下文单词进行预测，使用 Transformer encoder，上下层都通过注意力机制直接相连。此时的概率计算为：$$p\left(w_1, w_2, \ldots, w_N\right)=\prod_{i=1}^N p\left(w_i \mid w_1, w_2, \ldots, w_N\right)$$
BERT 就使用了堆叠的多层、双向、Transformer 作为基本结构，模型输入包含三个部分：词嵌入、段嵌入和位置嵌入。采用双向的 Tramsformer 提取特征，但是本质仍然是自编码模型，对计算资源较低的设备不友好。

RoBERTa 使用更大的未标记数据，训练时间更长，添加长序列训练，同时采用 BPE 进行分词。

#### Permuted Language Model
自编码模型在 NLG 的任务中性能较差。Permuted LM 结合自回归 LM 和自编码 LM 的优点，pLM 的目标函数形式为：$$\max _\theta \mathbb{E}_{z \sim Z_N}\left[\sum_{t=1}^N \log p_\theta\left(x_{z_{T=t}} \mid x_{z_{T<t}}\right)\right]$$
其中，$\theta$ 是在所有的 permutation 中共享的参数，$Z_N$ 表示输入序列 $T$ 的所有可能的 permutation 的集合，$z_{T=t}, z_{T<t}$ 分别表示permutation $z$ 中的第 $t$ 个元素和 $[1,2,\dots,t-1]$ 之间的元素。

因为 BERT 在训练的时候使用 mask，但是在 fine tune 的时候不使用，导致两个时期的数据不一致。pLM 则可以同时实现双向编码同时避免 mask 的问题，其不再对数据按顺序建模，而是给出数据的所有排列，然后最大化数据的对数似然，从而任何位置都可以利用来自所有位置的上下信息。

常见的模型是 XLNET 和 MPNet。

### 模型架构设计方法
ELMO 采用多层 RNN 架构，每一层都是双向的 LSTM。正向和反向的最大似然作为训练的目标函数，相比于 word vector，ELMO 引入上下文信息 可以改善 多义问题，但是整体能力偏弱。

PFM 应用研究有两个方向：
+ 带 fine tune 的，如 BERT
+ 有 zero/few-shot prompts 的，如 GPT

BART 采用 编码器-解码器结构的 seq2seq 模型构建降噪自编码器：![[Pasted image 20230302100319.png]]
预训练的时候，对文本添加噪声，然后使用 seq2seq 模型重建原始文本。encoder 是双向的 Transformer，decoder 根据 encoder 输出的编码表征和没有被 mask 的序列来重构原始序列。

#### mask 的设计方法
注意机制首先将重点单词聚合为句子向量，将重要句子向量聚合为文本向量，这允许模型对不同的输入给予不同的关注。

基于 RoBERTa 的 SpanBERT 采用 dynamic masking 和 single segment pretraining 技术，还提出了跨度掩码和跨度边界目标（SBO）来屏蔽一定长度的单词：![[Pasted image 20230302102253.png]]
BERT 和 GPT 只能在没有联合训练的情况下分离训练编码器和解码器。Song等人提出 掩码 seq2seq 预训练模型 MASS，在训练阶段，编码器的输入序列被随机掩码为长度为 $k$ 的连续段，这些段将通过 MASS 的解码器恢复。
UniLM 通过为输入数据中的两个句子设计不同的掩码来完成 NLG 模型的学习。

#### 提升方法

1. 提升模型性能：大多数预训练模型都需要大量数据，这往往很难实现。百度发布的ERNIE Tiny是一个小型化的ERNIE，将预测速度提高了4.3倍；Lan等人提出ALBERT 也减少了内存消耗，提高了训练速度，且没用性能损失。
2. 提升多任务学习的性能：ERNIE 主要由两部分组成，Transformer编码器和任务嵌入。Transformer编码器使用自注意力来捕获每个 token 的上下文信息并生成上下文表征，任务嵌入将不同特征应用于任务，通过引入多任务学习来实现词汇、语法和语义的预训练。UniLM 使用三个预训练任务：单向LM、双向LM 和 编码器-解码器 LM。通过自注意层的掩码机制在预训练阶段同时完成三种目标任务。
3. 提升不同下游任务的性能：预训练模型匹配下游任务是非常重要的。BERT-WWM 直接使用在中文中使用 BERT 来进行 MLM 训练，但是会导致语义信息的丢失。ZEN 是一种基于BERT的文本编码器，它采用N-gram来增强性能，并以快速的收敛速度和良好的性能集成了大量文本信息。Tsai等人 提出了一种面向序列标记任务的多语言序列标记模型。
4. 示例：![[Pasted image 20230302103558.png]]ChatGPT基于PFM GPT-3.5使用RLHF进行 fine tune。与InstructGPT相比，ChatGPT使用不同的数据收集设置：
	1. 收集包含提示的大型数据集用于通过监督学习 fine tune GPT-3.5
	2. 给定 fine tune 后的模型和 prompt，模型生成多个输出。labeler 对这些数据进行打分和排序，构成新的数据集，用于训练 reward模型
	3. PPO 算法根据 reward 模型 优化 chatGPT

#### 指令对齐
目的是让LM遵循人类意图产生有意义的输出。一般通过监督的方式用高质量的语料库 fine tune 预训练的LM。也有使用 RL 进一步提高性能的。监督和RL方法都可以利用 chain-of-thought 推理来提高 AI 决策的透明度。

1. 有监督的 fine tune（SFT）：SFT 由输入输出和指令构成
2. 基于反馈的RL：RL 已经被用于增强NLP任务中的各种模型，其有助于优化语言生成任务中不可微目标函数，将这些问题看成是顺序决策问题，不过存在过拟合的风险。如 InstructGPT，但是比较难做
3. Chain-of-Thoughts (CoT)：CoT是一系列中间推理步骤，可以显著提高大型LMs执行复杂推理的能力




# ChatGPT is not all you need. A State of the Art Review of large Generative AI models
> 2023 年
1. 描述生成式 AI 如何部分的影响生成模型，同时对当前已发布的主要的生成模型做了分类
## Introduction

1. 专家系统基于知识库和推理引擎，且这种推理是通过 if else 实现的
2. 生成式 AI 则基于语料库或数据库生成判别器和生成器，判别器能够将输入映射到潜在的高维空间，生成器能够生成随机行为，实现无监督、半监督或有监督的生成行为。注意生成式 AI 和预测性机器学习如回归和分类不同。
3. 生成式模型的应用非常广泛。

## 生成式 AI 模型分类

根据输入和生成的形式对模型进行分类如下：![[Pasted image 20230218110937.png]]
一共有9类，而且包含的模型都是 21 年之后的，很新。

目前只有 6 家公司有能力做这种大型的生成式模型：google、openai、deepmind、meta ai、runway、nvidia。

### 文本生成图像

1. DALL-E2 ，来自 OpenAI，使用 CLIP 模型来预测给定图像下最相关的文本片段。为了生成图像，将一个先验模型和 CLIP 解码器组合，从而给定文本后，可以生成和 CLIP 得到的 embedding 最接近的图像。
2. IMAGEN：Google 提出，是一个由大型的 Transformer 语音模型组成的 文本生成图像 的 Diffusion 模型。Imagen 证明 增加语言模型的大小比增加图像 Diffusion 模型的大小更能提高样本保真度和图像文本对齐。
3. Stable Diffusion：开源 Diffusion 模型，使用 latent diffusion 模型，可以在 latent 空间中进行修改从而修改图像，主要包含两个部分：Text encoder 和 Image generator。
4. MUSE：基于 Transformer，比自回归或者 Diffusion 的生成效率更高，因为是在离散空间中基于掩码建模任务训练的。最大的优点就是速度快。

### 文本生成 3D 图像

1. DreamFusion：由 Google 开发，使用预训练的 2D 文本生成图像的Diffusion 模型来进行文本到 3D 图像的合成。Dreamfusion 用来自2D Diffusion 模型蒸馏的损失取代了先前的CLIP技术
2. Magic3D：来自 NVIDIA。Dreamfusion 模型存在两个问题：时间长、质量低。Magic3D 使用两阶段优化框架解决了这些问题。首先构建一个低分辨率的 diffusion prior，然后使用稀疏的3D哈希网格结构加速。然后进一步通过渲染优化 3D网格模型。

### 图像生成文本

1. Flamingo：DeepMind 创建的视觉语言模型，采用 few shot learning，只需要一些简单的输入提示（如对图像中的内容进行提问）就可以用在很多视觉和语言任务中。具体而言，Flamingo的输入包含具有视觉条件的自回归文本生成模型，能够提取与图像或视频交织的文本标记序列，并生成文本作为输出
2. VisualGPT：来自 OpenAI，利用了来自预训练语言模型GPT2的知识。为了弥合不同模态之间的语义差距，设计了一种新的编码器-解码器注意机制。模型的最大优点是它不需要像其他图像到文本模型那样多的数据。可以从 github访问 api。

### 文本生成视频

1. Phenaki：来自 Google，给定提示文本来合成视频。Phenak i是第一个可以从开放域下的可变时间提示生成视频的模型。为了解决数据问题，它在大型 图像-文本 对数据集以及少量 视频-文本 样本中进行联合训练。模型由三部分组成：C-ViViT encoder、transformer和 video generator。生成的视频可以长达几分钟，而模型则在1.4秒的视频上训练。可以从 github访问 api。
2. Soundify：Soundify是由Runway开发的一个将声音效果与视频相匹配的系统。

### 文本生成音频

1. AudioLM：来自 Google，AudioLM将输入音频映射到离散 的 token 序列中，并将音频生成看成是表征空间中的语言建模任务。通过对大量原始音频的进行训练，AudioLM学习在较短的 prompt 下生成自然连贯的音频。可以从 github访问 api。
2. Jukebox：OpenAI 开发，能够生成原始波形域下的音乐。通过分层VQ-VAE架构来解决 原始音频建模时的长距离依赖关系，将音频压缩到离散空间，设计损失函数来保留最多的信息量。仅限于英文歌曲。训练数据集来自LyricWiki的120万首歌曲。VQ-VAE有50亿个参数，在9秒音频剪辑上训练3天。可以从 github访问 api。
3. Whisper：OpenAI开发，音频生成文本，可以实现多语言语音识别、翻译和语言识别。基于680000小时的标记音频数据进行训练，encoder-deccoder transformer 结构。

### 文本生成文本

1. ChatGPT：来自 OpenAI，对话交互模型，基于 Transformer，通过强化学习进行训练。
2. LaMDA：Google，对话训练，基于 Transformer，137B个参数，并根据1.56T个单词的公共对话数据和web文本进行预训练。单个模型可以执行多个任务，可以生成好几个回答，然后根据一些安全规则进行过滤
3. PEER：Meta AI，协作式语言模型，用于帮助写作。基于四个步骤：Plan, Edit, Explain and Repeat，重复这四个步骤直到不再需要修改为止。主要采用 Wikipedia edit histories 数据进行训练。训练模型填充缺失的数据，然后根据这些合成数据再去训练其他模型。这样做的缺点是噪声很大，而且缺乏引用，于是通过一个并不总是有效的检索系统来弥补。整个框架是一个迭代过程。
4. Meta AI Speech from Brain：Meta AI开发的模型，用于帮助无法通过语音、打字或手势进行交流的人。模型试图从非侵入性大脑记录中直接解码语言。采用 wave2vec 2.0 自监督模型来识别听有声读物时志愿者大脑中复杂的言语表达，采用脑电图和脑磁图测量神经元活动

### 文本生成代码

1. OpenAI创建的AI系统，是一个通用编程模型。一般来说，编程可以分成两个部分：将问题分解为更简单的问题；将问题映射到已有的代码。第二部分是程序员最需要时间的部分，也是Codex最擅长的地方。模型从GPT-3进行了 fine tune，在openai网站上有 api。
2. Alphacode：是一个为需要更深入推理的问题生成代码的系统。模型受益于三个部分：广泛的数据集、基于 Transformer 的大型高效架构 和 大规模模型采样。通过 GitHub 库进行预训练，总计715.1GB代码（比 codex 多），架构方面，使用 encoder-decoder transformer 架构。

### 文本生成其他科研内容（如化学公式、蛋白质结构，解数学题等）

1. Galactica：由Meta AI和Papers with Code开发的自组织科学的模型
2. Minerva：使用逐步推理解决数学和科学问题的语言模型。

### 其他

1. Alphatensor：张量计算
2. GATO：机器人控制

## 总结和未来的工作

生成式大模型的构建限制有：
+ 大量的参数和数据集
+ 大量的计算时间和算力需求
+ 数据偏差
+ 道德约束






# GPT-1
> 2018 年
1. 作者证明，通过对未标记的文本语料库进行语言模型的生成式预训练，然后对特定任务进行 fine tune ，可以在这些任务中实现巨大的收益

## 框架
> 包括两个阶段：无监督预训练和 fine tune

### 无监督预训练

给定数据集 $\mathcal{U}=\left\{u_1, \ldots, u_n\right\}$，使用标准的语言模型最大化似然：$$L_1(\mathcal{U})=\sum_i \log P\left(u_i \mid u_{i-k}, \ldots, u_{i-1} ; \Theta\right)$$
其中，$k$ 为上下文窗口的大小，条件概率 $P$ 采用神经网络来建模，其参数为 $\Theta$ ，采用 SGD 进行训练。

实际采用的是 多层的 Transformer 的 decoder 作为语言模型，计算如下：$$\begin{aligned}
h_0 & =U W_e+W_p \\
h_l & =\text { transformer\_block }\left(h_{l-1}\right) \forall i \in[1, n] \\
P(u) & =\operatorname{softmax}\left(h_n W_e^T\right)
\end{aligned}$$
其中，$U=\left(u_{-k}, \ldots, u_{-1}\right)$ 为 token 的上下文向量，$n$ 为层数，$W_e$ 为 token 的 embedding matrix，$W_p$ 为位置的 embedding matrix。

### 有监督的 fine tune

完成模型预训练后，设有标记的数据集 $\mathcal{C}$ ，每个样本 都包含输入 token 序列 $x^1,\dots,x^m$ 和标签 $y$，输入通过预训练模型得到最后一层的 Transformer block 的激活 $h^m_l$ ，然后送到一个额外的线性层（参数为 $W_y$）进行预测：$$P\left(y \mid x^1, \ldots, x^m\right)=\operatorname{softmax}\left(h_l^m W_y\right)$$
然后最大化以下目标函数：$$L_2(\mathcal{C})=\sum_{(x, y)} \log P\left(y \mid x^1, \ldots, x^m\right)$$
同时为了改进泛化能力和收敛速度，将语言建模作为额外的目标函数，总体的优化目标变成：$$L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda * L_1(\mathcal{C})$$

### 特定任务时的输入转换

对于某些任务，如文本分类，可以直接使用前面的模型进行 fine tune。
但是对于其他的任务，需要结构化的输入。使用 遍历式的方法，将结构化的输入转换为预训练的模型可以处理的有序序列，如图：![[Pasted image 20230302135245.png]]


# BERT
> 2019 年

1. 提出 BERT 模型，即 Bidirectional Encoder Representations from Transformers，通过对模型层中的所有的上下文进行建模，来从未标记的数据中预训练深度表征
2. 预训练的 BERT 可以通过额外的 输出层 进行 fine tune，从而创建各种任务下的 SOTA 模型，而无需针对每个任务设计特定的架构

## Introduction
1. 有两种方法用于将预训练的 LM 用于下游任务：
	1. 基于特征的，如 ELMO 使用包含预训练表征的特定架构作为额外的特征
	2. fine tune：如 GPT，通过 fine tune 所有的预训练参数来针对下游任务进行训练
2. 作者任认为，这些技术限制了预训练表征的能力，因为他们的 LM 是单向的，每个 token 只能看到前面的 token。对于句子级别的任务来说，这种限制导致最后的结果并不是最优的，且基于 fine tune 的方法如果用于 QA 这类任务时反而有害，因为必须结合上下文。
3. 于是提出 BERT，使用双向的 Transformer encoder 来改善基于 fine tune 的 方法，使用 MLM 作为预训练目标，从输入中随机 mask 一些 token，目标是通过上下文预测被 mask 的 token，MLM 可以融合上下文，从而可以使用双向 Transformer
4. 此外还有一个 ”下一句预测“ 任务，联合预训练的文本对表征
5. 本文的贡献有：
	1. 证明了语言表征提取中双向预训练的重要性
	2. 预训练表征减少了大量工程化的特定任务架构的需求
	3. 在 11 个 NLP 任务中实现了 SOTA

## BERT
分为两个步骤，预训练和 fine tune，预训练期间，模型在不同的预训练任务中基于未标记的数据进行训练，fine tune 时，使用来自下游任务的标记数据对所有参数进行调整，且每个下游任务都有单独的 fine tune 模型。

整个过程为：![[Pasted image 20230302222844.png]]

在不同的任务上架构都是统一的，在预训练和 fine tune 的架构上只有很小的区别。

### 架构

多层双向 Transformer encoder，层数为 $L$，hidden size 为 $H$，self-attention head 数为 $A$，实现了两个模型：
+ base：L=12, H=768, A=12, 参数量 110M，和GPT 的参数差不多
+ large：L=24, H=1024,A=16, 参数量 340M

### 输入输出表征

输入要可以表示一个句子或者一对句子，其中 ”句子“ 是指任意长度的连续文本（不是实际的语言句子），序列是 BERT 的输入，可能是一个句子或者两个句子的组合。

使用 vocabulary 大小为 30000 的 WordPiece embeddings，每个序列的第一个 token 是 CLS，并且这个 token 对应的输出用作分类。对于句子对，使用 SEP 来分隔，同时在每个 token 中添加一个可以学习的 embedding，表明其属于句子 A 还是 B（如上图中的左边）。

将输入 embedding 记为 $E$，CLS 对于的输出的那个 hidden vector 记为 $C\in \mathbb{R}^H$，第 $i$ 个输入 token 对于的输出 hidden vector 为 $T_{i}\in \mathbb{R}^H$。

对于给定的 token，其对应的输入表征为 token embedding+segment embedding+position embedding：![[Pasted image 20230302224734.png]]

### 预训练 BERT
使用两个无监督任务训练 BERT。

#### MLM（Mask LM）
传统的 LM 只能以回归的方式（要么是根据前面预测下一个，要么是根据后面预测上一个），因为如果使用双向条件会使得每个单词间接地看到自己，在多层的上下文中，模型其实很容易预测目标。

为了成功训练双向的模型，作者选择随机 mask 一定百分比的输入 token，称这种方法为 MLM，然后 masked token 对应的最终的 hidden vector 经过 softmax 得到预测 token。

和 DAE 不同，这里只会预测 masked token，而不会预测全部的输入。

但是这会造成预训练过程和 fine tune 过程不匹配，因为 MASK 这个 token 在 fine tune 期间不匹配，为了缓解这种情况，实际不总是选择 MASK 这个 token，而是首先确定要被 mask 的 15% 的 token 的位置，然后按照比例进行，如果第 $i$  个 token 被选中，则：
+ 以 80% 的概率把它替换成 MASK
+ 以 10% 的概率替换称一个随机的 token（不是 MASK ）
+ 以 10% 的概率保持不变

#### NSP（Next Sentence Prediction）
类似于 问答 或者 自然语言推理 这类的任务都需要理解两个句子之间的关系，但是一般的 LM 模型是无法建模这些的。

于是预训练一个二分的 NSP 任务，数据可以从任何单语种数据库中生成。具体来说，在选择句子的时候，一半的概率选择两个连续的句子 A-B，一半的概率选择两个随机的句子 A、B。

图中的 $C$ （ CLS 对应的输出的 token）就是用于 NSP 任务的，虽然看起来很简单，但是这样做确实可以提高在 QA 和 NLI 任务中的性能。

#### 数据
训练数据：BooksCorpus，800M 单词 ；English Wikipedia，2,500M 单词

### fine tune
对于每个特定的任务，将特定的输入输出放进 BERT 中然后端到端的 fine tune 所有的参数就行。

在输出短， token 表征送到输出层用于 token level 的任务，CLS 对应的输出 $C$ 送到另一个输出层用于 分类相关的任务。





# GPT-2
> 2019 年

1. 提出 GTP-2，1.5B 参数的 Transformer 模型
2. 做了一个新的数据集：WebText
3. 主要的优势是 zero shot

## Introduction
1. 目前的AI系统只是在某个特定领域的专家，而非所谓的通用性AI，作者希望转向更为通用的系统，可以实现多个任务而无需为每个子任务创建特定的标注数据集
2. 怀疑是在单个领域的数据集进行训练导致的模型泛化性差
3. 多任务学习可以改善性能并且有很大的前景，但是在 NLP 用的不多
4. 本文将多任务和自监督+fine tune的模式结合起来，探索更为通用的迁移方法，可以在 zero shot 的情况下实现下游任务，且不需要进行任何架构或者参数方面的修改（也就是不需要下游任务数据集，不进行 fine tune），最终得到了 “还可以” 的结果

## 方法

核心是语言模型，从一系列的样本 $\left(x_1, x_2, \ldots, x_n\right)$ 中进行无监督分布估计，每个样本都包含可变长度的符号 $\left(s_1, s_2, \ldots, s_n\right)$，则：$$p(x)=\prod_{i=1}^n p\left(s_n \mid s_1, \ldots, s_{n-1}\right)$$
系统的输出不应该仅仅取决于输入，还应该和任何相关，即我们需要建模 $p(\text { output } \mid \text { input,task) }$，其中的任务条件，通常可以在架构级实现或者在算法级实现，但是，语言文字本身就可以当作这样一种条件，例如：
+ 对于翻译任务，可以将序列写成：translate to french, english text, french text
+ 对于 阅读理解任务，可以写成：answer the question, document, question, answer

作者推测，一个足够强大的语言模型可以学习去 推理和执行 语言文本序列中的任务，以便更好地进行预测，而无论其采用的方法。做到这一点就可以实现无监督的多任务学习。作者通过分析语言模型在各种任务的 zero shot 下的性能来进行测试。

### 训练数据集

GPT-2 使用了尽可能大且多样的数据集，从而可以用在尽可能多的领域和任务中。

采用网络爬虫，只爬取人类策划和过滤的网页来提高数据质量，最终得到 WebText 数据集， 经过清理之后得到超过 800万 个文档，总计 40G 的数据。

### 输入表征

1. 作者发现，标准的 byte level 的 LM 性能不如 word level LM
2. 于是采用 BPE 作为输入来减少 vocabulary 的大小，但是由于 BPE 使用基于贪婪频率的启发式方法来构建 vocabulary，直接将 BPE 应用于字节序列会导致次优合并，于是禁止 BPE在任何字节序列的字符类别之间合并
3. 从而可以将 word level 和 byte level 的优点相结合，且可以为任意 Unicode 编码的字符串分配概率而不用管前面的预处理、tokenization等

### 模型

基于 Transformer架构，和 OpenAI 的 GPT 差不多，修改在于 Layer normalization 被放到每个 sub-block 的输入之前，且在最后的注意力层之后添加了一个额外的 Layer normalization 层。

初始化的时候采用了修正的初始化方法，将上下文的 token 数从 512 增加到 1024，batch size 增加到 512。


![[Pasted image 20230302162510.png]]
参数最小的相当于原始的 GPT，参数最大的那个称为 GPT-2。




#  GPT-3
> 2020 年

1. 在 NLP 中，如果要在特定任务下实现较好的性能，需要对特定于任务的数据集进行微调
2. 而人类只需要几个例子或者简单的指令就可以完成一项新的任务
3. 本文表明，拓展语言模型可以大大提高未知任务、zero shot 下的性能
4. 提出 GPT-3， 175B 的参数量，并且在 few shot 的情况下测试性能
5. 在所有的任务中都不进行 fine tune，而仅仅通过文本交互实现指定的任务

## Introduction

1. 基于 Transformer 的 LM 在很多 NLP 任务中都取得了实质性的进展，但是其中的问题就是，需要任务特定的数据集和 fine tune
2. 很有必要提出方法来解决这种限制，因为为每个任务都提供一个有标签的数据集来 fine tune 限制了 LM 的通用性，且有些任务甚至无法收集对应的数据集；并且有研究发现，在预训练+ fine tune 这种范式下，模型越大，对分布的拟合不一定会越强
3. 人类学习语言相关的任务不需要很多任务相关的数据集，而仅仅需要一个引导性的指令或者演示几次就可以
4. 解决这些问题的一个潜在方法是元学习（自己定义的。。。），GPT-2 使用 in-context learning（语境学习）来实现，用文本输入来表示特定的任务 ，但是实际上其效果不如 fine tune
6. 语言模型另一个趋势是，加大模型参数，且随着规模的增加，in-context 学习能力也在增加
7. 本文提出训练 GPT-3，一共 175B 个参数，在 20 多个数据集上进行评估，测试三种条件：few shot、one shot 和 zero shot，在其中一个任务中的结果如图：![[Pasted image 20230302192953.png]]
8. 最终结果是，基本上在 zero shot 和 one shot 上都可以实现不错的性能，在 few shot 中甚至偶尔可以超过 经过 fine tune 的 SOTA
9. 但是对于自然语言推理这类的问题，GPT-3 的效果也不怎么样

## 方法
![[Pasted image 20230302195154.png]]

预训练方法、模型架构等的使用和 GPT-2 类似，但是探索了不同的 in context learning 的方法，区别如上图。

### fine tune
在特定任务的数据集上进行有监督的训练来更新模型的权重。

优点在于，fine tune 之后效果会很好，缺点在于对于每个任务可能都需要一个大的 fine tune数据集。

GPT-3 没有进行 fine tune，但是原则上可以进行。

### few shot
模型在推理的时候，给一些和任务相关的演示作为条件，但是模型不能更新权重。通常会给出 $k$ 个示例，设置 $k$ 的范围为10 到 100。

few shot 的优点是大大减少了特定任务下的数据需求，缺点是相比于最好的 fine tune 的 SOTA 性能还是要差很多。

### one shot
few shot 中 $k=1$ 的情况，有点类似于人类的学习方式（所谓举一反三）。

### zero shot
没有任何的演示输入，输入只有描述任务的指令。是所有方法中最难的，但是也是最方便的。

### 模型和架构
![[Pasted image 20230302201411.png]]
使用和 GPT-2 完全相同的架构，除了在 Transformer 层中使用了 alternating dense and locally banded sparse attention。

一共训练了 8 个模型，最大的称为 GPT-3。

### 数据集
采用近万亿单词的 Common Crawl 数据集，同时采用三个步骤来提高数据的质量。在多个数据集的混合中进行采样，最终过滤后的数据大小一共 570GB：![[Pasted image 20230302214118.png]]



# LaMDA
> 2022 年 1 月

1. 提出 LaMDA，是一个基于 Transformer 的、专门用于对话的语言模型
2. 有 137B 参数，采用 1.56T word 进行预训练
3. 在安全性方面，使用一组基于人类价值观的度量来量化安全性，通过少量的带有注释的数据进行 fine tune ，对 LaMDA 的输出进行过滤可以提高安全性
4. 通过使模型可以参考外部的知识源，如检索系统、翻译系统、计算器等，可以提高回答的真实性，同时也使用了 groundedness metric 来进行量化

## LaMDA 预训练

LaMDA 预训练用于预测文本数据集中的下一个 token，和先前的对话模型仅对对话数据进行训练不同，LaMDA 还从 公共的 web 文档进行预训练，所以还可以作为一个通用的语言模型（ fine tune 之前）。

预训练的数据集包含 2.97B 文档、1.12B 对话 和 13.39B 个对话的句子，总计 1.56T 个单词。采用 BPE 作为 token，词表大小为 32K。

最大的模型有 137B 个参数，使用 Transformer 的 decoder 作为基本架构，64 层，dmodel = 8192, dff = 65536, h = 128, dk = dv = 128，不同模型结构如下：![[Pasted image 20230305152451.png]]

最大的模型在 1024 个 TPU-v3 芯片训练 57.7 天，每个 batch 包含 256K token。

![[Pasted image 20230305152544.png]]
预训练的时候，当成是一个标准的语言模型。

## 指标
> 描述评估生成模型的一些指标。

### 基本指标：质量（SSI）、安全性 和 真实性
质量包括：敏感性、特异度和趣味性。

敏感度，衡量一个模型的反应在上下文中是否有意义，并且与之前所说的任何内容都不矛盾。但是不能仅靠 敏感性来评估，因为模型会倾向于，用“我不知道”回答每个问题，用“好”回答每个语句。

特异性，用来衡量一个反应是否特定于给定的环境。例如，如果用户说“我喜欢Eurovision”，而模型回答“我也是”，那么它的特异性得分为0，因为这种回答可以用于许多不同的环境。如果它回答“我也是。我喜欢欧洲电视台的歌曲”，那么这个回答得分1。

趣味性被通过 0/1 标签来衡量，如果测试人员他们判断回答可能 “吸引某人的注意力”或“引起他们的好奇心”，或者如果它是意外的、机智的或有洞察力的，那么这个回答会得分1。

安全性：设计了一个新的安全度量来衡量不安全的模型输出，遵循 Google’s AI Principles。

真实性：目标是确保LaMDA在任何可能的情况下都能产生与有已知来源的回答，避免一本正经的胡说八道。将真实度定义为，有权威的信息源支持的回答所占的百分比。定义信息度为，携带已知来源信息的回答占所有回答的百分比。定义引用准确度为，引用其来源URL的回答在所有对外部世界有明确声明的回答中所占的百分比。

### 角色扮演指标：帮助性和一致性
测试对话时，让模型扮演特定的角色，
帮助性：：如果模型的回答包含基于用户使用信息检索系统进行的独立研究的正确信息，并且用户认为它们有用，则该模型的响应被标记为有用。

角色一致性：如果模型的回答看起来像目标角色所说的话，那么这个回答就被标记为满足角色一致性。

## LaMDA fine tune

### 对 SSI 和 安全性 的 生成式和判别式 fine tune
生成式 fine tune 是指用 预训练的模型，根据给定上下文生成特定文本。
判别式 fine tune 是指用 预训练的模型，评估生成的文本的质量和安全性。

生成式的 fine tune 表示为：context-sentinel-response，损失仅在 response 部分计算。

判别式的 fine tune 表示为：context-sentinel-response-attribute-name- rating，损失仅在 rating 部分计算，例子：
![[Pasted image 20230305160557.png]]
具体操作上，首先 fine tune LaMDA 来生成候选的回答，然后过滤出满足安全性的回答，对得到的回答根据 SSI 指标进行排序。将排名最好的回答作为下一个回答的输入。

### 通过 fine tune 来学习调用外部信息检索系统
LaMDA等语言模型倾向于生成看似合理的输出，但其实与事实相矛盾（比如捏造虚假的新闻）。

本文提出，通过学习 参考外部知识源 和 使用工具 来进行 fine tune。

#### toolset
创建了一个包含信息检索系统、计算器和翻译系统的工具集（toolset）。数据集的数据表示一个问题，标签表示这个问题的答案。比如说
+ 如果输入 135+7721，那么其对于的标签为 7856
+ 如果输入 hello in French，那么其对于的标签为 Bonjour
+ 如果输入 How old is Rafael Nadal?，标签为 Rafael Nadal / Age / 35
同时这个 set 还可以获得网络中的开放内容，然后解析成一个训练的样本。

#### 对话收集
收集真实的人类对话数据，且主要关注交互式的信息获取。
fine tune 的过程分两步，第一步进行 多轮对话，获得 base model 的输出，然后 base model 生成一个特殊的 TS 字符，表明接下来的文本是 查询 用途，且需要被送入 toolset。
第二步接受 tool 返回的输出和对话的内容，用于预测最终的真实的输出。这个任务也可以输出一个额外的 research query，在推理阶段，模型的输出要么送到信息检索系统，要么送给用户。
![[Pasted image 20230305163228.png]]


# LLaMA
> 2023 年
1. 提出 LLaMA，是一种基础语言模型，参数量从 7B 到 65B，并且可以使用公开可用的数据集进行训练
2. 13B 参数的模型在大多数的测试中性能优于 175B 的 GPT-3，和 65B 的模型和 Chinchilla-70B、PaLM-540B 可以相提并论

## Introduction

1. 现有的很多模型都认为，参数越多，性能越好
2. 但是有研究表明，性能最好的模型不是参数最多的那个，而是由更大的数据集下训练的较小模型
3. 作者发现，在 1T 的token 上训练 7B 的模型，性能都可以持续提升
4. 这项工作重点是训练语言模型 LLaMA，使用更多的 token 但是能够在合理的推理速度下实现最好的性能，尽管 LLaMA-13B 比 GPT-3 小了十倍，但是在很多测试中性能优于 GPT-3，并且可以在单个 GPU 上运行
5. 而对于 LLaMA-65B 的模型，其性能可以和  Chinchilla-70B、PaLM-540B 等大模型相提并论
6. 还有一个优点就是，训练的时候只使用公开可用的数据。

## 方法
总结起来就一句话：使用标准的优化器，在大量数据集中训练大型 Transformer 模型。

### 预训练数据
所用的数据集是各种领域的混合公开数据集：![[Pasted image 20230303132151.png]]
采用 BPE 算法对数据进行 tokenize。
总体数据包含 1.4T 个 token，且除了 Wikipedia 和 Books，每个 token 只使用一次。

### 架构
主要是基于 Transformer，但是也有一些不同：
1. pre-normalization：在 transformer 的每个 sub layer 之前进行 normalize 而不是之后，同时采用 RMS normalization 方法
2. SwiGLU 激活函数：把 ReLU 几激活函数替换成 SwiGLU 来提高性能
3. Rotary Embeddings：移除位置编码，而是使用 rotary positional embeddings
不同大小模型的超参数的对比：![[Pasted image 20230303134010.png]]

### 优化
使用 AdamW 优化器，采用余弦 lr scheduler，weight decay 为 0.1，梯度裁剪为 1，warmup 数为 2000。

### 高效实现
一些优化训练速度的方法：
+ 使用 causal multi-head attention 减少内存使用和运行时间，不存储注意力权重，且不计算 mask 部分的 score 
+ 保存计算过程中的激活值，通过手动实现 backprop 的过程而不是使用 pytorch 的自动求导 来实现的

训练 65B 的模型时，在 2048 台 80G RAM 的A100 GPU 上训练 21天，总计1.4T 个 token，平均每台 GPU 每秒 380 个 token。

## 结果
对标 GPT-3，在 zero shot 和 few shot 情况下测试了 20 个任务：
+ zero shot：提供 任务的文本描述
+ few shot：提供1-64个任务样例

### Common Sense Reasoning（常识推理） 任务
![[Pasted image 20230303140803.png]]

### closed book QA 任务
![[Pasted image 20230303141048.png]]

### 阅读理解
![[Pasted image 20230303141151.png]]
比 GPT-3 高出很多。

### 数学推理

### 代码生成

### 性能评估
![[Pasted image 20230303144849.png]]
大多数的实验中，性能都可以稳步提高。


### 大规模多任务语言理解（MMLU）
MMLU 包含各领域知识组成的多项选择题，包含人文、社科、科技、工程和教育等多个领域。在 five-shot 条件下进行模型评估，结果如下：![[Pasted image 20230303143613.png]]
LLaMA-65B 在大多数领域平均结果 落后于 Chinchilla-70B 和 PaLM-540B。

一种可能的解释是，训练的数据中使用的书籍和学术论文数量有限。

## 指令 fine tune

![[Pasted image 20230303143951.png]]对指令的 简单 fine tune 可以快速导致 MMLU 的性能提升