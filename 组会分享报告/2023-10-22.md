# InstructTTS- Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt
> 港中文、腾讯 AI lab

1. 采用自然语言作为 style prompt 来控制合成语音中的风格
2. 首先构造了一个 speech corpus，每个语音样本不仅包含内容文本同时也包含风格描述
3. 提出 InstructTTS：
	1. 采用自监督学习和跨模态学习，提出一种新的  three-stage 的训练方法来得到鲁棒的 sentence embedding（可以从 style prompt 中捕获语义信息，从而控制说话风格）
	2. 提出在 discrete latent space 中建模声学特征，训练 discrete diffusion probabilistic model 来生成量化的声学 token 而非 mel 谱
	3. 在声学模型训练过程中采用 mutual information (MI) estimation and minimization 来最小化 style-speaker 和 style-content 之间的互信息

## Introduction

1. 现有两种方法学习 speaking style information：
	1. 一种采用 auxiliary categorical style label 作为条件
	2. 另一种模仿参考语音的 style
2. 本文贡献：
	1. 首次采用 natural language prompts 来建模 expressive TTS
	2. 引入 three stage 训练策略来得到 sentence embedding model
	3. 提出在 discrete latent space 中建模声学特征，把语音合成看成是一个 squence-to-sequence language modeling 任务
	4. 探索了两种 VQ acoustic features：mel-spectrogram based VQ features and waveform-based VQ features，两种特征都有效
	5. 在训练声学模型时采用 mutual information (MI) estimation and minimization

## 相关工作和背景（略）

## 数据集

采用 NLSpeech（内部、普通话中文语音数据集），44 小时的数据。每句话都有 5 style prompts（由不同的人标注）。

其他细节见论文。

## 方法

整体框架：
![](image/Pasted%20image%2020231014171548.png)
包含五个部分：
+ content encoder
+ style encoder
+ speaker embedding module
+ style-adaptive layer normalization (SALN) adaptor
+ discrete diffusion decoder

### content encoder

用于从 content prompts 提取 content representation，用的是 FastSpeech 2 中的 feed-forward transformer 结构，然后也用了 variance adaptor 来预测 duration 信息。

### style prompt embedding model

采用 RoBERTa 作为 prompt embedding 模型。设风格 prompt 序列为 $S=[S_1,S_2,...,S_M]$，然后在开头添加一个 CLS token，然后送入到 prompt embedding 模型中，然后把 CLS 对应的表征作为风格表征。同时为了实现：
+ style prompt space 包含足够的语义信息
+ style prompt space 的分布相对均匀且光滑

提出一个 three-stage training-fine-tuning strategy：
+ 训练一个基本的中文语言模型：基于中文数据训练  RoBERTa
+ 在有标记的数据下 fine tune 语言模型，采用 SimCSE 中的方法，用 InfoNCE 作为损失
+ 在 style prompt 和 语音 之间跨模态表征学习：希望来自 style prompt sentence 中的 embedding 和 来自语音中的 style representation 可以被映射到共享的语义空间中，基于 metric learning 提出 跨模态表征学习：

![](image/Pasted%20image%2020231015103233.png)
对于任何的 style prompt，随机选择 $N-1$ 个 negative audio samples 然后和一个 正样本组合得到一个 batch，而对于任何的 audio sample 也执行同样的操作，采用 contrastive ranking loss 和 InfoNCE loss 作为训练的目标函数。

### style encoder

包含三个部分：
+ 预训练的 style prompt embedding（图中的 prompt encoder）
+ adaptor layer 将 style embedding 映射到一个新的 latent space 中
+ audio encoder 从参考 mel 谱 中提取风格信息
> 当训练其他模型的时候，前面预训练的  prompt embedding model 参数是冻住的。

训练的时候，其中一个策略是最小化 style prompt embedding 和 audio embedding 的距离。但是为了确保 audio encoder 只编码风格相关的信息，同时还最小化了 风格-内容 之间的互信息。
> 互信息用于表明两个随机变量之间的相关性。

### 在离散 Latent Space 中建模 mel 谱

大部分 TTS 都是在连续空间中从文本映射到 mel 谱然后用 vocoder 生成波形。但是 mel 谱 中的 frequency bins 之间是在时间上高度相关的。

本文提出在离散的 latent space 中建模 mel 谱，首先用大规模的语音预训练一个 VQ-VAE，然后把量化后的  latent codes 作为 target，如图：
![](image/Pasted%20image%2020231015105930.png)
Mel-VQ-VAE 包含三个部分：
+ Mel-encoder $E_{mel}$ 将
+ Mel-decoder $G_{mel}$
+ codebook $\boldsymbol{Z}=\{\boldsymbol{z}_k\}_{k=1}^K\in\mathbb{R}^{K\times n_z}$，大小为 $K$，维度为 $n_z$。

假设输入 mel 谱 为 $\boldsymbol{s}\in\mathbb{R}^{F_{\boldsymbol{b}in}\times T_{\boldsymbol{b}in}}$，首先编码到 latent representation $\hat{\boldsymbol{z}}=E_{mel}(\boldsymbol{s})\in\mathbb{R}^{F_{bin}^{\prime}\times T_{bin}^{\prime}\times n_z}$，然后使用量化器将 $\hat{\boldsymbol{z}}_{i,j}$ 映射到最邻近的 codebook entry ${\boldsymbol{z}}_k$ 来得到量化后的表征：
$$\boldsymbol{z}_q=Q(\boldsymbol{\hat{z}}):=\left(\underset{\boldsymbol{z}_k\in\boldsymbol{Z}}{\operatorname*{\arg\min}}||\boldsymbol{\hat{z}}_{ij}-\boldsymbol{z}_k||_2^2\right)$$
然后 decoder 基于此表征重构 mel 谱，$\hat{\boldsymbol{s}}=G_{mel}(\boldsymbol{z}_q)$，同时采用 VQGAN 的结构引入对抗损失提高重构性能。

Mel-VQ-Diffusion：给定训练数据 $(\boldsymbol{x}_0,\boldsymbol{y})$，这里的 $\boldsymbol{y}$ 是 phone features, style features and speaker features 各种条件的组合，$\boldsymbol{x}_0$ 是 离散 的 GT-mel 谱 token。采用 mask and uniform transition matrix 来构造 diffusion 过程，具体来说，转移矩阵 $\boldsymbol{Q}_t\in\mathbb{R}^{(K+1)\times(K+1)}$ 定义为：
$$\boldsymbol{Q}_t=\begin{bmatrix}\alpha_t+\beta_t&\beta_t&\beta_t&\beta_t&\cdots&0\\\beta_t&\alpha_t+\beta_t&\beta_t&\beta_t&\cdots&0\\\beta_t&\beta_t&\alpha_t+\beta_t&\beta_t&\cdots&0\\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\\gamma_t&\gamma_t&\gamma_t&\gamma_t&\cdots&1\end{bmatrix}$$
即 有 $\gamma_t$ 的概率转移到 mask token，有 $K\beta_t$ 的概率从 $K$ 个类别中均匀采样，有 $\alpha_t$ 的概率保持不变，根据下式可以计算分布 $q(x_t|x_0)$ 。
$$\overline{\boldsymbol{Q}}_t\boldsymbol{c}(x_0)=\overline{\alpha}_t\boldsymbol{c}(x_0)+(\overline{\gamma}_t-\overline{\beta}_t)\boldsymbol{c}(K+1)+\overline{\beta}_t$$
最终的 stationary distribution 为：
$$p(\boldsymbol{x}_T)=[\overline{\beta}_T,\overline{\beta}_T,\cdots,\overline{\gamma}_T]$$
其中，$\overline{\alpha}_T=\prod_{t=1}^T\alpha_t,\overline{\gamma}_T=1-\prod_{t=1}^T(1-\gamma_t),\overline{\beta}_T=\\(1-\overline{\alpha}_T-\overline{\gamma}_T)/K$ ，$\boldsymbol{c}(\cdot)$ 表示将标量转为 one-hot 向量。

然后训练神经网络 $p_\theta(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\boldsymbol{y})$ 来估计后验分布 $q(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\boldsymbol{x}_0)$，损失为：
$$\begin{aligned}\mathcal{L}_{diff}&=\sum_{t=1}^{T-1}\left[D_{KL}[q(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\boldsymbol{x}_0)||p_\theta(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\boldsymbol{y})]\right]\\&+D_{KL}(q(\boldsymbol{x}_T|\boldsymbol{x}_0)||p(\boldsymbol{x}_T))\end{aligned}$$
训练时采用 classifier free guidance，优化下述目标函数：
$$\log(p(\boldsymbol{x}|\boldsymbol{y}))+\lambda\log(p(\boldsymbol{y}|\boldsymbol{x}))$$
用贝叶斯公式可以进一步写为：
$$\begin{aligned}\arg\max_x[\log p(\boldsymbol{x}|\boldsymbol{y})+\lambda\log p(\boldsymbol{y}|\boldsymbol{x})]\\=\arg\max_x[(\lambda+1)\log p(x|y)-\lambda\log p(x)]\\=\arg\max_x[\log p(x)+(\lambda+1)(\log p(x|y)-\log p(x))]\end{aligned}$$
然后采用一个可学习的  null vector $\boldsymbol{n}$ 来表示无条件的 $\boldsymbol{y}$。此时采样变成了：
$$p_\theta(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\boldsymbol{n})+(\lambda+1)(p_\theta(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\boldsymbol{y})-p_\theta(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\boldsymbol{n}))$$

### 通过 Multiple Vector Quantizers 在离散 Latent Space 中建模波形

采用预训练的 neural codec 在离散的 latent space 中预测波形。

codec 模型通常包含更多的 codebook：
![](image/Pasted%20image%2020231015115711.png)

由于音频序列很长，对于 10s 的音频，如果下采样因子是 240，那么就有 1000 个 token，如果有 8 个 codebooks，那就是 8000 个 token。于是提出 U-Transformer 来同时建模多个 codebook：
![](image/Pasted%20image%2020231015115616.png)
相比于  Mel-VQ-Diffusion，Wave-VQ-Diffusion 有三个不同：
+ 采用 U-transformer 架构来同时建模多个 codebook
+ 对于不同的 codebook，采用不同的 embedding tables
+ 为 diffusion 过程设计了一个  improved mask and uniform strategy，因为从第一层的 RVQ 到最后一层，codebook 包含的信息量逐渐减少；作者认为前几层的 token 更容易从条件中恢复，而后几层的更难恢复，因为和条件 $\boldsymbol{y}$ 关系不大。因此在 forward 过程刚开始的时候，mask 后面几层的 codebook，而在 forward 过程快结束时，mask 前面几层，从而确保在生成过程中一开始是比较简单的。

###  训练和推理细节

训练目标函数为：
$$\begin{gathered}\mathcal{L}=\mathcal{L}_{diff}+\mathcal{L}_{var}+\lambda_1I(\boldsymbol{z}_e;c)+\lambda_2I(\boldsymbol{z}_e;\boldsymbol{z}_{sid})+\\\lambda_3D_{Euc}(\boldsymbol{z}_p,\boldsymbol{z}_e)-\beta_1\mathcal{F}_1(\theta_1)-\beta_2\mathcal{F}_2(\theta_2)\end{gathered}$$
其中，$\mathcal{L}_{diff}$ 是 diffusion loss，$\mathcal{L}_{var}$ 为 duration, pitch and energy reconstruction loss，$I$ 为互信息，$D_{Euc}$ 为 L2 loss，$\mathcal{F}_1(\theta_1)$ 为 $q_{\theta_1}(\boldsymbol{z}_{\boldsymbol{s}id}|\boldsymbol{z}_e)$ 的似然近似。$\mathcal{F}_2(\theta_2)$ 为 $q_{\theta_2}(\boldsymbol{z}_e|\boldsymbol{c})$ 的似然近似。

推理时，采用  style prompt embedding 模型提取的特征作为风格特征。

整个训练和推理算法如下：
![](image/Pasted%20image%2020231016111733.png)
![](image/Pasted%20image%2020231016111800.png)

# VQTTS- High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature

> interspeech 2022，上海交大，人工智能实验室，俞凯

1. 提出 VQTTS，包含  AM txt2vec  和 vocoder vec2wav，采用自监督的 vector-quantized(VQ) 声学特征而非 mel 谱
2. 其中 txt2vec 是一个分类模型（而非传统的回归模型）
3. vec2wav 使用一个额外的 feature encoder + HiFiGAN generator
4. 可以在自然度方面实现 SOTA 的性能

## Introduction

1. 提出 VQTTS，txt2vec 只需要考虑时间轴上的相关性（而不需要预测在时间和频率上都高度相关的 mel 谱）

## Self-Supervised VQ Acoustic Feature

自监督的 VQ 特征很多已经被用在 ASR 中，如 Vq-wav2vec、wav2vec 2.0。

## VQTTS

已有工作表明，从 VQ acoustic feature 重构波形需要额外的韵律特征。

本文采用三维韵律特征 log pitch, energy and probability of
voice(POV)，然后归一化到 0 均值和单位方差。
> 下面将 VQ 和韵律特征合起来称为 VQ&pro。

VQTTS 包含两个部分：
+ 声学模型 txt2vec 从 phoneme 序列中预测 VQ&pros 
+ vocoder vec2wav 中 VQ&pros 中得到波形

### txt2vec

首先为所有的 phoneme 标上 phoneme-level（PL） 的 prosody。txt2vec 总体架构如下：
![](image/Pasted%20image%2020231016150132.png)
text encoder 包含 6 个  Conformer blocks，将输入 phoneme 编码为 hidden states $\mathbf{h}$，然后送到 PL prosody controller 用于预测 prosody label，通过 duration predictor 得到的 duration 进行拓展。decoder 包含 3 个  Conformer blocks，然后输出通过 LSTM + softmax 激活，用于实现 VQ acoustic feature classification。然后把 decoder 的输出和 VQ acoustic feature 拼接，通过 4 层卷积+layer norm+dropout。

phoneme duration 和 prosody feature 分别采用 L2 和 L1 损失训练，VQ acoustic feature 采用交叉熵损失，总损失如下：
$$\mathcal{L}_{\mathrm{txt2vec}}=\mathcal{L}_{\mathrm{PL\_lab}}+\mathcal{L}_{\mathrm{dur}}+\mathcal{L}_{\mathrm{VQ}}+\mathcal{L}_{\mathrm{pros}}$$

#### Phoneme-level prosody labelling

3 维归一化的 prosody feature 记为 $\mathbf{p}$，然后分别计算 $\Delta\mathbf{p},\Delta^2\mathbf{p}$，最终得到 9 维的特征 $[\mathbf{p},\Delta\mathbf{p},\Delta^2\mathbf{p}]$，然后在所有的帧上计算平均，最终每个 phoneme prosody 都得到一个向量。然后采用 k-means 进行聚类得到 $n$ 类，把聚类中心的 index 作为 PL prosody label。

PL prosody controller 结构如下：
![](image/Pasted%20image%2020231016151423.png)

其训练用于从 text encoder 的输出 $\mathbf{h}$ 中用 LSTM 预测 PL prosody labels。

推理的时候，这两个 LSTM 都采用 beam search decoding 的方法。

### vec2wav

vec2wav 模型架构如图：
![](image/Pasted%20image%2020231016153537.png)
VQ acoustic feature 和 prosody feature 分别通过卷积层，然后拼接之后再通过一个卷积层，然后通过 feature encoder 和 HifiGAN generator。feature encoder 用于 smoothing 不连续的 VQ acoustic feature，HifiGAN generator 用于生成最后的波形。

发现从 0 开始训练 vec2wav 很难收敛（在仅有 HiFiGAN loss 情况下），于是提出  multi-task warmup 的技巧，从 feature encoder 的输出中额外采用一个线性层来预测 mel 谱，此时损失函数变成：
$$\mathcal{L}_{\text{vec}2\text{wav}} = \mathcal{L}_{\text{Hi fi}\mathrm{GAN}}+\alpha\mathcal{L}_{\mathrm{mel}}$$
warmup 之后，移除掉这个任务，即令 $\alpha=0$。

## 实验和结果（略）

这里的 VQ-acoustic feature 是怎么来的呢？
用的是 预训练的 k-means-based vq-wav2vec 模型来提取 VQ acoustic feature。