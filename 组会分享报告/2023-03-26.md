# AUDIO DEEPFAKE DETECTION SYSTEM WITH NEURAL STITCHING FOR ADD 202
> 提出一种能够更有效的检测虚假语音的系统，分析了不同的数据增强能够极大的提高总体性能

## 系统描述

### 方法

1. ResNet 可以捕获区分性的局部特征，然后聚合成 utterance level 的 embedding
2. 提出的方法包含两个网络：
	1. embedding 网络： ResNet-34 + attention pooling（聚合 frame level 的特征），同时计算输出的一阶和二阶矩，一起作为 utterance level 的特征
	2. 分类网络：两个全连接层+2d 的softmax 用于区分真假
	3. 如图：![[Pasted image 20230316173143.png]]
	4. embedding 网络中用 relu 激活，分类网络中用 mish（能够在分类任务中获得更好的性能），采用 focal loss 进行训练

### Neural Stitching

特征有三个类型：
+ 等变性，equivariance，Equivariance studies how transformations of the input image are encoded by the representationinformation or not
+ 不变性，invariance，invariance being a special case where a transformation has no effect
+ equivalence，Equivalence studies whether two representations, for example two different parametrisations of a CNN, capture the same visual information or not
为了研究 equivalence，训练两个不同的 CNN 模型，然后分成四个部分，$\phi=\phi_1 \circ \phi_2$ 和 $\phi^{\prime}=\phi_1^{\prime} \circ \phi_2^{\prime}$，目标是找到映射 $E_{\phi_1->\phi_1^{\prime}}$ 使得 $\phi_2^{\prime} \circ E_{\phi_1->\phi_1^{\prime}} \circ \phi_1$ 能够获得和原始的 $\phi_2^{\prime} \circ \phi_1^{\prime}$ 相同的性能。

这个映射表示为 stitching layer，可以通过一组滤波器或者变换层实现。stitching 操作之后模型的分类性能显著优于之前，表明 **在同一数据集上训练的不同网络之间进行一定程度的特征交换可能是有帮助的**。

受此想法启发，作者希望通过将模型层分解为不同的 stitching part 来了解stitching 在单个模型中是如何工作的。先前的一些工作表明，网络的浅层倾向于学习更一般的表征，而网络的深层更专注于特定的任务。因此，我们在推理阶段 cut off one deep layer ，并将分类层直接 stitch 到ResNet的输出，结果出乎意料地好。

## 实验和结果

### 数据集和特征

将训练集和验证集合在一起得到 55K 条语音 作为总的训练集，每个语音分成固定长度的段，重叠 50%。然后随机选 4000 个段作为验证集，剩下的作为测试集。把 赛道1 和赛道 3.2 的 adaption set 作为评估集。

选择了三个特征：
+ LFCC
+ DCT-DFT spec，和 LFCC 提取过程差不多，只不过没有 linear filterbank 那个步骤
+ Log-linear filterbank energy (LLFB)，和 LFCC 提取过程差不多，只不过没有 DCT 那个步骤

之前的研究证明，静态特征优于动态特征，因此这里只使用了静态特征（没有 delta），结果如下：![[Pasted image 20230316200528.png]]

80维的 ：LFCC，FFT=1024，效果在赛道1是最好的。

应用了两种数据增强：
+ 加入 distortion 和 noises，数据集来自 RIR 和 MUSAN，包含reverb,  noise, music 和 babble 四种，SNR 是0-20 之间的随机数；音量也有影响，把音量随机设置为 -10dB~20dB，最总获得了五倍的数据量（4+1），然后随机 sample 60K 条语音
+ 模拟音频压缩的影响：所有的clean音频都通过音频压缩算法进行模拟，压缩算法包括：MP3, OGG, AAC 和 OPUS。最后还模拟电话的传输，将音频先下采样到8k然后上采样到16k，又获得了五倍的数据（4+1），然后随机选 40k 条。
![[Pasted image 20230316201511.png]]
最终的效果如下：![[Pasted image 20230316201534.png]]
加入噪声训练后，EER 急速下降，说明性能很好！

### 训练

池化层对最终的结果影响很大，本文比较了五种类型的池化层，发现 learnable dictionary encoding(LDE)pooling 和 self multi-head attention(MH) 效果不错。

不使用 neural stitching 之前，有些得分太接近于1 或者 0（太极端了），可能是由于发生了过拟合，使用 stitching 之后得分就合理了很多。

测试集中，使用 neural stitching 能够提升 EER 的性能（EER值降低），之后还进行了一系列的 fine tune 来得到最终的结果：![[Pasted image 20230316202300.png]]

fine tune 的时候，采用了 spec augment 数据增强，同时改变不同的 chunk size，最终结果如下：![[Pasted image 20230316202535.png]]
最好的可以实现 8.83% 的 EER。

然后在上面最优的模型的基础上，将数据重叠从 50% 增加到 70%，将 lr 降低一百倍，继续在这个模型的基础上进行训练。

### 最终提交结果
![[Pasted image 20230316202730.png]]
赛道 3.2 的第一。

# THE VICOMTECH AUDIO DEEPFAKE DETECTION SYSTEM BASED ON WAV2VEC2 FOR THE 2022 ADD CHALLENGE
> 1. 使用预训练的 wav2vec2.0 作为特征提取 + 下游分类器 检测伪造音频，利用 Transformer 不同层的上下文语音表征来捕获有区分性的信息
> 2. 同时使用了不同的数据增强技术来适用新场景

## 方法
![[Pasted image 20230319204607.png]]

### Wav2vec2 特征提取器

分析了 large 的模型，分别采用了 53 和 128 种语言训练，对应 XLS-53，XLS-128。

原始音频首先通过几个 CNN 层的特征提取器，每 20 ms 抽取 1024 维的特征，感受野的范围为 25 ms，然后特征通过 24 层 Transformer获得语音的上下文表征。模型通过对比损失采用自监督学习进行训练。目标是通过上下文表征预测被 mask 的表征的量化值（不是原始值）。

### 分类模型
![[Pasted image 20230319205545.png]]
预训练模型的最后一层 Transformer 的输出可以用于某些语音任务，但是，先前的工作表明，对于一些其他任务使用第一层或者中间层可以得到更多有区分性的信息。

本文使用不同 Transformer 层的表征作为下游模型的输入：
1. 首先，在每层的 Transformer 的输入应用  temporal normalization
2. 对于每个时间步 $t$，每层的输出表征为 $\mathbf{h}_{t, l}$，计算总的输出 $\mathbf{o}_t=\sum_{l=0}^L \alpha_l \mathbf{h}_{t, l}$，$l$ 代表层索引，$\alpha_l$ 为可训练的参数（加起来为1）。
3. 然后将总输出送到两个带 ReLU 和 dropout 的 FF 层中，通过一个 attentive statistical pooling 得到一个单一的 表征
4. 再接 FF 层得到最终的 embedding $\mathbf{e}$
5. 计算余弦相似度 $S=\cos (\mathbf{w}, \mathbf{e}) \in [-1,1]$，其中 $\mathbf{w}$ 是真实语音的 embedding。

模型通过 one-class softmax 损失，训练使得真实语音的得分尽可能地高。

## 实验

对每个 challenge，只使用对应的数据集进行训练。

### ADD 2022 数据集

基于 AISHELL-3 的 clean 语音，包含合成和转换。training 和 dev set 包含 28K 不重叠说话人的语音。

赛道 1 和 2 都包含一个 1K 语音的 adaptation set，100K 语音的 test set（无标签）。adaptation set 和 test set 的音频条件相似，用于使系统适应 test set。

### ASVspoof 2021 数据集

略

### 数据增强

在训练的时候，on the fly 地应用数据增强。主要的增强就是对语音使用 低通 FIR 滤波器，用于估计传输伪影和编解码器的影响。同时mask信号的一部分频率来提高泛化性能。

类似于 [[STC Antispoofing Systems for the ASVspoof2021 Challenge 笔记]] 中的过程，评估了窄带和宽带 FIR 滤波器。

对于 ADD2022，把训练和对应的 adaptation set 合并来提高泛化性。

对于赛道2 ，在每个 epoch 中，选择 20 % 的真实语音，然后选择对应的set中的不同话语的一段可变语音段，将该片段和原始语音随机重叠。

### 训练设置

Adam 优化器，默认学习率，dropout = 0.2，训练时 wav2vec2 的参数固定，只更新分类器参数，batch 为 8，梯度累计为 8，在 dev set 上性能连续10个 epoch不变时停止训练。

## 结果

在 ADD 2022 数据集上的结果：![[Pasted image 20230319214745.png]]
1. XLS-28 的总体效果更好
2. 主要改进还是来自于 adaptation set 的引入
3. 采用 FIR 可以将 EER 降低越 1%

在 ASV spoof 数据集上的效果：
![[Pasted image 20230319215152.png]]
1. NB 对于 LA 的效果很好，DF 更喜欢 WB

和其他系统比：![[Pasted image 20230319215415.png]]


# HuBERT- Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units

 1. 语音自监督有三个挑战：
	1. 每个输入语音中有多个声音单元
	2. 在训练的时候，没有有关声音单元的 lexicon 信息
	3. 声音单元的长度可变，无法进行确切的分割
2. 于是提出用 HuBERT 用于语音的自监督学习，采用离线的聚类方法来提供对齐的标签
3. 关键是仅在 mask 区域计算预测损失，迫使模型在连续输入上学习声学和语言的组合模型，模型主要依赖无监督模型聚类得到的一致性，而不是聚类标签本身

1. 本文引入 Hidden unit BERT，通过聚类生成带噪声的标签，然后用于 BERT-like 的模型进行预训练。BuBERT 模型从连续输入的语音中学习声学和语言模型。模型需要将没有被 mask 的输入建模为潜在表征（声学建模），同时为了减少预测误差，模型需要捕捉学习的表征之间的长时关系（灵感来自于 DeepCluster）
2. HuBERT 在Libri 的数据集上训练时，所有的数据集上的效果都优于 wav2vec2.0

## 方法
![[../语音自监督模型论文阅读笔记/image/Pasted image 20230323182536.png]]
### 学习 hidden unit

简单的离散隐变量模型如 K-mean 或者 GMM 可以推断出 hidden unit，且这些 unit 和潜在的声学单元之间存在某种相关性，图模型或者神经网络可以更好地发现声学单元或者数据的分布。

基于此，提出 acoustic unit discovery models 来提取 frame level 的目标。设一段语音序列 $X=\left[x_1, \cdots, x_T\right]$，一共有 $T$ 帧，得到的 hidden unit 记为 $h(X)=Z=\left[z_1, \cdots, z_T\right]$，其中 $z_t \in[C]$ 是一个 $C$ 类的 categorical variable，$h$ 代表某种聚类模型如 k-means 聚类。
> 这个过程首先是对原始音频提取语音特征，如 MFCC 特征，然后对这些特征进行聚类，最后得到的 time step 要和后面 CNN encoder 得到的 time step 一样（都是 $T$）
> 然后聚类成 $C$ 类，聚类得到的结果经过 code embedding 层之后，就是后面提到的 codeword $e_c$，下标 $c$ 代表第 $c$ 个类别的聚类中心。

### 通过掩码预测进行表征学习

令 $M \subset[T]$ 表示要 mask 的 indices $\tilde{X}=r(X, M)$ 表示被 mask 之后的输入，其中 $x_t$ 被 $\tilde{x}$ 取代（当 $t\in M$）。掩码预测模型 $f$ 输入为 $\tilde{X}$，然后在每个 time step 预测 target 的分布。

mask 过程涉及两个问题，如何 mask 和 在哪计算 预测损失。

采用了 wav2vec 2.0 的方法进行 mask 生成，随机选择 $p\%$ 的 time step 作为初始 indices，mask 的长度为 $l$ step。 然后分开计算 mask 部分和没有 mask 部分的损失，mask 部分的损失 $L_m$ 为：$$L_m(f ; X, M, Z)=\sum_{t \in M} \log p_f\left(z_t \mid \tilde{X}, t\right)$$
没被 mask 部分的损失 $L_u$ 和上面的差不多，出了求和的范围是 $t \notin M$ ，最终的计算为两个的加权：$$L=\alpha L_m+(1-\alpha) L_u$$
极端情况下，$\alpha=0$，只计算没被 mask 部分的损失（类似于声学模型）。而 $\alpha=1$ 时只计算被 mask 部分的损失，这时候类似于语言模型。

### 聚类融合

将多个聚类方法组合起来以提高性能。例如，不同 codebook size 的 k-mean 算法组合可以获得不同粒度的目标。

令 $Z^{(k)}$ 为第 $k$ 个聚类模型生成的目标序列，$L_m$ 可以重新写成：$$L_m\left(f ; X,\left\{Z^{(k)}\right\}_k, M\right)=\sum_{t \in M} \sum_k \log p_f^{(k)}\left(z_t^{(k)} \mid \tilde{X}, t\right)$$
$L_u$ 的计算过程差不多。有点类似于多任务学习，但是任务是通过无监督的聚类创建的。

### refine

一种改进表征的方法是 refine cluster assignments，通过在学习到的潜在表征中创建新一代的聚类，然后获得新的 unit。
> 就是把 Transformer encoder 中的某一层的输出拿出来当成是 $X$，然后通过前面的 acoustic unit discovery models 得到新的聚类中心，作为新的 hidden unit。

### 实现

模型和 wav2vec2.0 差不多，首先是卷积特征编码，然后接 BERT 编码器+投影层+code embedding 层。

考虑了三种不同的配置：base，large 和 x-large，模型具体配置如下：![[../语音自监督模型论文阅读笔记/image/Pasted image 20230323202045.png]]

CNN encoder 从 16KHz  的音频中生成 20ms 帧率的音频（也就是每秒得到 50 帧），然后根据前面的 mask 策略进行随机的 mask，然后输入到 BERT 中产生特征序列 $\left[o_1, \cdots, o_T\right]$ ，然后把问题看成一个分类问题，通过 softmax 得到：$$p_f^{(k)}(c \mid \tilde{X}, t)=\frac{\exp \left(\operatorname{sim}\left(A^{(k)} o_t, e_c\right) / \tau\right)}{\sum_{c^{\prime}=1}^C \exp \left(\operatorname{sim}\left(A^{(k)} o_t, e_{c^{\prime}}\right) / \tau\right)}$$
其中，$A$ 为投影矩阵，$e_c$ 为 codeword $c$ 的 embedding，$\text{sim}(\cdot,\cdot)$ 计算向量之间的余弦距离，$\tau$ 为缩放因子（temperature 系数，设置为 $0.1$），如果采用组合聚类的方法，每个聚类的投影矩阵 都不一样。
> 目的就是把 $e_c$ 当作 target，然后就和普通的 BERT 做 mask 预测差不多。
> 训练的时候就用最简单的交叉熵损失（因为本质是一个 $C$ 分类的问题），相比于 wav2vec2.0 的对比损失+多样性损失，简单了很多！

HuBERT 预训练完成之后，采用 CTC 损失在 ASR 任务中进行 fine tune（CNN 层不做fine tune，会 freeze），去掉投影层然后加上一个随机初始化的 softmax 层。

### 图解

预训练过程：
![[../语音自监督模型论文阅读笔记/image/Pasted image 20230323205222.png]]
> 注：图中有些变量和论文中的命名不一致。


refine 聚类：![[../语音自监督模型论文阅读笔记/image/Pasted image 20230323213137.png]]

