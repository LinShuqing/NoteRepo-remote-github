# 无监督域自适应综述
> APSIPA Transactions on Signal and Information Processing

1. 深度学习需要：
	1. 大量有标记的数据集
	2. 训练数据和测试数据是独立同分布的（i.i.d.）
2. 导致在未知域上的性能无法保证，尤其是针对 OOD 的数据
3. 无监督自适应（UDA）利用有标记的源数据和未标记的目标域数据来实现目标域中的任务，在图像处理、视频分析、NLP、时间序列分析等多方面都有很多进展

## Introduction

1. 深度学习受数据分布的变化的影响，即存在所谓的 域偏移，对于 分布外 （OOD） 数据（源分布和目标分布不同），模型性能可能严重下降
2. UDA 是一种可行的解决方案，将从有标记数据的源域学习的知识迁移到未知的目标域：![[Pasted image 20230413152225.png]]
3. 域自适应可以看成迁移学习的有标记数据仅在源域的一种特殊情况：![[Pasted image 20230413152543.png]]
4. 本文旨在从理论和实践角度给出 UDA 的模型和算法，同时比较了不同的技术，重点在于基于深度学习的 UDA。

## Overview

在 UDA 中，源于分布记为 $p_s(x, y) \in p_{\mathcal{S}}$，然后一个不同的目标域记为 $p_t(x, y) \in p_{\mathcal{T}}$，有标记的数据集 $\mathcal{D}_{\mathcal{S}}$ 来自于源分布，无标记数据集 $\mathcal{D}_{\mathcal{T}}$ 来自于目标分布的边缘分布 $p_t(x)$，UDA 的目标是，通过同时在 $\mathcal{D}_{\mathcal{S}}, \mathcal{D}_{\mathcal{T}}$ 中学习，以提高在源域中训练的模型的泛化性。

记 $\mathcal{Y}=\{1,2, \ldots, c\}$ 是分类任务的标签，$\mathcal{Y}$ 也可以是生成任务中连续的值。UDA 基于以下假设：

对于假设 $h$：$$\begin{aligned}
& \mathcal{L}_t(h) \leq \mathcal{L}_s(h)+d\left[p_{\mathcal{S}}, p_{\mathcal{T}}\right]+ \\
& \min \left[\mathbb{E}_{x \sim p_s}\left|p_s(y \mid x)-p_t(y \mid x)\right|, \mathbb{E}_{x \sim p_t}\left|p_s(y \mid x)-p_t(y \mid x)\right|\right]
\end{aligned}$$
其中，$\mathcal{L}_t(h),\mathcal{L}_s(h)$ 是在假设 $h$ 下在目标域和源域的损失，$d[\cdot]$ 表示 散度测度（如 JS 散度）  ，第三项 $\min \left[\mathbb{E}_{x \sim p_s}\left|p_s(y \mid x)-p_t(y \mid x)\right|, \mathbb{E}_{x \sim p_t}\left|p_s(y \mid x)-p_t(y \mid x)\right|\right]$ 是一个可以忽略的值，从而源域中的损失+域之间的散度可以看成是目标域损失的上界，源域损失可以通过有监督的模型训练来进行优化，那么最终 UDA 的目标就是最小化两个域之间的**散度**。

域偏移可以分为四类：![[Pasted image 20230413164348.png]]
本文主要关注 UDA 中的 covariate shift。

## 方法

目前流行的方法的汇总：![[Pasted image 20230413164819.png]]

### Statistic Divergence Alignment（SDA）

想法就是，学习域不变的特征，也就是最小化 latent space 中不同域之间的差异，核心思想就是选择一个测度来量化这种差异，即 divergence measure，如 MMD（Maximum Mean Discrepancy） 、correlation alignment 、contrastive domain discrepancy、Wasserstein distance、graph matching loss 等等。

MMD 计算观察样本的分布差异，即通过比较两个域的 smooth function 的平均值，值越大表示差异越大。通常将 RKHS 空间中的单位球作为 smooth function，当两个分布相同时，函数值为 0 。然后还有一个 alignment component，其目标是分类，计算 MMD 并将其最小化，最终的目标是对齐源域和目标域。
>有两个分布的样本，通过寻找在样本空间上的一个映射函数 $f$，求两个分布的样本在 $f$ 上的函数值的均值，通过把两个均值作差可以得到两个分布在 $f$ 下的 mean discrepancy。寻找一个 $f$ 使得这个 mean discrepancy 有最大值，就得到了MMD。
>最后取MMD作为检验统计量（test statistic），从而判断两个分布是否相同。如果这个值足够小，就认为两个分布相同，否则就认为它们不相同。更加简单的理解就是：求两堆数据的均值的距离。
>![[Pasted image 20230413193731.png]]

CORAL 被定义为两个域的特征之间的二阶统计量差异，其实就是协方差。

CDD 将 label 纳入到 MDD，通过最小化 CDD，不同类之间的差异变大，同类之间的差异变小。而如果标签缺失，则采用 对比自适应网络（CAN）来估计标签，然后再最小化 CDD。

Wasserstein距离（也被称为最优传输距离）也可以用于测量分布之间的差异。

graph matching（图匹配）旨在找到两个图之间的最佳对应关系，对于一批样本，特征提取可以被视为无向图中的节点。两个节点之间的距离表示它们的相似性。域差异被定义为源和目标域构成的图之间的 match cost。

### Adversarial Learning

前面的方法是手动选度量，其实可以自适应地学习这个度量。

根据前面的公式，为了最小化目标域损失的上界（不等号右边），对抗 UDA 通过采用判别器来公式中的最小化域差异项：![[Pasted image 20230413193744.png]]
特征提取器 $f(\cdot)$ 用于提取特征 $f(x) \in \mathbb{R}^K$，我们希望 $d\left[p_s(f(x)), p_t(f(x))\right]$ 尽可能地小。也就是说，除了要训练分类器能够正确的对源数据进行分类，$f(\cdot)$ 要优化以使得源分布和目标分布尽可能地相似。这个过程主要通过一个有监督的域判别器来实现，$\mathbb{R}^K \rightarrow(0,1)$，总的来说，三个模块的目标函数定义为：$$\begin{array}{ll}
\max _{C l s}^{\max } & \underset{x \sim p_s}{\mathbb{E}} \log C(f(x), y) \\
\max _{D i s} & \underset{E}{\mathbb{E}} \ln \log (1-\operatorname{Dis}(f(x))+\underset{x \sim p_t}{\mathbb{E}} \log \operatorname{Dis}(f(x)). \\
\max _f & \underset{x \sim p_s}{\mathbb{E}} \log C(f(x), y)+\lambda_{x \sim p_t}^{\mathbb{E}} \log (1-\operatorname{Dis}(f(x)),
\end{array}$$
其中，$\lambda \in \mathbb{R}^{+}$ 用于平衡两个损失。
> 分类器和在源域训练差不多，目标是尽可能准确地分类源域数据。
> 判别器用于判断特征是来自源域还是目标域
> 特征提取器 $f$ 这里有两个目的，一个是尽可能欺骗判别器，来生成在源域和目标域之间有着较少差异的特征，另一个目的是输入好的特征来给分类器使其可以正确分类。

在具体的模型上，domain adversarial neural network (DANN) 采用所谓的梯度反转层作为判别器。adversarial discriminative domain adaptation (ADDA) 通过源域训练初始的目标模型，然后进行对抗自适应，这相当于目标域特定分类器。

由于传统的对抗 UDA 中通常使用 JS 散度，但是 JS 散度对于两个完全不重叠的分布，其散度是一个固定值（也就是无法区分他们之间到底差多少。Wasserstein距离可以解决这种问题，同时也被用于 GAN 中，因此也有用于 估计Wasserstein距离 的判别器。

### Normalization Statistics

DNN 中，batch norm 通常可以实现更快的训练和更平滑的优化和更快的收敛，原因在于 BN 中有两个低阶统计量，即均值和方差，以及两个高阶的可学习的统计量，即缩放因子和偏置（scale，bias）。

早期的一些工作把 BN 用于DA，AdaBN 在把模型用于目标域的时候，除了 BN 层其他的参数都是直接用源域的；AutoDIAL 用于泛化 AdaBN，通过一个额外的 domain alignment layers 来重新训练网络的权重：![[Pasted image 20230413195901.png]]

### Generative Domain Mapping

前面的方法是在 latent space 中对齐特征，也可以在原始数据上直接对目标域的数据进行映射。网络就可以在生成的目标域的数据（来自于源域）上进行训练，此外，网络也可以同时和 GAN 进行训练：![[Pasted image 20230413201146.png]]
> 意思就是，训练一个 style translator，将源域和目标域的数据尽可能地靠近，然后就和之前一样，一个判别器用于区分源域和目标域，一个分类器用于对转换后的源域数据进行分类。
> 完成训练后，直接将目标域的数据输入到分类器中即可进行分类。
> 原理有点类似于 cyclegan。
> 其实如果不做分类任务的话，这个目标就和图像风格转换差不多了。

### Self-training

self-training 是一种训练方案，利用未标记的目标域数据来实现域自适应，self training 是一个 “回合制” 的训练策略，有两步：
1. 在目标域中生成伪标签
2. 使用生成的伪标签和数据来训练网络
> 具体来说：
> 	+ 利用已标记的数据来训练一个好的模型，然后使用这个模型对未标记的数据进行标记。
> 	+ 进行伪标签的生成，因为我们知道，已训练好的模型对未标记数据的所有预测都不可能都是好的，因此对于经典的 Self-training，通常是使用分数阈值（confidence score）过滤部分预测，以选择出未标记数据的预测标签的一个子集。
> 	+ 将生成的伪标签与原始的标记数据相结合，并在合并后数据上进行联合训练。
> 	+ 整个过程可以重复 n 次，直到达到收敛。
> 效果还挺好的，有些已经超过了 基于对抗训练的方法。

这其中一个比较重要的问题是，伪标签的噪声往往很大从而导致标签不可靠。很多论文都是专注于解决这个问题。

### Self-supervision

UDA 的另一个解决方法是，在训练时引入辅助的自监督任务，可以通过在源域分类+目标域重构来实现对齐，也可以同时在源域和目标域进行重构。然后通过重构损失优化重构网络。

### Low density target boundary

也有用半监督方法的，即基于聚类的 UDA，来自同一类的目标域样本一般在聚类上分布接近，从而目标域的分类决策界应该位于低密度区域。

## 应用

1. 图像：图像分类、图像检测、图像分割、图像生成
2. 医学图像处理
3. 视频分析
4. 自然语言处理
5. 时间序列分析

# Improving Distortion Robustness of Self-supervised Speech Processing Tasks with Domain Adaptation
> Interspeech 2022

1. 语言失真会降低语言处理模型的性能
2. 提出使用 domain adversarial training 来提高模型的鲁棒性
3. 基于 SUPERB，在五个不同的语音处理任务上进行实验
4. 由于不知道失真的具体类型，分析了两个域（将所有的失真看成一个域）和多个域（不同的失真看成是不同的域）的情况，获得了较好的效果

## Introduction

1. 域不匹配会降低声学模型的性能，一些常见的域不匹配包括：
	1. 口音
	2. 多语言
	3. 失真（本文关注）
2. 过去已经有一些方法用从噪声语音中恢复干净的语音，但是在未知域中不能保证性能
3. 自监督模型在目标域中进行继续训练可以提高性能
4. 另一个方法是基于 DANN 进行域对抗训练（DAT），DANN 包括特征提取、下游模型和联合训练的域分类器。域分类器用于区分目标域和源域，从而使得特征提取器的输出可以是域不变的
5. DAT 已经被用于 带口音的 ASR、多语言情感识别、TTS、说话人识别和语音增强等
6. 本文将 DAT 的概念和自监督结合，提出使用 DAT 继续训练自监督模型（无标签）来提高失真场景下的性能，效果很好，有时候还超过了在有标签的失真数据下训练的模型

## 噪声鲁棒的自监督训练

一般来说，语音中的上游任务看成是一个特征提取器 $f(s;\theta_f)$，输入波形 $s$ 输出表征 $\hat{z}$，下游任务是一个标签预测 $y(\hat{z},\theta_y)$，域不匹配发生时，测试数据的分布和训练数据的分布不同，如语音失真。

本文的任务就是使模型 $\theta_f$ 可以适用于不同的域以提高泛化性。

给定源域数据 $\mathcal{S}=\{s_1,\dots,s_N\}$ 包含 $N$ 条语音，对应的标签 $\mathbf{y}=\{y_1,\dots,y_N\}$，如果测试集 $\mathcal{C}=\{c_1,\dots,c_N\}$ 和训练集是同一个分布下的，那么效果当然很好，但是如果是不同的分布 $\mathcal{C}^{\prime}=\left\{c_1^{\prime}, \cdots, c_L^{\prime}\right\}$，此时性能可能下降。

假设有无标签的数据集 $\mathcal{T}=\left\{\left\{\hat{s}_i^1\right\},\left\{\hat{s}_i^2\right\} \cdots,\left\{\hat{s}_i^K\right\}\right\}$ 包含 $K$ 种失真，$\left\{\hat{s}_i^k\right\}$ 是失真 $k$ 的音频集合，本文考虑两种情形，$\mathcal{C}^{\prime} \in \mathcal{T} \text { and } \mathcal{C}^{\prime} \notin \mathcal{T}$（也就是测试数据是否在失真集内）。用 $\mathcal{T}$ 来更新自监督模型 $\theta_f$，使其泛化到目标域 $\mathcal{C}^{\prime}$，有两个步骤：
1. 上游预训练的时候连续训练
2. 采用 DAT 进行 fine tune

### DAT

引入一个域判别器（也可以说域分类器）$d(\hat{z},\theta_d)$ 用来预测给定输入语音 $s$ 的域，参数通过以下公式联合优化：$$\begin{gathered}
\theta_y \leftarrow \theta_y-\alpha \frac{\partial L_y}{\partial \theta_y}, \quad \theta_d \leftarrow \theta_d-\beta \frac{\partial L_d}{\partial \theta_d} \\
\theta_f \leftarrow \theta_f-\eta\left(\frac{\partial L_y}{\partial \theta_f}-\lambda \frac{\partial L_d}{\partial \theta_f}\right)
\end{gathered}$$
其中，$L_f,L_y,L_d$ 分别为特征提取器、标签预测器和域分类器的损失。$\alpha,\beta,$ 为学习率，特征提取器有一个负的域分类器损失项从而实现对抗训练，梯度反转缩放因子 $\lambda$ 用于缩放负损失。

对于域分类器，有两种不同的设置。

A. 两个域
数据只包含两个域，clean 的语音 和 失真的语音 $\mathcal{T}$。此时域分类器 通过 BCE 损失进行优化：$$\mathcal{L}_{B C E}=-\frac{1}{N+M} \sum_{i=1}^{N+M} d_i \cdot \log p_i+\left(1-d_i\right) \cdot \log \left(1-p_i\right)$$
其中，$p_i$ 为域分类器经过 sigmoid 之后的输出，$d_i$ 为标签预测器输出的label，$N,M$ 代表样本数。

B. 多个域
语音被分成 $K+1$ 个域，$K$ 个失真的，一个 clean 的。这时有两个目标函数：
CE 损失：$$\mathcal{L}_{C E}=-\frac{1}{N+M} \sum_{i=1}^{N+M} \sum_{k=0}^K d_i^k \log p_i^k$$
和前面的差不多，就是改成了多分类的损失。但是在 DAT 时，最大化这个损失（前面有负号）会导致域分类器的错误分类。

熵损失 Entropy loss 定义为：$$\mathcal{L}_E=-\frac{1}{N+M} \sum_{i=1}^{N+M} \sum_{k=0}^K p_i^k \log p_i^k$$
通过在 DAT 阶段最大化这个函数，可以使得域分类器输出不同类别的均匀分布，也就是确保它能够对于每个域输出相似的概率。
> 以为域分类器本质是一个多分类问题，如果没有熵损失，对于失真的域，它可能每输出的都是 $K$ 个中确定的一个，从而导致 CE loss 非常大，但是这个时候模型学习的就不是区分 clean 域和 distortion 域，而是区分不同的 distortion 域，这显然不是我们需要的。


## 实验

在五个任务中做实验：
+ 意图分类：三类，action, object, and location
+ 情感识别：9个情感类
+ 关键词识别
+ 说话人识别
+ 语音识别

### 数据配置

噪声数据 $\mathcal{T}$ 包含三种失真：
+ musan 噪声
+ 高斯噪声
+ 混响
三个域的比例是 0.3,0.4,0.3，每个语音只有一种失真，加性噪声的 SNR 从 10 到 20 dB。

测试集三种配置：
+ 原始的测试集
+ 原始的测试集+失真（和目标域一致的）
+ 原始数据集+失真（新的）

### 上游模型

采用预训练的 HuBERT 作为基本模型，首先用前面的数据集 $\mathcal{T}$ 训练了 60 epoch（3+1=4，各类占 1/4）。

### 域分类器

mean pooling 层+线性层

## 结果

具体结果见原论文。

结论：
1. continual training 大部分情况下可以提高性能
2. DAT + ce loss 大部分情况下也都优于 baseline，但是在 ASR 上的改进没有 continual training 的效果大
3. DAT + continual training 在个别情况下比直接拿有标签的数据效果还好，说明潜力很大！
4. 当在训练时不知道具体的失真类型时，二域设置是有帮助的。

# Listen, Adapt, Better WER- Source-free Single-utterance Test-time Adaptation for Automatic Speech Recognition
> Interspeech 2022

1. Test-time Adaptation（也被称为 Source-free Domain Adaptation） 最开始用在 CV 中，通过调整在源域训练的模型来实现对测试域样本更好的预测（通常是 OOD）
2. 本文提出用于语音识别的 Single-Utterance Test-time Adaptation (SUTA) 框架，首次研究ASR中的 TTA 
3. 经验表明，SUTA 可以提高域外目标语料库和域内测试样本上 ASR 模型的性能

## Introduction

1. iid 条件下，基于深度学习 ASR 效果很好，但是当测试数据和训练数据的 分布不同时，性能会验证下降
2. UDA 是一种常用的方法，但是 UDA 也需要源域数据和目标域数据，而这通常会首先限制：
	1. 出于隐私原因，源数据有时候可能没有
	2. 需要收集和处理目标数据，耗时
3. TTA 可以在很少的目标域数据（一个batch或者就一个 instance）且不需要获得源域数据的情况下，有效地适应模型，在 CV 中用处很大
4. 大部分的 TTA 都限制在 batch level ，而 SITA 提出一种 single instance TTA 方法，不需要预先收集 batch 的 test samples，但是仍专注于 cv ，且依赖于数据增强和 BN 层
5. 本文提出了Single-Utterance Test-time Adaptation，SUTA 框架，可以用于任何基于 CTC 的 ASR 模型，只需要一条语音就可以以无监督的方式进行 TTA

## 方法

设 ASR 模型为 $g(y \mid x ; \theta)$，参数为 $\theta$，输入语音 $x$，输出没有归一化的词类别 $y$。其中，$\theta$ 可以分为两部分，$\theta_{\mathbf{f}}$ 在 adaptation 阶段参数被冻结，$\theta_{\mathbf{a}}$ 在 inference 的时候更新。测试数据集 $D_{test}$ 包含 $n$ 条语音 $\{x_1,x_2,\dots,x_n\}$，本文重点在于，使用一条语音 $x_i$ 无监督地适应 $\theta_{\mathbf{a}}$。

使用带有 CTC loss 的 Transformer Encoder 作为 ASR 模型，但其实可以用在各种模型上。

设 字符+ CTC blank token 的数量为 $C$，CTC 输出的时间帧为 $L$，则输出 $\mathbf{O}\in\mathbb{R}^{L\times C}$。

### SUTA

![[Pasted image 20230414211224.png]]

#### Entropy minimization

由于测试的时候 label 未知，自适应时采用无监督的、基于熵的损失函数。Entropy Minimization 用于在大量目标数据的情况下进行域自适应。

本文使用 EM 目标函数，基于单条语音，对模型参数进行无监督的 TTA，损失计算为：$$\mathcal{L}_{e m}=\frac{1}{L} \sum_{i=1}^L \mathcal{H}_i=-\frac{1}{L} \sum_{i=1}^L \sum_{j=1}^C \mathbf{P}_{\mathrm{ij}} \log \mathbf{P}_{\mathbf{i j}}$$
> 计算的时候忽略了 blank token 来避免类不平衡问题。

#### Minimum class confusion

在 EM 目标函数中，Minimum Class Confusion（MCC）目标函数作为一种替代的模型参数调整方法，主要通过减少不同类别之间的相关性来实现。

MCC 损失计算为：$$\mathcal{L}_{m c c}=\sum_{j=1}^C \sum_{j^{\prime} \neq j}^C \mathbf{P}_{\cdot \mathbf{j}}^{\top} \mathbf{P}_{\cdot \mathbf{j}^{\prime}}$$
其中，$\mathbf{P}_{\cdot \mathbf{j}} \in \mathbb{R}^L$ 表示第 $j$ 类在长为 $L$ 帧下的概率向量。通过对混淆矩阵非对角元素添加惩罚来最小化不同类之间的相关性。
> 公式其实就是在求所有非对角元素的值的和。最小化这个值，可以减少类之间的相关性，因为如果类相关性比较大，那么输出的时候这两个类的概率值会比较接近，体现在 confusion 上就是值比较大，通过最小化 confusion 就可以减少类相关性。

#### Temperature smoothing

基于 EM 损失或者 MCC 损失的原始的 TTA 效果提升不大，原因可能是因为 entropy loss 对于一些预测信心比较大的帧，值很小，从而导致不能从这些帧中获得指导，甚至还可能存在梯度消失的问题。从而使得 $\mathcal{L}_{em}$ 主要受那些不确定的帧的影响，导致更新的方向不可靠。
> 也就是说，我们既要用上这些确定性较大的帧，又要让这些帧的影响比较大（因为对于确定性大的帧，entropy 小，导致其对loss的影响很小，我们的目的是增大这些影响。）

本文使用 temperature scaling 方法来平滑输出的概率分布，从而保留高置信帧的影响，平滑后的输出分布为：$$\mathbf{P}_{\cdot \mathbf{j}}=\frac{\exp \left(\mathbf{O}_{\cdot \mathbf{j}} / T\right)}{\sum_{j^{\prime}=1}^C \exp \left(\mathbf{O}_{\cdot \mathbf{j}^{\prime}} / T\right)}$$
其中，$\mathbf{O}_{\cdot \mathbf{j}}$ 为第 $j$ 类在所有帧的输出 logits，$T$ 大于 $1$。

#### 总训练目标函数

为了防止过拟合，在两个损失中进行了优化：$$\mathcal{L}=\alpha \mathcal{L}_{e m}+(1-\alpha) \mathcal{L}_{m c c}$$
其中，$\alpha$ 为超参数。

算法的总体流程如下：![[Pasted image 20230415110135.png]]
从语音 $x$ 开始，计算得到模型的输出 $\mathbf{O}$，然后进行 temperature smoothing，通过最小化训练损失来更新参数 $\theta_{\mathbf{a}}$，迭代 $N$ 次之后得到的模型 $g\left(y \mid x ; \theta_{\mathbf{f}}, \theta_{\mathbf{a}}^{\mathbf{N}}\right)$ 用于最后测试集的推理。

## 实验

### 源 ASR 模型

采用 Wav2vec 2.0-base CTC model 模型，模型在 Librispeech 上预训练，所以 Librispeech 就是源域。

### 数据集

目标域在以下数据集进行测试：
+ Librispeech，用于验证模型是否可以在源域中性能不下降，同时还引入了高斯噪声
+ CHiME-3，真实环境下带噪数据
+ Common voice
+ TEDLIUM-v3

### Baseline TTA 模型

之前没有 single utterance 的TTA，于是用 Single-utterance dynamic pseudo labeling 作为 baseline，通过最小化 CTC loss 来使用 ASR 模型预测伪标签。伪标签通过贪婪算法在每个迭代步骤动态更新。

### 实现细节

设 layer normalization 的参数为 LN，特征提取的参数为 feat，整个模型的参数为 all。

采用 AdamW，学习率搜索发现，对上面的三个部分，最佳的学习率分别是 2e-4, 2e-5, 1e-6（要学习的参数越多， lr 越小）。

所有的实验中，$\alpha=0.3$，迭代次数为 10，$\theta_{\mathbf{a}}$ 为 LN+feat，在 3090 GPU 上训练。
 
### 结果

![[Pasted image 20230415151207.png]]
说明：
1. SOTA 结果最好是肯定的，因为是在目标域下训练和测试的
2. SUTA 在所有的数据集中都优于 SDPL 的 baseline
3. 看右边第一列，TTA 好像也可以提高域内样本的性能

![[Pasted image 20230415151918.png]]
左边的表给出了消融实验的结果：
1. EM 和 MCC 损失都可以提高性能，$\alpha=0.3$ 是最好的
2. temperature smoothing 非常重要
3. 只 fine tune LN的效果不如 fine tune LN+feat，但是fine tune all 反而导致轻微的性能下降
4. 迭代步数超过10性能会变差
5. 音频长度也有影响，小于 2s 的音频WER 相比于长于 2s 的音频的 WER 更低

# AdapterBias- Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks
> NAACL 2022

1. Adapter 还是需要相对大量的参数
2. 提出 AdapterBias，在 Transformer 层的 hidden output 中添加 token 无关的 shift，也就是只加了一个 vector 和一个 linear 层
3. 进行了大量的实验来表明其有效性

## Introduction

1. 在低资源的数据上 fine tune 预训练的模型 会不稳定
2. 于是提出 [[../经典模型和算法/PEFT/Adapter]]，但是不知道 adapter 是不是能够进一步实现 parameter-efficient
3. Diff pruning 通过学习 task specific 的 "diff" 向量，拓展原始的预训练参数，同时通过 L0 正则化提高稀疏性
4. BitFit 表明，对于中小型数据，只在 一些 bias term 上进行 fine tune 可以和 fine tune 整个模型相竞争
5. 这些工作的核心都是在PLM 输出层添加  task-specific shifts 用于适应不同的任务
6. 基于此理念，本文提出了由一个 vector 和一个线性层 $L_\alpha$ 组成的 AdapterBias，在 shifts 中添加 token-dependent biases：
	1. vector 表示特定任务的 shift
	2. $L_\alpha$ 为输入 token 产生权重
	3. 通过 vector 和 weight 来添加 token-dependent shift
	4. 和 BitFit 有点类似，比较如下：![[Pasted image 20230416105422.png]]
	5. 最终可以以更少的参数获得和 Adapter 相当的性能，而且通过一些正则化措施可以进一步降低参数量

## 方法

为了在不同的任务中进行更好的自适应，adapter 需要 token-specific。AdapterBias 则基于输入的 token 为 bias 产生合适的权重。

在 fine tune 预训练模型是，记训练数据为 $D=\left(x_i, y_i\right)_{n=1}^N$，假设模型的参数为 $\theta$，AdapterBias 的参数为 $\theta^\prime$，训练的时候，冻结 $\theta$ ，只 fine tune $\theta^\prime$。

架构如图：![[Pasted image 20230416110023.png]]
包含两个部分：
+ vector $v$
+ linear layer $L_\alpha$

$v$ 是 task-specific shift，$L_\alpha$ 输出一个 token-dependent 加权权重向量 $\alpha=\left[\alpha_1, \alpha_2 \ldots \alpha_m\right]^T$，其中 $\alpha_i$ 为第 $i$ 个 token’s representation shift 的权重。通过加权计算，AdapterBias 能够专注于对任务更重要的权重，并且可以有效地适用于不同的下游任务。

> 这里 第二个 sub layer 的 layer norm 层也做了 fine tune。

定义 bias 为 $B$：$$B=v \otimes \alpha^T=\left(\begin{array}{llll}
\alpha_1 v & \alpha_2 v & \ldots & \alpha_m v
\end{array}\right)$$
其中，$v \in \mathbb{R}^r, \alpha \in \mathbb{R}^{m},B \in \mathbb{R}^{r \times m}$，表示 $m$ 个 token，每个 token 的表征为 $r$。具体来说，计算过程如下：![[Pasted image 20230416111114.png]]
假设序列的长度为 $m=3$，第一层 layer norm 输出的表征为 $(r_1,r_2,r_3)$，其维度就是 Transformer 中的 $d_{model}$（BERT 中就是 768）。通过 FFN 之后的 token 作为 $L_\alpha$ 的输入，而其输出 $\alpha \in \mathbb{R}^3$，然后乘起来得到 $B$，比如其中的 $b_1$ 就可以看成是 第一个 token 的 bias。

### 进一步提高 AdapterBias 的参数效率

方法1：跨层共享
根据 Adapter 中的实验，低层的 adapter其实是存在一些冗余的，于是可以在不同的层中共享 adapter 的权重来减少参数。

方法2：$L_0$ 正则化
对 $L_\alpha$ 添加 dropout，进一步提高参数效率。此时的优化问题可以看成：$$\min _{\theta^{\prime}} L\left(D ; \theta, \theta^{\prime}\right)+\lambda\left\|\theta_{L_\alpha}^{\prime}\right\|_0$$
其中，$L(D;\cdot)$ 表示原来的损失，$\lambda$ 为超参数。

## 实验

在 HuggingFace PyTorch 中的 BERT 和 RoBERTa 上进行实验。

在 GLUE 上的结果：![[Pasted image 20230416133039.png]]
可以用最少的参数达到和其他模型相当的性能。

不同模型的泛化性对比：![[Pasted image 20230416133534.png]]
在多个模型上效果都不错。

消融实验：![[Pasted image 20230416134415.png]]
共享线形层 $L_\alpha$ 的参数效果最好。
对于正则化，可以在 base 模型中提高性能，但是在 large 模型中不能。

# Parameter-Efficient Transfer Learning for NLP
> ICML 2021

1. fine tune 预训练模型在很多下游任务中效率通常都很低，因为需要为每个任务独立训练一个全新的模型
2. 提出使用 adapter 模块进行迁移学习，即在原模型的参数保持不变的条件下，对于每个任务只要添加很少的可训练的参数
3. 将 BERT 模型迁移到 26 个不同的文本分类任务中，可以获得接近 SOTA 的性能

## Introduction

1. 从预训练模型中进行迁移学习可以在 NLP 任务中获得很好的性能
2. 本文的目的是，建立一个在所有方面任务都表现良好的系统而不用为每个任务训练一个全新的模型，提出的迁移学习策略可以获得 compact model，即在每个任务中只有少量的附加参数，通过增量训练这些附加的参数而不改变原始模型的参数，且可以获得较好的性能
3. NLP 中两个常见的迁移学习方法是，基于特征的迁移和 fine tune，这两个方法都需要重新训练模型的权重
4. 提出的 adapter 参数效率很高，其本质是**在预训练的网络层之间添加一个新的模块**，相比于前面两种迁移方法：
	1. 考虑以 $\boldsymbol{w}$ 为参数的神经网络模型 $\phi_{\boldsymbol{w}}(\boldsymbol{x})$
	2. 基于特征的迁移学习将 $\phi_{\boldsymbol{w}}$ 和一个新的模型 $\chi_{\boldsymbol{v}}$ 进行组合，得到 $\chi_{\boldsymbol{v}}\left(\phi_{\boldsymbol{w}}(\boldsymbol{x})\right)$，训练的时候只训练其中的 task specific 的参数 $\boldsymbol{v}$
	3. 而 fine tune 则更简单，直接调整整个原始模型的参数 $\boldsymbol{w}$
	4. adapter 定义了一个新函数 $\psi_{\boldsymbol{w}, \boldsymbol{v}}(\boldsymbol{x})$，其中 $\boldsymbol{w}$ 来自于预训练的模型参数，初始参数 $\boldsymbol{v}_0$ 满足 $\psi_{\boldsymbol{w}, \boldsymbol{v}_0}(\boldsymbol{x}) \approx \phi_{\boldsymbol{w}}(\boldsymbol{x})$，训练的时候，只更新参数 $\boldsymbol{v}$，这里的 $\boldsymbol{v}$ 通常是在原来的模型上添加新的模块来实现，而且参数 $\boldsymbol{v}$ 的数量远小于 $\boldsymbol{w}$
5. 基于 adapter的 fine tune 有点类似于多任务学习和连续学习
	1. 多任务学习也是要一个 compact model，但是需要同时访问多个任务，而 adapter 不需要
	2. 连续学习就是从连续不断的任务流中一直学学学，其缺点就是可能会忘掉之前的任务
	3. adapter 不同之处在于，各个任务互不相干，且原始模型的参数不会更新（也就是不会忘记预训练学到的知识）
6. 本文的关键创新就在，设计了一个有效的 adapter 模块，将他和 BERT 集成起来，效果几乎可以和直接进行 fine tune 相当，但是只要调整原始参数 3% 的参数大小，是一个即插即用的模块

## Adapter

核心原理：adapter tuning 策略在原始的网络中添加新的层，训练过程中，原始模型层的参数保持不变，新添加的 adapter  参数随机初始化。

Adapter模块 的两个特点：
+ 参数少
+ 近乎恒等的初始化（为了实现稳定训练）

考虑 Transformer 的标准模型：![[../经典模型和算法/PEFT/image/Pasted image 20230416094446.png]]
Adapter 位于每个 sub layer 的输出，通过残差连接之后的输出直接传递到 layer norm 中。

为了限制参数的大小，还提出了一个 bottleneck 的结果，即 adapter 首先将 FFN 输出的 $d$ 维的特征投影到一个较小的维度 $m$，然后通过一个非线性层，再投影回 $d$ 维。那么包含 bias 的总的模块参数为 $2md+d+m$，如果 $m\ll d$ ，则可以极大地限制参数量，实际用的时候只用了原始模型 $0.5-8\%$ 左右的参数，由于 adapter 内部本身也有一个残差连接，可以很方便地实现恒等初始化（所有的参数都是 0）。

同时对于每个任务也单独训练了 layer norm 层的参数，类似于 batch norm 的 parameter-efficient，但是效果不太好。

## 实验

采用预训练的 BERT 作为 base 模型，分类的时候将第一个 token 的输出作为 classification token，然后训练一个 linear 层实现分类。

4 TPU 上训练，batch 为 32。

### GLUE benchmark

采用 BERT-LARGE 模型，330M 参数训练 adapter的时候，通过超参数扫描获得最佳的超参配置。

结果：
![[../经典模型和算法/PEFT/image/Pasted image 20230416100127.png]]

参数和性能的 trade-off：![[../经典模型和算法/PEFT/image/Pasted image 20230416101051.png]]


### 消融实验
![[Pasted image 20230416100730.png]]
纵轴-横轴表示删掉位于其中层的 adapter 的效果，对角线表示删掉单独的这一层的 adapter，右上角表示全删掉（也就是不用 adapter的结果）。
表明：
1. 删除单层对效果影响较小
2. 全删会导致性能大幅下降
3. 删掉低层的影响更小，删掉高层的影响更大