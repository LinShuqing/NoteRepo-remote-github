# WavLM- Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing 笔记

> Microsoft，2022

1. SSL 在 ASR 中很大成功，但是在其他任务上却很少使用
2. 提出 WavLM，全栈式解决所有的下游语音任务
3. 通过在预训练时学习进行 掩蔽语音预测和去噪，前者任务可以保留语音内容的建模，而后者任务通过语音降噪任务可以提高在非 ASR 任务中的潜能

## Introduction

1. SSL 在 NLP 领域取得了巨大成功，对于语音，主要专注于 ASR
2. 对于非 ASR 任务，需要一个全栈的通用的预训练模型
3. 但是语音中不同的任务关注语音信号的不同方面，ASV 学习说话人特征而不关注内容，ASR 则只关注内容而不关注说话人特征，SD 和 SS 则涉及多个说话人，SUPERB 证明，使用来自不同层的 embedding 的加权和，预训练模型在全栈语音任务上有很大潜力（不同层有不同任务的信息）
4. 现有预训练模型存在一些缺点：
	1. 多说话人任务效果不行
	2. 训练数据和真实场景相差较大
5. 本文提出 WavLM，提出 masked speech denoising 和 prediction 框架，输入是被 mask 的模拟带噪或者重叠语音，目标是预测伪标签，有点类似于 HuBERT，由于同时预测 mask 语音和降噪，模型不仅可以学 ASR 的信息，也可以学非 ASR 的信息
6. 将 门控相对位置偏移（grep）添加到Transformer结构，提高了ASR的模型性能，与wav2vec 2.0和HuBERT中使用的卷积相对位置相比， gate 允许通过调节当前语音内容来自适应地调整相对位置偏差，训练采用 94k小时的公开音频
7. 在19个任务上评估模型，效果都很好，而且代码开源！！！

## 相关工作（略）

## 背景：HuBERT

HuBERT 的 backbone 是 $L$ 层的 Transformer ，训练时，输入是 masked 的声学特征 $\mathbf{u}$，输出是 hidden state $\mathbf{h}^{L}$，通过预测离散的目标 $\mathbf{z}$ 来优化模型，这里每个 $z_t\in[C]$ 都是 $C$ 类的 categorical 变量。

HuBERT 采用 masked speech prediction 任务，损失仅在 masked 的区域进行计算。

迭代进行 re-clustering 和 re-training，第一次迭代，目标是 MFCC 聚类，第二次迭代，通过对第一次迭代得到的模型所生成的表征来进行聚类。

## WavLM

提出 masked speech denoising 和 prediction 框架。

### 模型架构
![](image/Pasted%20image%2020230525223344.png)
采用 Transformer 作为 backbone，包含
+ 卷积特征编码，包含 七个  temporal convolution+layer norm+GELU，一帧大概 25 ms，输出为 $\mathbf{x}$
+ Transformer Encoder，采用 gated relative position bias，令 $\left\{\mathbf{h}_i\right\}_{i=1}^T$ 表示 attention 的输入，每个 $\mathbf{h}_i$ 都投影到 Q、K、V ：$$\mathbf{q}_i, \mathbf{k}_i, \mathbf{v}_i=\mathbf{h}_i \mathbf{W}^Q, \mathbf{h}_i \mathbf{W}^K, \mathbf{h}_i \mathbf{W}^V$$
然后 attention 计算为：$$\begin{aligned}
a_{i j} & \propto \exp \left\{\frac{\mathbf{q}_i \cdot \mathbf{k}_j}{\sqrt{d_k}}+r_{i-j}\right\} \\
\tilde{\mathbf{h}}_i & =\sum_{j=1}^T a_{i j} \mathbf{v}_j
\end{aligned}$$
这里的 $r_{i-j}$ 就是 gated relative position bias，其计算为：$$\begin{aligned}
& g_i^{\text {(update })}, g_i^{(\text {reset) }}=\sigma\left(\mathbf{q}_i \cdot \mathbf{u}\right), \sigma\left(\mathbf{q}_i \cdot \mathbf{w}\right) \\
& \tilde{r}_{i-j}=w g_i^{(\text {reset) }} d_{i-j} \\
& r_{i-j}=d_{i-j}+g_i^{\text {(update })} d_{i-j}+\left(1-g_i^{\text {(update })}\right) \tilde{r}_{i-j}
\end{aligned}$$其中 $d_{i-j}$ 为可学习的 scalar relative position bias，$\mathbf{u},\mathbf{w},w$ 都是可学习的参数

本文的 $d_{i-j}$ 是 bucket relative position embedding，embedding 的参数在所有层中共享，采用 $n=320$ 个embedding：$$d_{|i-j|}= \begin{cases}|i-j|, & |i-j|<\frac{n}{4} \\ \left\lfloor\frac{n}{4}\left(\frac{\log (|i-j|)-\log \left(\frac{n}{4}\right)}{\log (m)-\log \left(\frac{n}{4}\right)}+1\right)\right\rfloor, & \frac{n}{4} \leq|i-j|<m \\ \frac{n}{2}-1, & |i-j| \geq m\end{cases}$$
$m=800$ 表示 maximum offset。

与 wav2vec 2.0 和 HuBERT 中的 卷积相对位置嵌入 相比，gates考虑了内容，并通过调节当前语音内容自适应地调整 relative position bias。直观地说，如果一帧是无声的，而另一帧属于语音片段，那么两帧之间相同的距离偏移往往会起到不同的作用。

### Masked Speech Denoising 和 Prediction

手动生成具有噪声和重叠的语音作为输入，目标是预测**原始语音**（也就是不加噪或者不重叠前的）在masked的区域上的伪标签。

具体来说，从每个 batch 的语音中随机选几条，然后在随机区域将它和随机噪声或者另一条随机语音混合。
> 注：这里的噪声音频和混合音频是从同一个 batch 中的其他语音选的，然后随机裁剪，根据一个随机的能量比进行缩放

通过这一操作，模型开源从噪声或者重叠的语音中识别出主要的说话人，同时预测其说话的内容！

采用 mask prediction loss  优化网络，给定语音 $\mathbf{u}$ 和处理后的（加噪重叠）语音 $\mathbf{u}^{\prime}$，目标是生成伪标签 $\mathbf{z}$，和 HuBERT 一样，采用 k-mean 对 MFCC  或者 latent representation 特征进行聚类作为伪标签，最后的木目标函数为：$$\mathcal{L}=-\sum_{l \in K} \sum_{t \in M} \log p\left(z_t \mid \mathbf{h}_t^L\right)$$
### 预训练数据

先前的模型的数据集都是 audiobook 里的，但是和真实场景相差较大。于是采用两个数据集拓展数据：
+ 10k hours GigaSpeech ，来自 audiobooks 和 YouTube
+ VoxPopuli，多语言无标签数据集，4000K 小时，23 种语言，但是只用了英语的部分，有 24K 小时
总的数据集有 94K 小时，LibriLight, VoxPopuli 和 GigaSpeech.

### 训练稳定性

attention 训练过程种，使用 fp16 训练会出现 overflow，也就是计算 attention score 的时候 $\frac{\mathbf{q}_i \cdot \mathbf{k}_j}{\sqrt{d}}$ 会大于 fp16 的上界。

采用了一个简单的方法来提高其上界，softmax 满足
$$\operatorname{softmax}(\mathbf{x}+\alpha)_k=\operatorname{softmax}(\mathbf{x})_k$$
其中，$\alpha$ 为常数，则计算 score 的时候变为：$$\begin{aligned}
\alpha_{i, j} & \propto \exp \left\{\frac{\mathbf{q}_i \cdot \mathbf{k}_j}{\sqrt{d}}+r_{i-j}\right\} \\
& =\exp \left\{\left(\frac{\mathbf{q}_i}{c \sqrt{d}} \cdot \mathbf{k}_j-\max _{j^{\prime} \leq T}\left(\frac{\mathbf{q}_i}{c \sqrt{d}} \cdot \mathbf{k}_{j^{\prime}}\right)\right) \times c+r_{i-j}\right\} .
\end{aligned}$$
标量参数 $c=32$。

## 实验

### 预训练

WavLM Base 和 WavLM Base+ 有 12 层 Transformer encoder，hidden state 维度 768，attention head 8，94.70M 参数量。

WavLM Large 有 24 层 Transformer encoder，hidden state 维度 1024，attention head 12，316.62M 参数量。

不同层对不同任务的关注度：![](image/Pasted%20image%2020230526215831.png)
颜色越深，表示任务越看重这一层的特征。

分析：
+ bottom layers 对 说话人相关的任务 贡献更大；top layers 对 ASR 相关任务更重要




# Self-Supervised Learning with Random-Projection Quantizer for Speech Recognition - 笔记
> ICML 2023

1. 提出一种用于语音识别的自监督学习方法
2. 采用随机投影（RQ）量化器（Quantizer）生成离散的标签，quantizer 基于一个随机初始化的矩阵来对语音输入进行投影，在随机初始化的 codebook 中进行最近邻查找。
3. 自监督学习期间，矩阵和码本都**不被更新**。随机投影量化器没有经过训练，并且与语音识别模型分离
4. 在LibriSpeech上，非流式模型下，提出的自监督学习实现了与先前工作类似的 WER，流式模型下，提供了比wav2vec 2.0和w2v BERT更低的 WER和延迟。在多语言任务方面，该方法也比wav2vec 2.0和w2v BERT有了显著的改进

## Introduction

1. 语音识别的自监督学习的一个常见设计原则是表征学习，如很多基于 BERT 的方法，但是有一个问题，连续语音和离散的文本token之间有差距，解决方法是学习语音表征或者离散表征
2. 但是将表征学习和自监督学习结合有两个限制：
	1. 模型架构：表征学习可能需要上下文，但是有些下游任务不允许上下文
	2. 复杂性：两者的目标并不总一样，需要找到一种平衡
3. 于是提出 BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ)，对语音信号进行 mask，然后输入到 encoder，基于未被 mask 的部分预测 mask 的部分，而 mask 输出的目标是由随机投影量化器提供的 label。RPQ 将语音信号投影到随机初始化的矩阵，在随机初始化的 codebook 中寻找最近的 vector，其索引就是 label
4. 学习的过程中，投影矩阵和 codebook 都不会被更新
5. 进一步研究了 表征学习 和 自监督学习 的关系，证明两个目标其实内在不一致，从而可以在没有表征学习的情况下设计自监督学习

## 基于 RPQ 的自监督学习

Quantizer 随机初始化投影矩阵和 codebook，使用矩阵来投影输入语音信号，在 codebook 中找到最近的 vector。

训练时不更新矩阵和 codebook。

输入数据归一化为 0，1 的标准正太分布以避免投影后的向量发生 collapse（也就是只对应到很小的一部分的 codebook）。

如图：![](image/Pasted%20image%2020230521104337.png)
训练完之后，使用 encoder 对 ASR 任务进行 fine tune。

mask 的时候，以固定概率对每一帧判断是否 mask（伯努利分布），mask 的值是 0，1 的高斯噪声。

### RPQ

给定输入向量 $x$ 是一个从语音信号计算得到的 $d$ 维向量，RPQ 将 $x$ 映射为离散的 label：$$y=\underset{i}{\operatorname{argmin}} \| \operatorname{norm}_{l 2}\left(c_i\right)-\text { norm }_{l 2}(A x) \|$$
其中，$A$ 为随机初始化的 $h\times d$ 投影矩阵，$C=\{c_1,\dots,c_n\}$ 为随机初始化的 $h$ 维向量，$\operatorname{norm}_{l2}$ 是 $l2$ 归一化，$A$ 采用 Xavier 初始化，codebook $C$ 采用标准正态分布初始化，训练的时候这两个参数固定。

### 预训练

预训练时，在 encoder 顶端添加 softmax 层，学习 label。提出的方法可以适用于任何架构（因为 RPQ 是独立的），实验时采用 Conformer block 。

对于非流式的模型，上下文可知，BERT 直接可以用起来。

对于流式模型，提出两种兼容的方法：
+ 仅基于过去的来预测 mask
+ 对未来的上下文进行 mask（注意这个 mask 和预训练的 mask 是不一样的）

### fine tune

基于下游数据集进行uce有监督 fine tune，在预训练过程中的softmax层丢弃。具体的 ASR 模型采用 RNN transdrs 这类的端到端模型，encoder 部分就是预训练的 encoder + 一层额外的投影层来适应下游任务，decoder 是 LSTM。
> fine tune 时，encoder 是会被更新的。

### RPQ 为什么有效

两个问题：
+ 量化质量
+ 量化器对 SSL 的影响

将 Quantizer 和 VQ-VAE 进行比较（它的 Quantizer 是会被更新的），实验表明，RPQ 的质量其实并不好，但是对 SSL 仍然有效，而且随着数据集的增加差距减小。

因为 SSL-ASR 的目的是训练模型学习上下文信息，而 RPQ 可以保留语音数据的分布，从而使得模型可以学习处理原始信号并推断语音数据中的上下文信息。

## 实验

实验基于 Lingvo 库。

### LibriSpeech

输入语音是 80 维 log-mel filter bank，stride 为 10ms，fine tune 的时候 vocab size 是1024。

![](image/Pasted%20image%2020230521213249.png)
算法在流式和非流式预训练都优于wav2vec 2.0和w2v BERT。

### 多语言任务

![](image/Pasted%20image%2020230521213354.png)
使用所提出的BEST-RQ，平均 WER进一步降低了3%。这证明了一个简单的 RPQ 对于多语言预训练也是有效的。且有了更多的微调数据，BEST-RQ 的表现甚至比w2v BERT更好。
# Representation Mixing for TTS Synthesis 笔记
> Bengio，2019

1. 基于字符和音素的 TTS 很强，但是两者只能选一个
2. 提出 representation mixing 混合表征法，可以在一个 encoder 中组合多种语言信息，也就是可以选其中一个也可以混合使用

## Introduction

1. TTS 是给定语言序列 $l$，来生成音频特征序列 $a$，且两者的长度和维度一般不同，因此需要对齐，本文通过联合学习对齐这两种类型的信息来解决对齐问题

### 数据表征

音频特征采用对数mel谱，语言特征可以是粗粒度如 grapheme（字素，或者说字符），细粒度的包含发音信息的如 phoneme（音素）

### 表征混合的 motivation

某些情况下是需要发音知识的，如文本中存在多音字。

如果没有外部的发音信息，TTS 通常会产生很模糊的输出。所以就需要组合 grapheme 和 phoneme 到一个单一的 encoder 中。

## 表征混合

系统的输入包含数据序列 $l_j$ 和掩码序列 $m$。
其中 $l_j$ 包含 字符序列 $l_c$ 和音素序列 $l_p$ 的混合；mask （0/1）用于给出来自哪个序列。训练的时候，在 word level 随机混合（将所有的空格和标点看成字符）。

例如，字符序列为 "the cat"，音素序列为 "@d@ah @k@ah@t"，这里的@ 用于分隔音素，那么训练的时候可能是：
+ "the @k@ah@t", 对应的 mask 为 $[0,0,0,0,1,1,1]$
+ "@d@ah cat"，对应的 mask 为 $[1,1,0,0,0,0]$
这其实可以看成是一种数据增强，同时可以平滑化字符和音素信息，使其不过度依赖任何一种表征。

### 将 embedding 进行组合

混合序列 $l_j$ 分别通过两个 embedding 矩阵来得到 $e_c,e_p$，然后通过 mask 序列生成混合表征 $e_j$，然后再通过 mask 自己的 embedding $e_m$ 进一步得到 $e_f$，最后作为后面的模型的输入：$$\begin{aligned}
& e_j=(1-m) * e_c+m * e_p \\
& e_f=e_m+e_j
\end{aligned}$$
如图：
![](../语音合成论文笔记/image/Pasted%20image%2020230609225334.png)

### Stacked Multi-scale Residual Convolution（多尺度残差卷积网络）

![](../语音合成论文笔记/image/Pasted%20image%2020230609225551.png)

前面得到的 embedding $e_f$ 输入到 Stacked Multi-scale Residual Convolution（SMRC）子网络中，其包含多个多尺度的卷积层，每个多尺度卷积层的多尺度依次在 channel 维度上concatenate 1×1、3×3和5×5 的kernel。然后再通过残差连接，最后接 BN。

然后将得到的结果通过 BLSTM 层，得到模型的 encoder 部分。

### Noisy Teacher Forcing 的重要性

音频信息通过 带有 dropout 的 pre-net （训练和测试的时候都要）。可以提高模型在生成时候的稳健性。

### 基于 Attention 的 RNN decoder

通过 LSTM network 网络驱动 ，采用 Gaussian mixture 注意力方法，基于上下文和 pre-net 来实现注意力激活。

后面的 LSTM decoder 以prenet激活、注意力激活和之前层的隐藏状态为条件。

最终的 hidden state 通过投影层来匹配音频帧的维度。

### 截断的 BPTT

decoder 采用截断的 BPTT 进行更新，仅处理相关音频序列的一个子序列，同时复用语言序列，直到相关音频序列结束：
![](../语音合成论文笔记/image/Pasted%20image%2020230609230910.png)
其实方法很简单，就是重复的部分可以 batch 计算，其他的部分就单独计算。

最后采用 L-BFGS 从mel谱生成语音。

## 实验

基于 LJSpeech 进行训练，每个 word 选择字符还是音素的概率是 0.5。

研究三个问题：
1. 以 仅基于字符的模型作为 baseline，表征混合（RM）是否有改进
2. RM是否通过在固定PWCB上训练的 baseline 上对未知单词（PWCB）推理进行 character backoff 来改善音素
3. 在用表征混合训练的模型中，PWCB 是否优于基于字符的模型

结果：
![](../语音合成论文笔记/image/Pasted%20image%2020230609231902.png)

+ 混合表征模型的质量更好