# Adapter

核心原理：adapter tuning 策略在原始的网络中添加新的层，训练过程中，原始模型层的参数保持不变，新添加的 adapter  参数随机初始化。

Adapter模块 的两个特点：
+ 参数少
+ 近乎恒等的初始化（为了实现稳定训练）

考虑 Transformer 的标准模型：![](image/Pasted%20image%2020230416094446.png)
Adapter 位于每个 sub layer 的输出，通过残差连接之后的输出直接传递到 layer norm 中。

为了限制参数的大小，还提出了一个 bottleneck 的结果，即 adapter 首先将 FFN 输出的 $d$ 维的特征投影到一个较小的维度 $m$，然后通过一个非线性层，再投影回 $d$ 维。那么包含 bias 的总的模块参数为 $2md+d+m$，如果 $m\ll d$ ，则可以极大地限制参数量，实际用的时候只用了原始模型 $0.5-8\%$ 左右的参数，由于 adapter 内部本身也有一个残差连接，可以很方便地实现恒等初始化（所有的参数都接近 0）。

同时对于每个任务也单独训练了 layer norm 层的参数，类似于 batch norm 的 parameter-efficient，但是效果不太好。

# Prefix Tuning
> 论文 - Prefix-Tuning: Optimizing Continuous Prompts for Generation，斯坦福，2021，[IJCNLP](https://aclanthology.org/venues/ijcnlp/)

1. 提出 prefix-tuning，frozen 预训练模型的参数，优化一个连续的 task-specific 的向量（称为 prefix）
2. prefix-tuning 用于 GPT-2 和 BART ，只学习 0.1% 的数据效果优于 fine tune，且泛化性更好

## Introduction

1. fine tune 很贵很耗时
2. 一种方法是 lightweight fine tune，如 Adapter
3. 像 GPT-3 这种模型不需要 task-specific 的部署，而是需要 prompting（或者说是 in-context learning）
4. 由此提出 prefix tuning：![](image/Pasted%20image%2020230506153115.png)
5. 上面是 Transformer fine tune 的情况，红色的部分表示全部 fine tune，下面是提出 prefix tuning，prefix 是红色部分，把 prefix （或者说是 “virtual tokens”）放在输入的 token 前面作为新的输入，在 fine tune 的时候，只会 fine tune prefix 的这些向量
6. 还有一个优点，就是可以在一个 batch 中同时测试多个不同的任务（相当于每个 instance 接入不同的 prefix）

## 问题描述

考虑一个条件生成任务，输入是上下文 $x$，输出 $y$ 是 token 序列，如图：![](image/Pasted%20image%2020230506153935.png)
两个任务：
+ table to text，$x$ 是数据表，$y$ 是文本描述
+ 摘要：$x$ 是文本，$y$  是摘要

### 自回归语言模型

模型 $p_\phi(y\mid x)$ ，参数为 $\phi$，$z=[x;y]$ 表示两者的拼接，$X_{idx},Y_{idx}$ 表示索引。time step $i$ 的输出记为 $h_{i}\in\mathbb{R}^{d}$，$h_i=\left[h_i^{(1)} ; \cdots ; h_i^{(n)}\right]$ 表示这个 time step 所有层的输出，上标表示 Transformer 的第 $j$ 层。

则自回归LM模型描述为：$$h_i=\mathbf{L M}_\phi\left(z_i, h_{<i}\right)$$
最后一层 Transformer 层的 $h_i$ 用于计算下一个 token 的概率 $p_\phi\left(z_{i+1} \mid h_{\leq i}\right)=\operatorname{softmax}\left(W_\phi h_i^{(n)}\right)$，$W_\phi$ 是预训练的权重矩阵。

## Fine Tune

提出 Prefix tuning 的直觉来自于，合适的上下文（context）可以在不改变 LM 参数的情况下引导 LM，context 可以引导模型从 $x$ 中提取的内容来影响 $x$ 的编码。在离散 token 下的优化可能有帮助，但是计算上有挑战。

于是提出，把指令优化为连续的 word embedding，向上可以传播到所有的 Transformer 层，向后可以传播到接入的 后续 token。

具体方法很简单，在自回归 LM 模型输入的头部追加一部分 $z=[\operatorname{PREFIX} ; x ; y]$，图中的 $P_{idx}$ 代表 prefix 的索引。

具体来说，通过初始化一个可训练的矩阵 $P_{\theta} \in \mathbb{R}^{|P_{idx}|\times \operatorname{dim}(h_i)}$  来存储参数：$$h_i= \begin{cases}P_\theta[i,:], & \text { if } i \in \mathrm{P}_{\mathrm{idx}} \\ \operatorname{LM}_\phi\left(z_i, h_{<i}\right), & \text { otherwise }\end{cases}$$
这个过程中，固定参数 $\phi$，只训练 $\theta$。
> 这里的每一层都有prefix，且每层的 prefix 都不共享，和 prompt tuning 只在输入有 soft prompt 不一样！

直接优化 $P_{\theta}$ 不稳定，且会导致性能下降（对学习率很敏感），所以通过一个小的矩阵 $P_\theta^{\prime}$ 来重参数化：$P_\theta[i,:]=\operatorname{MLP}_\theta\left(P_\theta^{\prime}[i,:]\right)$（两个的特征维度不一样，但是长度一样）。

## 实验和结果

table to text：![](image/Pasted%20image%2020230506161527.png)
在三个数据集上效果都好于其他的方法。

摘要：![](image/Pasted%20image%2020230506161803.png)
效果不如 fine tune，原因可能是：
+ XSUM包含的样本更多，文本也更长
+ 摘要任务更困难更复杂

# P-tuning
> 论文 - GPT Understands, Too，2021，清华

1. 传统 fine tune + GPT 在**自然语言理解NLU**方面效果不太行（prefix tuning 主要针对于 NLG 任务）
2. 通过提出的 P-tuning 方法，可以使 GPT 的性能优于或类似于 BERT
3. 在 few shot 的情况下，P-tuning 优于一些 sota 方法

## Introduction

1. 根据目标函数，预训练模型可以分为三类：
	1. 用于自然语言生成，单向模型，如 GPT
	2. 用于自然语言理解，双向模型，如 BERT
	3. 混合模型，如 UniLM
2. 研究发现，GPT 范式在 fine tune 自然语言理解任务中效果不好
3. 而 GPT-3 的出现表明，单向模型+prompt 可能有助于自然语言理解，但是手动选取 prompt 非常困难，有一些工作关注自动搜索离散 prompt，但是神经网络的本质是连续的，离散 prompt 可能不是最优的
4. **提出 P-tuning，在连续空间中自动搜索参数化的 prompt，然后通过梯度下降法来优化连续的 prompt**
5. 把 P-tuning 用于 GPT 中，在两个 NLU 任务上实验发现，与具有相同规模的BERT模型相比，GPT表现出有竞争力的性能甚至优于 BERT。
6. ALBERT+P-tuning 可以在 SuperGLUE benchmark 上获得最优性能

## Motivation

大模型非常强，但是迁移能力较差，而对下游任务进行 fine tune 又不现实。

于是这些模型采用手工制作的 prompt 来引导下游任务，但是这种 prompt 的性能不可靠，可能一个单词的变化会导致巨大的性能差异：![](image/Pasted%20image%2020230507155443.png)

## P-tuning

给定预训练模型 $\mathcal{M}$ ，离散的 tokens 序列 $\mathbf{x}_{1: n}=\left\{x_0, x_1, \ldots, x_n\right\}$ 被 embedding 层映射为 embedding $\left\{\mathbf{e}\left(x_0\right), \mathbf{e}\left(x_1\right), \ldots, \mathbf{e}\left(x_n\right)\right\}$。在特定情况中，基于上下文 $\mathbf{x}$ ，使用一组目标 tokens $\mathbf{y}$ 的输出 embedding 来进行下游任务的处理。

prompt $\mathbf{p}$ 把上下文 $\mathbf{x}$，目标 $\mathbf{y}$ 和自身合成一个模板 $T$，如，在预测一个国家的首都的任务中，模板可能是 The capital of Britain is “MASK”，其中 The capital of ... is ... . 是prompt，Britain 是 $\mathbf{x}$ ，“mask” 是目标 $\mathbf{y}$ ：![](image/Pasted%20image%2020230507160413.png)

设 $\mathcal{V}$ 代表语言模型的词表，$[P_i]$ 表示模板中的第 $i$ 个prompt。给定模板 $T=\left\{\left[\mathrm{P}_{0: i}\right], \mathbf{x},\left[\mathrm{P}_{i+1: m}\right], \mathbf{y}\right\}$，相比于传统的离散 prompt 要满足 $[P_i]\in\mathcal{V}$ 然后把模板映射为 $\left\{\mathbf{e}\left(\left[\mathrm{P}_{0: i}\right]\right), \mathbf{e}(\mathbf{x}), \mathbf{e}\left(\left[\mathrm{P}_{i+1: m}\right]\right), \mathbf{e}(\mathbf{y})\right\}$，P-tuning 将 $[P_i]$ 视为一个伪 token，此时模板被映射为：$$\left\{h_0, \ldots, h_i, \mathbf{e}(\mathbf{x}), h_{i+1}, \ldots, h_m, \mathbf{e}(\mathbf{y})\right\}$$
其中，$h_i(0\le i < m)$ 是可以训练的 tensor，从而可以通过下游任务的目标函数进行训练，找出最优的连续的 prompt：$$\hat{h}_{0: m}=\underset{h}{\arg \min } \mathcal{L}(\mathcal{M}(\mathbf{x}, \mathbf{y}))$$
想法很直接，但是有两个挑战：
+ 如果 $h$ 随机初始化，很容易陷入局部最小
+ $h_i$ 之间应该是相关而非独立的0

于是通过一个小型的神经网络作为 encoder 来建模 $h_i$（也就是不直接学习$h_i$），即可解决这两个问题，实际中，这个网络选择的是 LSTM+ReLU+两层MLP，即：$$\begin{aligned}
h_i & =\operatorname{MLP}\left(\left[\overrightarrow{h_i}: \overleftarrow{h_i}\right]\right) \\
& =\operatorname{MLP}\left(\left[\operatorname{LSTM}\left(h_{0: i}\right): \operatorname{LSTM}\left(h_{i: m}\right)\right]\right)
\end{aligned}$$
在推理的时候，可以丢弃 LSTM，只要最终的 $h$。

## 结果
![](image/Pasted%20image%2020230507161832.png)

# Prompt Tuning
> 论文 - The Power of Scale for Parameter-Efficient Prompt Tuning，Google，2021 EMNLP

1. 提出 prompt tuning，基于 frozen 的语言模型来学习 soft prompt 以用于下游任务
2. soft prompt 是通过反向传播学习得到的，而不是人为输入的
3. 发现，模型越大，和 full fine tune 的差距越小
4. **可以看成是 prefix tuning 的一种简化**
5. soft prompt 的鲁棒性很强

## Introduction

1. 研究表明，prompt design 非常有效，但是有一些缺点，如需要人为设计、受输入条件的限制等
2. 提出 prompt tuning，freeze 预训练模型，对每个任务，允许把 k 个额外的 token 放在输入文本之前，这些 soft prompt 是可以端到端训练的，可以缩小和 fine tune 之间的差距：![](image/Pasted%20image%2020230506174616.png)
3. 具体如图：![](image/Pasted%20image%2020230506174730.png)
4. 本文的主要贡献包括：
	1. 提出 prompt tuning，在 LM 中可以和 fine tune 相竞争
	2. 模型越大，性能和鲁棒性的提升越明显
	3. 在域迁移问题中，prompt tuning 效果好于 fine tune
	4. 提出 prompt ensembling，证明其有效性

## Prompt Tuning

将所有的任务都看成是  “Text to Text” 任务，T5 模型为 $\operatorname{Pr}_\theta(Y \mid X)$ ，那么 prompt 是指，在生成 $Y$ 的过程中，额外添加的信息（条件）。通过添加一系列的 tokens 序列 $P$，使得概率 $\operatorname{Pr}_\theta(Y \mid[P ; X])$ 尽可能地大，这个过程中并不会修改模型的参数。而且这个 prompt 通常是手动给的，并且是模型 embedding 的一部分，同时受参数 $\theta$ 的控制。

prompt tuning 移除这个限制，也就是说，prompt 有其自己的参数 $\theta_P$ ，且可以被更新，此时的模型生成变成 $\operatorname{Pr}_{\theta ; \theta_P}(Y \mid[P ; X])$ 能够通过最大 $Y$ 的似然进行反向传播，并且只在 $\theta_P$ 上计算梯度。

给定 $n$ 个 tokens $\{x_1,x_2,\cdots,x_n\}$ ，T5 模型首先获得这些 tokens 的 embedding，从而形成一个矩阵 $X_e\in\mathbb{R}^{n\times e}$ ，$e$ 为 embedding 的维度。提出的 soft prompt 则表示为 $P_e\in\mathbb{R}^{p\times e}$，$p$ 为 prompt 的长度，然后模型的输入为 $[P_e;X_e]\in\mathbb{R}^{(p+n)\times e}$，这个过程中只更新 $P_e$ 的参数。

需要考虑两个点：
+ 参数的初始化：可以随机，也可以从 一些特定任务的 embedding 中选
+ prompt 的长度

主要在 T5 模型中测试的。

> 其实 prompt tuning 和 prefix tuning 的思路几乎一样，区别在于，prompt tuning 不会在 Transformer 的每一层都有一个 prefix，而只是在输入层有一个 prompt，而 prefix tuning 对于每一层的 Transformer layer 都不一样。

# LoRA
> 论文 - LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS，微软，2022

1. fine tune 大模型非常困难，提出 LoRA，freeze 预训练模型的权重，在每个 Transformer 结构加入 可训练的 rank decomposition matrices，减少下游任务训练所需的参数量
2. 拿 GPT-175B 模型做对比，参数可以减少 10000 倍，并且相比于 adapter 没有额外的推理延迟

## Introduction

1. 背景：fine tune 预训练模型非常困难
2. 有些研究通过 fine tune 部分参数或者学习新的模块来缓解，但是现有的方法都会引入 推理延迟（inference latency），而且效果可能不如 fine tune 的 baseline，导致存在效率和性能的权衡
3. **模型在学习过程中，权重的调整是“低秩”的**，于是提出 Low-Rank Adaptation（LoRA）方法，通过优化自适应过程中 dense layer 变化的秩分解矩阵来间接训练神经网络中的一些 dense layer，同时保持预训练模型的权重冻结：![](image/Pasted%20image%2020230426221503.png)以GPT-3 为例，即使图中的 $d=12288$，$r$ 等于 1 或者 2 都足够了。
4. LoRA 的优点有：
	1. 预训练模型可以共享，可以为不同的任务构建不同的 LoRA 模块
	2. 实现高效训练
	3. 采用线性设计，在部署的时候可以合并训练的矩阵和原始的冻结权重，不引入推理延迟
	4. 能够和其他的方法组合

记 Transformer 层的输入输出维度为 $d_{model}$，用 $W_q,W_k,W_v,W_o$ 分别表示注意力机制中的 query、key、value 和 输出投影矩阵，用 $W$ 或者 $W_0$ 表示预训练的权重矩阵，$\Delta W$ 表示累积的梯度，$r$ 表示 LoRA 模块的秩，采用 Adam 优化器，$d_{ffn}=4\times d_{model}$。

## 语言模型存在问题

设有一个自回归的预训练语言模型 $P_\Phi(y\mid x)$，参数为 $\Phi$，如 GPT 模型。

如果要把这个模型自适应到下游任务中，每个任务所用的训练集为 $\mathcal{Z}=\left\{\left(x_i, y_i\right)\right\}_{i=1, \ldots, N}$，其中 $x_i,y_i$ 都是 tokens 序列， 在 full fine tune 的情况下，模型初始化为预训练的权重 $\Phi_0$，更新为 $\Phi_0+\Delta \phi$，通过最大化以下目标函数：$$\max _{\Phi} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left(P_{\Phi}\left(y_t \mid x, y_{<t}\right)\right)$$

这种 full fine tune 的缺点是，对于每个下游任务，都要重新学习一个不同的参数集，其参数 $\Delta\Phi$ 的维度和 $\Phi_0$ 相同，如果预训练模型很大，那这个参数就会很大。

本文采用了一个参数高效的方法，将参数增量 $\Delta\Phi$ 通过一个更小的参数集进行编码 $\Delta\Phi=\Delta\Phi(\Theta)$，且 $\Theta$ 的维度远小于 $\Phi_0$。此时优化问题变成：$$\max _{\Theta} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left(p_{\Phi_0+\Delta \Phi(\Theta)}\left(y_t \mid x, y_{<t}\right)\right)$$
当模型采用 GPT-3 175B 时，参数量能够变为原来的 $0.01\%$。

##  方法

现有的方法缺点：
+ Adapter 会引入延迟
+ 直接优化 prompt 很难做

### 低秩参数更新

研究表明，预训练的语言模型具有较低的 "instrisic dimension"，于是假设更新的权重也具有较低的秩，对于预训练的权重矩阵 $W\in\mathbb{R}^{d\times k}$ ，当更新参数的时候，通过添加一个低秩的分解来替代 $\Delta W$：$W_0+\Delta W=W_0+B A$，其中 $B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}, r \ll \min (d, k)$， 训练的时候，$W_0$ 保持冻结，$A,B$  包含可训练的参数，在前向传播的时候，两个分量都和输入相乘：$$h=W_0 x+\Delta W x=W_0 x+B A x$$
$A$ 采用随机高斯初始化，$B$ 初始化为 0。

**在部署的时候，直接用 $W=W_0+BA$ 替换原来的权重，此时计算的时候维度不变因此不会引入推理延迟。**

**而且通过增加秩 $r$ ，模型性能会收敛到 full fine tune 的性能。**

# Towards a Unified View of Parameter-Efficient Transfer Learning
> CMU，2022，ICLR

1. 分解了一些 SOTA 的参数高效迁移学习方法，提出了一个统一的框架以建立其之间的联系
2. 将这些方法看成是对预训练模型中的特定的 **隐藏状态** 的修改
3. 框架能够在不同的方法之间传递设计方法，从而可以实现新的方法

## Introduction

1. 已经提出了很多的参数高效 fine tune 方法，可以在没有灾难性遗忘的情况下快速适应新任务，并且在 OOD 中鲁棒性很强
2. 但是对这些方法成功的原理知之甚少，之间的联系也不清楚，本文回答一下三个问题：
	1. 方法之间的联系是什么
	2. 方法之间是否存在一些共同的设计要点，这些要点是什么
	3. 方法之间的设计要点是否可以迁移到其他的方法中实现更好的变体
3. 不过实验首先表明，现有的参数高效微调方法在更高资源和具有挑战性的任务上仍然落后于完全微调

## 预备知识

Transformer 一共包含 $L$ 层，其中的一层如图：![](image/Pasted%20image%2020230509153557.png)
传统的 attention 计算如下：$$\operatorname{Attn}(\boldsymbol{Q}, \boldsymbol{K}, \boldsymbol{V})=\operatorname{softmax}\left(\frac{\boldsymbol{Q} \boldsymbol{K}^T}{\sqrt{d_k}}\right) \boldsymbol{V}$$
其中，$\boldsymbol{Q} \in \mathbb{R}^{n \times d_k},\boldsymbol{K} \in \mathbb{R}^{m \times d_k},\boldsymbol{V} \in \mathbb{R}^{m \times d_v}$，MHA 在 $N_h$ 个 head 上并行计算 attention，每个 head 都被权重矩阵 $\boldsymbol{W}_q^{(i)}, \boldsymbol{W}_k^{(i)}, \boldsymbol{W}_v^{(i)} \in \mathbb{R}^{d \times d_h}$ 投影到对于的 QKV 中，给定 $m$ 个 vector 的序列 $\boldsymbol{C} \in \mathbb{R}^{m \times d}$ 和 query 向量 $\boldsymbol{x} \in \mathbb{R}^{d}$，MHA 计算如下：$$\operatorname{MHA}(\boldsymbol{C}, \boldsymbol{x})=\operatorname{Concat}\left(\operatorname{head}_1, \cdots, \operatorname{head}_{\mathrm{h}}\right) \boldsymbol{W}_{{o}}, \operatorname{head}_{\mathrm{i}}=\operatorname{Attn}\left(\boldsymbol{x} \boldsymbol{W}_q^{(i)}, \boldsymbol{C} \boldsymbol{W}_k^{(i)}, \boldsymbol{C} \boldsymbol{W}_v^{(i)}\right)$$
其中，$\boldsymbol{W}_{{o}}\in\mathbb{R}^{d\times d}$，$d$ 为模型的维度，$d_h=d/H_h$。

而针对于全连接层，计算如下：$$\operatorname{FFN}(\boldsymbol{x})=\operatorname{ReLU}\left(\boldsymbol{x} \boldsymbol{W}_1+\boldsymbol{b}_1\right) \boldsymbol{W}_2+\boldsymbol{b}_2$$
其中，$\boldsymbol{W}_1\in\mathbb{R}^{d\times d_m},\boldsymbol{W}_2\in\mathbb{R}^{d_m\times d}$，通常，在 Transformer 中，$d_m=4d$。

从数学角度看一些现有的 PEFT 方法：

Adapter 通常采用下投影矩阵 $\boldsymbol{W}_{down}\in\mathbb{R}^{d\times r}$ 将输入 $\boldsymbol{h}$ 投影到 bottleneck 维度 $r$，然后接一个非线性激活，然后是上投影 $\boldsymbol{W}_{up}\in\mathbb{R}^{r\times d}$ ，最后还有一个残差连接：$$\boldsymbol{h} \leftarrow \boldsymbol{h}+f\left(\boldsymbol{h} \boldsymbol{W}_{\text {down }}\right) \boldsymbol{W}_{\text {up }}$$
Prefix Tuning 在 MHA 的 keys 和 values 之前放置 $l$ 个 prefix vector 组成新的  keys 和 values ，此时 attention head 的计算公式变为：$$\operatorname{head}_i=\operatorname{Attn}\left(\boldsymbol{x} \boldsymbol{W}_q^{(i)}, \operatorname{concat}\left(\boldsymbol{P}_k^{(i)}, \boldsymbol{C} \boldsymbol{W}_k^{(i)}\right), \operatorname{concat}\left(\boldsymbol{P}_v^{(i)}, \boldsymbol{C} \boldsymbol{W}_v^{(i)}\right)\right)$$

Prompt Tuning 简化了 Prefix Tuning，只在输入的 word embedding 上添加，类似于之前的 P-tuning。

LoRA 在 Transformer 层中添加一个可以训练的低秩矩阵。对于预训练的权重矩阵 $\boldsymbol{W}\in\mathbb{R}^{d\times k}$ ，采用低秩分解进行权重更新 $\boldsymbol{W}+\Delta W=\boldsymbol{W}+\boldsymbol{W}_{\text {down }} \boldsymbol{W}_{\text {up }}$，其中 $\boldsymbol{W}_{\text {down }} \in \mathbb{R}^{d \times r}, \boldsymbol{W}_{\text {up }} \in \mathbb{R}^{r \times k}$ 是可训练的参数，LoRA 把这种更新应用到 query 和 value 的投影矩阵 $\boldsymbol{W}_q, \boldsymbol{W}_v$ 上，然后更新如下：$$\boldsymbol{h} \leftarrow \boldsymbol{h}+s \cdot \boldsymbol{x} \boldsymbol{W}_{\text {down }} \boldsymbol{W}_{\text {up }}$$
其中，$s\ge1$ 为可训练的缩放因子。

## 统一视角

首先推到了一种 prefix tuning 的等价形式，使其和 adapter 建立联系，然后提出了一种统一的框架。

### Prefix Tuning

Prefix Tuning 中，通过在原始的 attention 的 key 和 value 的前面添加 $l$ 个可学习的向量来修改注意力模块：$$\begin{aligned}
& \text { head }=\operatorname{Attn}\left(\boldsymbol{x} \boldsymbol{W}_q, \operatorname{concat}\left(\boldsymbol{P}_k, \boldsymbol{C} \boldsymbol{W}_k\right), \operatorname{concat}\left(\boldsymbol{P}_v, \boldsymbol{C} \boldsymbol{W}_v\right)\right) \\
& =\operatorname{softmax}\left(\boldsymbol{x} \boldsymbol{W}_q \operatorname{concat}\left(\boldsymbol{P}_k, \boldsymbol{C} \boldsymbol{W}_k\right)^{\top}\right)\left[\begin{array}{c}
\boldsymbol{P}_v \\
\boldsymbol{C} \boldsymbol{W}_v
\end{array}\right] \\
& =(1-\lambda(\boldsymbol{x})) \operatorname{softmax}\left(\boldsymbol{x} \boldsymbol{W}_q \boldsymbol{W}_k^{\top} \boldsymbol{C}^{\top}\right) \boldsymbol{C} \boldsymbol{W}_v+\lambda(\boldsymbol{x}) \operatorname{softmax}\left(x \boldsymbol{W}_q \boldsymbol{P}_k^{\top}\right) \boldsymbol{P}_v \\
& =(1-\lambda(\boldsymbol{x})) \underbrace{\operatorname{Attn}\left(\boldsymbol{x} \boldsymbol{W}_q, \boldsymbol{C} \boldsymbol{W}_k, \boldsymbol{C} \boldsymbol{W}_v\right)}_{\text {standard attention }}+\lambda(\boldsymbol{x}) \underbrace{\operatorname{Attn}\left(\boldsymbol{x} \boldsymbol{W}_q, \boldsymbol{P}_k, \boldsymbol{P}_v\right)}_{\text {independent of } \boldsymbol{C}},
\end{aligned}$$
其中，$\lambda(\boldsymbol{x})=\frac{\sum_i \exp \left(\boldsymbol{x} \boldsymbol{W}_q \boldsymbol{P}_k^{\top}\right)_i}{\sum_i \exp \left(\boldsymbol{x} \boldsymbol{W}_q \boldsymbol{P}_k^{\top}\right)_i+\sum_j \exp \left(\boldsymbol{x} \boldsymbol{W}_q \boldsymbol{W}_k^{\top} \boldsymbol{C}^{\top}\right)_j}$ 是标量缩放因子，表示在 prefix 上归一化的注意力权重。

上式中，第一项和 prefix 无关，第二项仅和 prefix 有关，进一步写成：$$\boldsymbol{h} \leftarrow(1-\lambda(\boldsymbol{x})) \boldsymbol{h}+\lambda(\boldsymbol{x}) \Delta \boldsymbol{h}, \quad \Delta \boldsymbol{h}:=\operatorname{softmax}\left(\boldsymbol{x} \boldsymbol{W}_q \boldsymbol{P}_k^{\top}\right) \boldsymbol{P}_v$$
定义 $\boldsymbol{W}_1=\boldsymbol{W}_q \boldsymbol{P}_k^{\top}, \boldsymbol{W}_2=\boldsymbol{P}_v, f=\text { softmax }$，则：$$\boldsymbol{h} \leftarrow(1-\lambda(\boldsymbol{x})) \boldsymbol{h}+\lambda(\boldsymbol{x}) f\left(\boldsymbol{x} \boldsymbol{W}_1\right) \boldsymbol{W}_2$$
这个公式和 adapter 非常相似，区别在于 prefix tuning 是做了加权的。

### 统一框架

所以的方法都可以看成是对改变量 $\Delta\boldsymbol{h}$ 的一种学习，记要被修改的原始的隐表征为 $\boldsymbol{h}$ ，计算 $\boldsymbol{h}$ 对应的子模块的输入记为 $\boldsymbol{x}$（例如，两者可以分别为 attention的输出和输入），然后三个方法就可以统一写为：![](image/Pasted%20image%2020230509214245.png)
> 这里的 head attention 表示 multi-head attention 中的每个 head 都有一个 attention 的操作，表明每个 head 的输出表征都会更新，而且每个 head 都有 prefix

表中，Functional Form 表示计算 $\Delta\boldsymbol{h}$ 的函数形式。Modified Representation 表明修改的是模型的哪一部分。Insertion Form 表明模型插入模型的方式，如图：![](image/Pasted%20image%2020230509214730.png)
+ adapter 是顺序方式插入，输入输出都是 $\boldsymbol{h}$
+ prefix tuning 和 LoRA 相当于是并行插入，其输入是 $\boldsymbol{x}$。
最后，Composition Function 表明修改后的 $\Delta\boldsymbol{h}$ 和 原始的 $\boldsymbol{h}$ 是如何组合的：
+ adapter 是执行简单的 additive
+ prefix tuning 用的是 gated additive
+ LoRA 通过一个常数因子缩放变化量之后加到原来的表征中

其实，prompt tuning 类似于 prefix tuning，但是prompt 只修改第一层中的 head attention，而对于不同的 adapter 的变体则类似于 adapter 中的表示方法。

通过这样一种统一的表征，我们可以沿着这些设计维度确定哪些是关键设计因子，从而在不同的方法中传递不同的因子。

### 变体

例如：
+ Parallel Adapter 通过将 prefix tuning 的并行思想引入到 Adapter 中，神奇的是已经有研究是关于这个了
+ Multi-head Parallel Adapter 又进一步，像 prefix tuning 一样，采用 parallel adapters 修改 head attention 的输出
+ Scaled Parallel Adapter 将LoRA 中的 composition 和 insertion form 引入 Adapter

## 实验和结果

四个任务：
+ 摘要
+ 翻译
+ 推理
+ 情感分类
结果：![](image/Pasted%20image%2020230509221020.png)
不同 PEFT 方法的对比，左边图是摘要和翻译任务，右边是推理和分类任务。

不同插入格式的对比：![](image/Pasted%20image%2020230509221255.png)
并行效果优于顺序。

修改不同表征：![](image/Pasted%20image%2020230509221509.png)
修改 FFN 层效果优于修改 attention 模块。

不同变体的结果对比：![](image/Pasted%20image%2020230509221801.png)
MAM Adapter 效果最好。

这里的 MAM Adapter 是作者提出的缝合怪，采用 Scaled parallel adapter 修改 FFN，同时像 prefix tuning 一样修改 head attention，得到所谓的  Mix-And-Match adapter (MAM Adapter)。

# LLaMA-Adapter- Efficient Fine-tuning of Language Models with Zero-init Attention

> CUHK MMLab，2023

1. 提出 LLaMA-Adapter，一种轻量级的 adaptation 方法，可以高效地利用指令来微调模型
2. 采用 52K self-instruct demonstrations，LLaMA-Adapter 只有 1.2M 的可学习的参数（相比于 LLaMA 7B 模型），在 8台 A100 GPU 上只需要训练一个小时
3. 具体来说，采用可学习的 adaption prompts 集合，放在 高层的 transformer layers 的文本输入 tokens 的前面，然后提出 zero-init attention 机制，可以自适应地将新的知识引入 LLaMA，同时还可以保留其预训练的知识
4. 最终可以实现和 full fine tune 的 Alpaca 7B 性能相当，同时方法还可以拓展到多模态输入

## Introduction

1. 最近的 instruction-following models 模型很成功，如 ChatGPT
2. Stanford Alpaca 提出了 LLaMA 模型使其更容易复现，Alpaca 利用GPT-3.5以 self-instruct 方式，对 LLaMA 中的整个 7B 参数进行 fine tune，得到一个性能类似于 GPT-3.5 的模型
3. 但是 full fine tune 大规模 LLaMA 仍然耗时、计算量大、不支持多模态且难以转移到不同的下游场景，于是提出 LLaMA-Adapter
4. 采用 52K instruction-output 数据用于训练，但是资源效率优于 Alpaca
5. 提出的 LLaMA-Adapter 有以下四个特点：![](image/Pasted%20image%2020230510151118.png)
	1. 只有 1.2M 的可训练参数，但是性能和 7B Alpaca 差不多
	2. 一小时的 fine tune。在8个A100 GPU上 收敛不到一个小时。
	3. 灵活且即插即用。对于不同的场景可以训练不同的 adapter
	4. 支持多模态。可以拓展到图像输入用于多模态推理


## LLaMA-Adapter

给定数据集包含 52K 的 instruction-to-output data 和一个预训练好的 $N$ 层 Transformer 的 LLaMA 模型， 引入可学习的 adaption prompts 集合用于指令微调。

$L$ 层的 prompts 的集合记为 $\{P\}^L_{l=1}$ ，其中 $P_l\in\mathbb{R}^{K\times C}$ 表示每层包含 $K$ 个 prompt，$C$ 为模型的特征维度。注意这里的 $L\le N$ 表示只有 top $L$ 层才有 prompt，从而只 fine tune 那些具有高阶语义的层。

以第 $l$ 层为例，设输入的原本的 word token 长为 $M$，记为 $T_l\in\mathbb{R}^{M\times C}$，然后把 adaptation prompt 和输入 tokens 进行拼接得到：$$[P_l;T_l]\in\mathbb{R}^{(K+M)\times C}$$
从而，$P_l$ 学习的指令知识可以引导 $T_l$ 生成上下文响应。

### Zero-init Attention

如果 adaptation prompts 是随机初始化的，开始训练的时候可能干扰 word tokens，从而损害fine tune的性能和稳定性。于是将最后 $L$ 层的 Transformer 的注意力机制修改为 zero-init 注意力，如下图：![](image/Pasted%20image%2020230511150500.png)

假设第 $l$ 层，模型正在基于 $[P_l;T_l]$ 生成第 $(M+1)$ 个单词，其对应的 embedding 记为 $t_l\in\mathbb{R}^{1\times C}$，计算 attention 为，首先获得query, key, value：$$\begin{aligned}
Q_l & =\operatorname{Linear}_{\mathrm{q}}\left(t_l\right) \\
K_l & =\operatorname{Linear}_{\mathrm{k}}\left(\left[P_l ; T_l ; t_l\right]\right) \\
V_l & =\operatorname{Linear}_{\mathrm{v}}\left(\left[P_l ; T_l ; t_l\right]\right)
\end{aligned}$$
然后计算 attention score（归一化之前）：$$S_l=Q_l K_l^T / \sqrt{C} \in \mathbb{R}^{1 \times(K+M+1)}$$
这个 score 是一个向量，表示 $t_l$ 和 $K+M+1$ 个 tokens 之间的相似度，同时 $S_l$ 可以写成以下两部分：$$S_l=[S_l^K;S_l^{M+1}]^T$$
其中，$S_l^K\in\mathbb{R}^{K\times 1},S_l^{M+1}\in\mathbb{R}^{(M+1)\times 1}$ 分别表示 $K$ 个 adaptation promps 和 $M+1$ 个 word tokens 的 score，前者表示prompts 对 $t_l$ 的贡献，这是在训练初期导致干扰的部分。

于是采取一个可学习的 gating factor 记为 $g_l$，用来自适应的控制 $S_l^K$ 的重要程度。$g_l$ 初始化为 0，从而可以消除欠拟合的问题，然后训练的时候逐步增加，最后的归一化的 score 计算为：$$S_l^g=\left[\operatorname{Softmax}\left(S_l^K\right) \cdot g_l ; \quad \operatorname{Softmax}\left(S_l^{M+1}\right)\right]^T$$
> 其实就是，开始训练的时候，$g_l$ 很小，此时可以认为 adaptation prompt 不起作用，随着训练的进行增加 $g_l$ 再逐步引入。

最后考虑 value 计算最终的输出：$$t_l^o=\operatorname{Linear}_{\mathrm{o}}\left(S_l^g V_l\right) \in \mathbb{R}^{1 \times C}$$

### 多模态

![](image/Pasted%20image%2020230511150847.png)
LLaMA-Adapter 可以很轻松用在多模态：
+ 首先利用预先训练的视觉编码器，例如 CLIP，提取其多尺度全局特征，记为 $\{I_m\}_{m=1}^M$，$M$ 为缩放数量，$I_m\in\mathbb{R}^{1\times C_m}$，然后把这 $M$ 个特征拼接起来，再通过一个 投影层：$I_p=\operatorname{Projection}\left(\operatorname{Concat}\left(\left\{I_m\right\}_{m=1}^M\right)\right)$，得到 $I_p\in\mathbb{R}^{1\times C}$
+ 重复 $I_p$ 向量 $K$ 次，element-wise 地加到长为 $K$ 的 adaptation prompts 上，对于第 $l$ 层，得到的多模态 prompt 记为：$P_l^v=P_l+\operatorname{Repeat}\left(I_p\right) \in \mathbb{R}^{K \times C}$
+ 最后当成正常的 LM 模型 fine tune 即可

也可以简单地拓展到音视频。

## 实验和结果

使用 Stanford Alphaca 的 Alphaca-52K 指令数据进行训练，每条数据包含：
+ instruction：任务描述
+ input：上下文
+ output：GPT3.5 的回答

8 个 A100 GPU 训练 5个 epoch，基于 7B 的 LLaMA 模型，top-p 采样。

仅通过微调120万个参数，产生的回答与完全微调的 Alpaca 和大规模 GPT-3 相当：
![](image/Pasted%20image%2020230511171906.png)
