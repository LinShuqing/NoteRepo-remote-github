> 2020 年

1. 在 NLP 中，如果要在特定任务下实现较好的性能，需要对特定于任务的数据集进行微调
2. 而人类只需要几个例子或者简单的指令就可以完成一项新的任务
3. 本文表明，拓展语言模型可以大大提高未知任务、zero shot 下的性能
4. 提出 GPT-3， 175B 的参数量，并且在 few shot 的情况下测试性能
5. 在所有的任务中都不进行 fine tune，而仅仅通过文本交互实现指定的任务

## Introduction

1. 基于 Transformer 的 LM 在很多 NLP 任务中都取得了实质性的进展，但是其中的问题就是，需要任务特定的数据集和 fine tune
2. 很有必要提出方法来解决这种限制，因为为每个任务都提供一个有标签的数据集来 fine tune 限制了 LM 的通用性，且有些任务甚至无法收集对应的数据集；并且有研究发现，在预训练+ fine tune 这种范式下，模型越大，对分布的拟合不一定会越强
3. 人类学习语言相关的任务不需要很多任务相关的数据集，而仅仅需要一个引导性的指令或者演示几次就可以
4. 解决这些问题的一个潜在方法是元学习（自己定义的。。。），GPT-2 使用 in-context learning（语境学习）来实现，用文本输入来表示特定的任务 ，但是实际上其效果不如 fine tune
6. 语言模型另一个趋势是，加大模型参数，且随着规模的增加，in-context 学习能力也在增加
7. 本文提出训练 GPT-3，一共 175B 个参数，在 20 多个数据集上进行评估，测试三种条件：few shot、one shot 和 zero shot，在其中一个任务中的结果如图：![[Pasted image 20230302192953.png]]
8. 最终结果是，基本上在 zero shot 和 one shot 上都可以实现不错的性能，在 few shot 中甚至偶尔可以超过 经过 fine tune 的 SOTA
9. 但是对于自然语言推理这类的问题，GPT-3 的效果也不怎么样

## 方法
![[Pasted image 20230302195154.png]]

预训练方法、模型架构等的使用和 GPT-2 类似，但是探索了不同的 in context learning 的方法，区别如上图。

### fine tune
在特定任务的数据集上进行有监督的训练来更新模型的权重。

优点在于，fine tune 之后效果会很好，缺点在于对于每个任务可能都需要一个大的 fine tune数据集。

GPT-3 没有进行 fine tune，但是原则上可以进行。

### few shot
模型在推理的时候，给一些和任务相关的演示作为条件，但是模型不能更新权重。通常会给出 $k$ 个示例，设置 $k$ 的范围为10 到 100。

few shot 的优点是大大减少了特定任务下的数据需求，缺点是相比于最好的 fine tune 的 SOTA 性能还是要差很多。

### one shot
few shot 中 $k=1$ 的情况，有点类似于人类的学习方式（所谓举一反三）。

### zero shot
没有任何的演示输入，输入只有描述任务的指令。是所有方法中最难的，但是也是最方便的。

### 模型和架构
![[Pasted image 20230302201411.png]]
使用和 GPT-2 完全相同的架构，除了在 Transformer 层中使用了 alternating dense and locally banded sparse attention。

一共训练了 8 个模型，最大的称为 GPT-3。

### 数据集
采用近万亿单词的 Common Crawl 数据集，同时采用三个步骤来提高数据的质量。在多个数据集的混合中进行采样，最终过滤后的数据大小一共 570GB：![[Pasted image 20230302214118.png]]
