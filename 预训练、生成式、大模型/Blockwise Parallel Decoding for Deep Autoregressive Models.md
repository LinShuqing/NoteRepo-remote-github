> NIPS 2018，UC Berkeley，Google Brain
<!-- 翻译 & 理解 -->
<!-- Deep autoregressive sequence-to-sequence models have demonstrated impressive performance across a wide variety of tasks in recent years. While common archi- tecture classes such as recurrent, convolutional, and self-attention networks make different trade-offs between the amount of computation needed per layer and the length of the critical path at training time, generation still remains an inherently sequential process. To overcome this limitation, we propose a novel blockwise parallel decoding scheme in which we make predictions for multiple time steps in parallel then back off to the longest prefix validated by a scoring model. This allows for substantial theoretical improvements in generation speed when applied to architectures that can process output sequences in parallel. We verify our approach empirically through a series of experiments using state-of-the-art self-attention models for machine translation and image super-resolution, achieving iteration reductions of up to 2x over a baseline greedy decoder with no loss in quality, or up to 7x in exchange for a slight decrease in performance. In terms of wall-clock time, our fastest models exhibit real-time speedups of up to 4x over standard greedy decoding. -->
1. 自回归的 sequence-to-sequence 模型的生成过程是 sequential 的
2. 提出 blockwise parallel decoding，并行预测多个 time steps，然后通过 score 模型进行验证，回退到最长的 prefix，从而提高生成速度
3. 在 state-of-the-art self-attention models 上进行实验，相比于 baseline greedy decoder，迭代次数减少 2 倍，最多 7 倍

## Introduction
<!-- Neural autoregressive sequence-to-sequence models have become the de facto standard for a wide variety of tasks, including machine translation, summarization, and speech synthesis (Vaswani et al., 2017; Rush et al., 2015; van den Oord et al., 2016). One common feature among recent architectures such as the Transformer and convolutional sequence-to-sequence models is an increased capacity for parallel computation, making them a better fit for today’s massively parallel hardware accelerators (Vaswani et al., 2017; Gehring et al., 2017). While advances in this direction have allowed for significantly faster training, outputs are still generated one token at a time during inference, posing a substantial challenge for many practical applications (Oord et al., 2017). -->
1. 自回归的 sequence-to-sequence 模型在推理时一个 token 一个 token 生成，很难用于实际应用
<!-- In light of this limitation, a growing body of work is concerned with different approaches to accel- erating generation for autoregressive models. Some general-purpose methods include probability density distillation (Oord et al., 2017), subscaling (Kalchbrenner et al., 2018), and decomposing the problem into the autoregressive generation of a short sequence of discrete latent variables followed by a parallel generation step conditioned on the discrete latents (Kaiser et al., 2018). Other techniques are more application-specific, such as the non-autoregressive Transformer for machine translation (Gu et al., 2018). While speedups of multiple orders of magnitude have been achieved on tasks with high output locality like speech synthesis, to the best of our knowledge, published improvements in machine translation either show much more modest speedups or come at a significant cost in quality. -->
2. 一些方法用于加速自回归模型的生成，如概率密度蒸馏、子缩放、分解问题为短序列的自回归生成离散潜变量，然后并行生成
<!-- In this work, we propose a simple algorithmic technique that exploits the ability of some architectures, such as the Transformer (Vaswani et al., 2017), to score all output positions in parallel. We train variants of the autoregressive model to make predictions for multiple future positions beyond the next position modeled by the base model. At test time, we employ these proposal models to independently and in parallel make predictions for the next several positions. We then determine the longest prefix of these predictions that would have generated under greedy decoding by scoring each position in parallel using the base model. If the length of this prefix is greater than one, we are able to skip one or more iterations of the greedy decoding loop. -->
3. 本文提出利用 Transformer 的并行评分能力，训练自回归模型的变体，预测多个未来位置；测试阶段，使用这些 proposal 模型并行预测下几个位置，然后通过基础模型并行评分，确定这些预测的最长前缀，从而跳过贪婪解码循环的一次或多次迭代；
<!-- In our experiments, our technique approximately doubles generation speed at no loss in quality relative to greedy decoding from an autoregressive model. Together with knowledge distillation and approximate decoding strategies, we can increase the speedup in terms of decoding iterations to up to five-fold at a modest sacrifice in quality for machine translation and seven-fold for image super-resolution. These correspond to wall-clock speedups of three-fold and four-fold, respectively. -->
4. 相比于自回归模型的贪婪解码，可以将生成速度提高一倍，同时保持质量不变
<!-- In contrast to the other previously mentioned techniques for improving generation speed, our approach can furthermore be implemented on top of existing models with minimal modifications. Our code is publicly available in the open-source Tensor2Tensor library (Vaswani et al., 2018). -->
<!-- Greedy Decoding -->
## Greedy Decoding
<!-- In a sequence-to-sequence problem, we are given an input sequence x = (x1 , . . . , xn ), and we would like to predict the corresponding output sequence y = (y1, . . . , ym). These sequences might be source and target sentences in the case of machine translation, or low-resolution and high-resolution images in the case of image super-resolution. One common approach to this problem is to learn an autoregressive scoring model p(y | x) that decomposes according to the left-to-right factorization -->
在 sequence-to-sequence 问题中，给定输入序列 $x = (x_1, \ldots, x_n)$，预测对应的输出序列 $y = (y_1, \ldots, y_m)$。学习自回归 scoring model $p(y | x)$ 进行从左到右合成：
$$\log p(y\mid x)=\sum_{j=0}^{m-1}\log p(y_{j+1}\mid y_{\leq j},x).$$
<!-- The inference problem is then to find y∗ = argmaxy p(y | x). -->
推理是，目标找到 $y^* = \arg\max_y p(y | x)$。
<!-- Since the output space is exponentially large, exact search is intractable. As an approximation, we can perform greedy decoding to obtain a prediction yˆ as follows. Starting with an empty sequence yˆ and j = 0, we repeatedly extend our prediction with the highest-scoring token yˆj+1 = argmaxyj+1 p(yj+1 | yˆ≤j , x) and set j ← j + 1 until a termination condition is met. For language generation problems, we typically stop once a special end-of-sequence token has been generated. For image generation problems, we simply decode for a fixed number of steps. -->
由于输出空间庞大，精确搜索不可行。一般用贪婪解码得到预测的 $\hat{y}$：
+ 从空序列 $\hat{y}$ 和 $j = 0$ 开始，重复地用最高分数的 token 扩展预测：$\hat{y}_{j+1} = \arg\max_{y_{j+1}} p(y_{j+1} | \hat{y}_{\leq j}, x)$，$j \leftarrow j + 1$，直到满足终止条件
<!-- Blockwise Parallel Decoding -->
## Blockwise Parallel Decoding
<!-- Standard greedy decoding takes m steps to produce an output of length m, even for models that can efficiently score sequences using a constant number of sequential operations. While brute-force enumeration of output extensions longer than one token is intractable when the size of the vocabulary is large, we can still attempt to exploit parallelism within the model by training a set of auxiliary models to propose candidate extensions. -->
标准的贪婪解码需要 $m$ 步生成长度为 $m$ 的输出。当词表很大时，可以通过训练一组辅助模型来得到 candidate extensions。
<!-- Let the original model be p1 = p, and suppose that we have also learned a collection of auxiliary models p2,...,pk for which pi(yj+i | y≤j,x) is the probability of the (j + i)th token being yj+i given the first j tokens. We propose the following blockwise parallel decoding algorithm (illustrated in Figure 1), which is guaranteed to produce the same prediction yˆ that would be found under greedy decoding but uses as few as m/k steps. As before, we start with an empty prediction yˆ and set j = 0. Then we repeat the following three substeps until the termination condition is met: -->
![](image/Pasted%20image%2020240807174619.png)

假设原始模型为 $p_1 = p$，还有一组辅助模型 $p_2, \ldots, p_k$，其中 $p_i(y_{j+i} | y_{\leq j}, x)$ 是给定前 $j$ 个 token 时第 $(j + i)$ 个 token 为 $y_{j+i}$ 的概率。提出 blockwise parallel decoding 算法，保证生成相同的预测 $\hat{y}$，但只用 $m/k$ 步：
<!-- • Predict: Get the block predictions yˆj+i = argmaxyj+i pi(yj+i | yˆ≤j , x) for i = 1, . . . , k. -->
+ 预测：对于 $i = 1, \ldots, k$，得到 block 预测：$y_{j+i} = \arg\max_{y_{j+i}} p_i(y_{j+i} | \hat{y}_{\leq j}, x)$
<!-- Verify: Find the largest kˆ such that yˆj+i = argmaxyj+i p1(yj+i | yˆ≤j+i−1, x) for all 1 ≤ i ≤ kˆ. Note that kˆ ≥ 1 by the definition of yˆj+1. -->
+ 验证：找到最大的 $\hat{k}$，使得对于所有 $1 \leq i \leq \hat{k}$，$\hat{y}_{j+i} = \arg\max_{y_{j+i}} p_1(y_{j+i} | \hat{y}_{\leq j+i-1}, x)$。注意 $\hat{k} \geq 1$。
<!-- Accept:Extendyˆwithyˆj+1,...,yˆj+kˆ andsetj←j+kˆ. -->
+ 接受：将 $\hat{y}$ 拓展为 $\hat{y}_{j+1}, \ldots, \hat{y}_{j+\hat{k}}$，$j \leftarrow j + \hat{k}$
<!-- In the predict substep, we find the local greedy predictions of our base scoring model p1 and the auxiliary proposal models p2 , . . . , pk . Since these are disjoint models, each prediction can be computed in parallel, so there should be little time lost compared to a single greedy prediction. -->
在预测阶段中，找到 $p_1$ 和 $p_2, \ldots, p_k$ 的局部贪婪预测。
> 由于这些是不相交的模型，每个预测都可以并行计算，因此与单个贪婪预测相比，几乎没有时间损失。
<!-- Next, in the verify substep, we find the longest prefix of the proposed length-k extension that would have otherwise been produced by p1. If the scoring model can process this sequence of k tokens in fewer than k steps, this substep will help save time overall provided more than one token is correct. -->
在验证阶段，找到长度为 $k$ 的拓展的最长前缀，这个前缀由 $p_1$ 生成。如果 scoring model 可以在少于 $k$ 步内处理这个长度为 $k$ 的 token 序列，这个步骤将有助于节省时间，前提是多于一个 token 是正确的。
<!-- Lastly, in the accept substep, we extend our hypothesis with the verified prefix. By stopping early if the base model and the proposal models start to diverge in their predictions, we ensure that we will recover the same output that would have been produced by running greedy decoding with p1. -->
在接受阶段，用验证的前缀进行拓展。如果 base 模型和 proposal 模型在预测上出现分歧则会提前停止，确保结果与使用 $p_1$ 进行贪婪解码的结果相同。
<!-- The potential of this scheme to improve decoding performance hinges crucially on the ability of the base model p1 to execute all predictions made in the verify substep in parallel. In our experiments we use the Transformer model (Vaswani et al., 2017). While the total number of operations performed during decoding is quadratic in the number of predictions, the number of necessarily sequential operations is constant regardless of output length. This allows us to execute the verify substep for a number of positions in parallel without spending additional wall-clock time. -->
提高解码的关键在于，base model $p_1$ 在验证阶段中并行进行验证。
> 虽然解码期间的操作总数与预测数量的平方成正比，但顺序操作数是常数，不受输出长度影响。从而可以并行执行验证阶段的多个位置，而不需要额外的 wall-clock 时间。
<!-- Combined Scoring and Proposal Model -->
## Combined Scoring and Proposal Model
<!-- When using a Transformer for scoring, the version of our algorithm presented in Section 3 requires two model invocations per step: one parallel invocation of p1 , . . . , pk in the prediction substep, and an invocation of p1 in the verification substep. This means that even with perfect auxiliary models, we will only reduce the number of model invocations from m to 2m/k instead of the desired m/k. -->
使用 Transformer 进行 scoring 时，算法需要每步两次模型调用：预测阶段并行调用 $p_1, \ldots, p_k$，验证阶段调用 $p_1$。即使模型完美，调用次数也只能从 $m$ 减少到 $2m/k$，而非 $m/k$。
<!-- As it turns out, we can further reduce the number of model invocations from 2m/k to m/k + 1 if we assume a combined scoring and proposal model, in which case the nth verification substep can be merged with the (n + 1)st prediction substep. -->
如果假设 scoring 和 proposal 模型合并，可以将模型调用次数从 $2m/k$ 减少到 $m/k + 1$。
<!-- More specifically, suppose we have a single Transformer model which during the verification substep computes pi(yj+i′+i | yˆ≤j+i′,x) for all i = 1,...,k and i′ = 1,...,k in a constant number of operations. This can be implemented for instance by increasing the dimensionality of the final projection layer by a factor of k and computing k separate softmaxes per position. Invoking the model after plugging in the k future predictions from the prediction substep yields the desired outputs. -->
具体来说，假设有一个 Transformer 模型，在验证阶段对所有 $i = 1, \ldots, k$ 和 $i' = 1, \ldots, k$ 计算 $p_i(y_{j+i'+i} | \hat{y}_{\leq j+i'}, x)$，这可以通过增加最终投影层的维度 $k$ 倍，每个位置计算 $k$ 个独立的 softmax 实现。在预测阶段得到 $k$ 个未来预测后，调用模型得到输出。
<!-- Under this setup, after kˆ has been computed during verification, we will have already computed pi(yj+kˆ+i |y≤j+kˆ,x)foralli=1,...,k,whichisexactlywhatisrequiredfortheprediction substep in the next iteration of decoding. Hence these substeps can be merged together, reducing the number of model invocations by a factor of two for all but the very first iteration. -->
此时，在验证阶段计算 $\hat{k}$ 后，已经计算了 $p_i(y_{j+\hat{k}+i} | y_{\leq j+\hat{k}}, x)$，这正是下一次解码迭代的预测阶段所需的。因此这两个阶段可以合并，除了第一次迭代外，模型调用次数减少一半。
<!-- Figure 2 illustrates the process. Note that while proposals have to be computed for every position during the verification substep, all predictions can still be made in parallel. -->
如下图：
![](image/Pasted%20image%2020240807182335.png)